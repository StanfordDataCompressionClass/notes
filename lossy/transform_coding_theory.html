<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Transform Coding Theory</title>


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../././mdbook-admonish.css">
        <link rel="stylesheet" href=".././mdbook-admonish.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "light" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../contents.html">EE274: Data Compression, course notes</a></li><li class="chapter-item expanded "><a href="../lossless_iid/coverpage.html"><strong aria-hidden="true">1.</strong> Lossless data compression: basics</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../lossless_iid/intro.html"><strong aria-hidden="true">1.1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="../lossless_iid/prefix_free_codes.html"><strong aria-hidden="true">1.2.</strong> Prefix Free Codes</a></li><li class="chapter-item expanded "><a href="../lossless_iid/kraft_ineq_and_optimality.html"><strong aria-hidden="true">1.3.</strong> Kraft Inequality</a></li><li class="chapter-item expanded "><a href="../lossless_iid/entropy.html"><strong aria-hidden="true">1.4.</strong> Entropy and Neg-log likelihood thumb rule</a></li><li class="chapter-item expanded "><a href="../lossless_iid/huffman.html"><strong aria-hidden="true">1.5.</strong> Huffman coding</a></li><li class="chapter-item expanded "><a href="../lossless_iid/aep.html"><strong aria-hidden="true">1.6.</strong> Asymptotic Equipartition Property</a></li><li class="chapter-item expanded "><a href="../lossless_iid/arithmetic_coding.html"><strong aria-hidden="true">1.7.</strong> Arithmetic coding</a></li><li class="chapter-item expanded "><a href="../lossless_iid/ans.html"><strong aria-hidden="true">1.8.</strong> Asymmetric Numeral Systems</a></li><li class="chapter-item expanded "><a href="../lossless_iid/non_iid_sources.html"><strong aria-hidden="true">1.9.</strong> Non IID Sources and Entropy Rate</a></li><li class="chapter-item expanded "><a href="../lossless_iid/context_based_coding.html"><strong aria-hidden="true">1.10.</strong> Context-based coding</a></li><li class="chapter-item expanded "><a href="../lossless_iid/lz77.html"><strong aria-hidden="true">1.11.</strong> Universal Compression with LZ77</a></li><li class="chapter-item expanded "><a href="../lossless_iid/practical_tips.html"><strong aria-hidden="true">1.12.</strong> Practical Tips on Lossless Compression</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="../lossy/coverpage.html"><strong aria-hidden="true">2.</strong> Lossy data Compression</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../lossy/quant.html"><strong aria-hidden="true">2.1.</strong> Basics and Quantization</a></li><li class="chapter-item expanded "><a href="../lossy/rd.html"><strong aria-hidden="true">2.2.</strong> Rate-Distortion Theory</a></li><li class="chapter-item expanded "><a href="../lossy/transform_coding_theory.html" class="active"><strong aria-hidden="true">2.3.</strong> Transform Coding Theory</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="../resources.html"><strong aria-hidden="true">3.</strong> Resources</a></li><li class="chapter-item expanded affix "><li class="spacer"></li><li class="chapter-item expanded "><a href="../homeworks/coverpage.html"><strong aria-hidden="true">4.</strong> Homeworks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../homeworks/HW1.html"><strong aria-hidden="true">4.1.</strong> HW1</a></li><li class="chapter-item expanded "><a href="../homeworks/HW1_sol.html"><strong aria-hidden="true">4.2.</strong> HW1 Solution</a></li><li class="chapter-item expanded "><a href="../homeworks/HW2.html"><strong aria-hidden="true">4.3.</strong> HW2</a></li><li class="chapter-item expanded "><a href="../homeworks/HW2_sol.html"><strong aria-hidden="true">4.4.</strong> HW2 Solution</a></li><li class="chapter-item expanded "><a href="../homeworks/HW3.html"><strong aria-hidden="true">4.5.</strong> HW3</a></li><li class="chapter-item expanded "><a href="../homeworks/HW3_sol.html"><strong aria-hidden="true">4.6.</strong> HW3 Solution</a></li><li class="chapter-item expanded "><a href="../homeworks/HW4.html"><strong aria-hidden="true">4.7.</strong> HW4</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="../projects.html"><strong aria-hidden="true">5.</strong> Project</a></li><li class="chapter-item expanded affix "><li class="spacer"></li><li class="chapter-item expanded "><a href="../scl_tutorial/SCL_tutorial.html"><strong aria-hidden="true">6.</strong> SCL Tutorial</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../scl_tutorial/basics.html"><strong aria-hidden="true">6.1.</strong> SCL Basics</a></li><li class="chapter-item expanded "><a href="../scl_tutorial/exercise.html"><strong aria-hidden="true">6.2.</strong> SCL Exercise</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="../quiz_problems_2023.html"><strong aria-hidden="true">7.</strong> Quiz Problems (2023)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="lossy-compression-theory"><a class="header" href="#lossy-compression-theory">Lossy compression theory</a></h1>
<p>We continue with lossy compression and rate distortion function (Shannon's RD theory) and applications in current technologies.</p>
<p>We'll start with things that seem unrelated but we'll bring it all together towards the end. We'll only touch on some of the topics, but you can learn more in the references listed below and in EE 276.</p>
<p>We'll start with the rate distortion function and see how it carries over to sources with memory. We'll also look into Gaussian sources with memory. Finally, we'll look at the implications to transform coding which is commonly used today.</p>
<h2 id="reminder-from-linear-algebra"><a class="header" href="#reminder-from-linear-algebra">Reminder from linear algebra:</a></h2>
<p>Consider $Y = A X$ (matrix vector product)</p>
<p>Then the square of the Euclidean norm (sum of square of components), also denoting the energy in the signal is
$$||Y||^2 = Y^T Y = X^T A^T A X$$</p>
<p>In particular, if $U$ is a unitary transformation, i.e., all rows and columns orthonormal vectors $U^T U = U U^T = I$, then we have
$$Y = U X =&gt; ||Y||^2 = ||X||^2$$
This is called the Parseval's theorem which you might have seen for Fourier transform. In words, this says that the energy in transform domain matches the energy in the original.</p>
<p>If $Y_1 = U X_1$ and $Y_2 = U X_2$, then $||Y_1 - Y_2||^2 = ||X_1-X_2||^2$. That is to say, unitary transformation preserves Euclidean distances between points.</p>
<h2 id="lossy-compression-recap"><a class="header" href="#lossy-compression-recap">Lossy compression recap</a></h2>
<p><img src="images/lossy_block_diagram.png" alt="" /></p>
<p>Recall the setting of lossy compression where the information is lossily compressed into an index (equivalently a bit stream representing the index). The decoder attempts to produce a reconstruction of the original information.</p>
<p>The two metrics for lossy compression are:</p>
<ul>
<li>$rate = {{log N} \over k}$ bits/source component</li>
<li>distortion $d(X^k, \hat{X}^k) = {1 \over k} \sum_{i=1}^k d(X_i,\hat{X}_i)$ [single letter distortion - distortion between k-tuples defined in terms of distortion between components]</li>
</ul>
<h2 id="transform-coding"><a class="header" href="#transform-coding">Transform coding</a></h2>
<p><strong>Notation:</strong> we denote $X^k = (X_1,\dots,X_k)$ as $\underline{X}$ which can be thought of as a column vector.</p>
<p><img src="images/transform_block_diagram.png" alt="" /></p>
<p>Here we simply work with an arbitrary transform $T$, with the only requirement being that $T$ is invertible and we are able to efficiently compute $T$ and $T^{-1}$. In this framework, we simply apply our usual lossy encoding in the transform domain rather than in the original domain.</p>
<p>In particular, when $T (X) = U X$ for some some unitary $U$ (e.g., Fourier transform, wavelet transform). Then
$$||Y-\hat{Y}||^2 = ||X-\hat{X}||^2 $$
This corresponds to the squared-error distortion. Any lossy compression you do on $Y$, you get the same square error distortion for the original sequence $X$ as for the $Y$.</p>
<p>Why work in the transform domain? Often in the transform domain, data is simpler to model, e.g., we can construct transform in a way that the statistics of $Y$ are simpler or we get sparsity. Then we can appropriately design lossy compressor to exploit the structure</p>
<p>Nowadays people even go beyond linear transforms, e.g., learnt transforms using deep learning models.
Can even go to a vector in a smaller dimensional space, e.g., in VAE based lossy encoders. This can allow doing very simple forms of lossy compression in the transform domain.</p>
<h2 id="shannons-theorem-recap"><a class="header" href="#shannons-theorem-recap">Shannon's theorem recap</a></h2>
<p>For "memoryless sources" ($X_i$ are iid ~$X$),</p>
<p>$$R(D) = min_{E[d(X,\hat{X})] &lt;= D} I(X; \hat{X})$$</p>
<p>We sometimes write $R(X,D)$ to represent this quantity $R(D)$ when we want to be explicit about the source in question.</p>
<h3 id="beyond-memoryless-sources"><a class="header" href="#beyond-memoryless-sources">Beyond memoryless sources</a></h3>
<p>Consider source $X^n$, reconstruction $\hat{X}^n$. Then,</p>
<p>$$R(X^n, D) = min_{E[d(X^n, \hat{X}^n)] \leq D} {1\over n} I(X^n; \hat{X}^n)$$</p>
<p>Just like $R(X,D)$ was the analog of entropy of $X$, $R(X^n, D)$ is the analog of entropy of the n-tuple.</p>
<p>Now assume we are working with a process $\mathbf{X} = X_1,X_2,X_3,...$ which is stationary. Then we can define $R(\mathbf{X}, D) = \lim_{n\rightarrow \infty} R(X^n, D)$. Similar to our study of entropy rate, we can show that this limit exists.</p>
<p>Shannon's theorem for lossy compression carries over to generality. That is, the best you can do for stationary processes in the limit of encoding arbitrarily many symbols in a block is $R(\mathbf{X}, D)$.</p>
<h2 id="rate-distortion-for-gaussian-sources"><a class="header" href="#rate-distortion-for-gaussian-sources">Rate distortion for Gaussian sources</a></h2>
<p><strong>Note:</strong> For the remainder of this discussion, we'll stick to square error distortion.</p>
<p>Why work with Gaussian sources? It is a good worst case assumption if you only know the first and second order statistics about your source. This holds both for estimation and lossy compression.</p>
<p>For $X ~ N(0,\sigma^2)$, denote $R(X,D)$ by $R_G(\sigma^2, D)$.</p>
<p>Recall from last lecture that $R_G(\sigma^2, D) = {1\over 2} \log {\sigma^2 \over D}$ for $D &lt; \sigma^2$ (above it is just $0$).</p>
<p>We can compactly write this as $R_G(\sigma^2, D) = [1/2 \log {\sigma^2 \over D}]<em>+$, where $[x]</em>+ = max{0,x}$. This is shown in the figure below.</p>
<p><img src="images/gaussian_rd.png" alt="" /></p>
<p>Similarly for $X_1 \sim N(0, \sigma_1^2)$, $X_2 \sim N(0, \sigma_2^2)$ independent, denote $R(X^2, D)$ by $R_G\left(\begin{bmatrix}\sigma_1^2\ \sigma_2^2\end{bmatrix} , D\right)$.</p>
<p>It can be shown that
$$R_G\left(\begin{bmatrix}\sigma_1^2\ \sigma_2^2\end{bmatrix} , D\right) = min_{{1\over 2} (D_1+D_2)\leq D} {1\over 2} [R_G(\sigma_1^2, D_1) + R_G(\sigma_1^2, D_2)]$$
Another way to write this is
$$R_G\left(\begin{bmatrix}\sigma_1^2\ \sigma_2^2\end{bmatrix} , D\right) =min_{\frac{1}{2} (D_1+D_2)\leq D}\  {1\over 2} \left[({1\over 2} \log \frac{\sigma_1^2}{D_1})<em>+ + ({1\over 2} \log \frac{\sigma_2^2}{D_2})</em>+\right]$$</p>
<p>Intuition: the result is actually quite simple - the solution is just greedily optimizing the $X_1$ and $X_2$ case (decoupled), and finding the optimal splitting of the distortion between $X_1$ and $X_2$.</p>
<p>Using convex optimization we can show that the minimum is achieved by a reverse water filling scheme, which is expressed in equation as follows:</p>
<p>For a given parameter $\theta$, a point on the optimal rate distortion curve is achieved by setting</p>
<ul>
<li>$D_i = \min {\theta, \sigma_i^2}$ for $i = 1,2$</li>
<li>$D = {1\over 2} (D_1+D_2)$</li>
</ul>
<p>And the rate given by
$${1\over 2} \left[({1\over 2} \log \frac{\sigma_1^2}{D_1})<em>+ + ({1\over 2} \log \frac{\sigma_2^2}{D_2})</em>+\right]$$</p>
<p>This can be expressed in figures as follows (assuming without loss of generality that $\sigma_1^2 &lt; \sigma_2^2$):</p>
<p>When $D$ is smaller than both $\sigma_1^2$ and $\sigma_2^2$, we choose both $D_1$ and $D_2$ to be equal to $D$ ($\theta=D$ in this case). We assign equal distortion to the two components, and higher rate for the component with higher variance.</p>
<p><img src="images/water_filling_1.png" alt="" /></p>
<p>When $D$ exceeds $\sigma_1^2$ but is below $\frac{1}{2}(\sigma_1^2 + \sigma_2^2)$, we set $D_1$ to be $\sigma_1^2$, and choose $D_2$ such that the average distortion is $D$. The idea is that setting $D_1$ higher than $\sigma_1^2$ doesn't make sense since the rate is already $0$ for that component.</p>
<p><img src="images/water_filling_2.png" alt="" /></p>
<p>When $D$ is equal to $\frac{1}{2}(\sigma_1^2 + \sigma_2^2)$ we can just set $D_1 = \sigma_1^2$ and $D_2 = \sigma_2^2$. Here the rate is $0$ for both components!</p>
<p><img src="images/water_filling_3.png" alt="" /></p>
<p>This generalizes beyond $2$ components. For $X_1, X_2, ... ,X_n$ independent with $X_i \sim N(0,\sigma_i^2)$, we define $R_G(\underline{\sigma^2}, D)$ analogously, and can very similarly show that $$R_G(\underline{\sigma^2}, D) = min_{{1\over n} \sum D_i \leq D} {1\over n} \left[{1\over 2} log {\sigma^2 \over D_i} \right]_+$$.</p>
<p>Similar to before, the minimum is given by $D_\theta = {1 \over n} \sum_{i=1}^n \min {\theta, \sigma_i^2}$, $R_\theta = {1 \over n} \sum_{i=1}^n [{1\over 2} \log {\sigma_i^2\over D_i}]$.</p>
<h2 id="rate-distortion-for-stationary-gaussian-source"><a class="header" href="#rate-distortion-for-stationary-gaussian-source">Rate-distortion for stationary Gaussian source</a></h2>
<p>Going back to a process $X^n$ zero mean Gaussian, then for any unitary transformation $U$ if $Y^n =U X^n$ then we can show $R(X^n, D) = R(Y^n, D)$ [since the distortion is the same in both domains]. Recall that by using the transformation it's possible to go from a scheme for compressing $X^n$ to a scheme for compressing $Y^n$ (and vice versa) without any change in the distortion.</p>
<p>Therefore we can take the diagonalizing unitary matrix which converts $X^n$ to a $Y^n$ such that $Y^n$ has independent components. The variances of $Y^n$ will be the eigenvalues of the covariance matrix.</p>
<p>Thus, we have</p>
<p>$$R(X^n, D) = R_G ((\lambda_1, \dots, \lambda_n), D)$$
where the $\lambda_i$'s are the eigenvalues of the covariance matix of $X^n$.</p>
<p>When $X^n$ are the first $n$ components of a stationary Gaussian process $\mathbf{X}$ with covariance matrix $\Phi_n = {\phi_{|i-j|}}$ for $1\leq i \leq n$ and $1\leq j \leq n$, with $\phi_{k} = Cov(X_i,X_{i-k})$. Then we have
$$R(X^n, D) = R_G(\overrightarrow{\lambda}^n,D)$$
where $\overrightarrow{\lambda}^n$ is the vector of eigenvalues of $\Phi_n$.</p>
<p>Now, we use a theorem to show a profound result for Gaussian processes.</p>
<blockquote>
<p><strong>Theorem (Toeplitz distribution)</strong>
Let $S(\omega) = \sum_{k=-\infty}^{\infty} \phi_k e^{-j\omega k}$ be the spectral density of $\mathbf{X}$ and $G$ be a continuous function. Then
$$\lim_{n\rightarrow \infty} \frac{1}{n}\sum_{i=1}^n G(\lambda_i^{(n)}) = \frac{1}{2\pi}\int_{-\pi}^{\pi} G(S(\omega) d\omega$$</p>
</blockquote>
<p>Specializing this theorem to $G(\lambda)= \min{\theta,\lambda}$ and to $G(\lambda)= \left[\frac{1}{2}\log\frac{\lambda}{\theta}\right]_+$, we get</p>
<blockquote>
<p>The rate distortion function of a stationary Gaussian process with spectral density $S(\omega)$ is given parametrically by
$$D_{\theta} = \frac{1}{2\pi}\int_{-\pi}^{\pi} \min{\theta,S(\omega)} d\omega$$
$$R_{\theta} = \frac{1}{4\pi}\int_{-\pi}^{\pi} \left[\log\frac{S(\omega)}{\theta}\right]_+ d\omega$$</p>
</blockquote>
<p>This is shown in the figure below, suggesting that the reverse water-filling idea extends to Gaussian processes once we transform it to the continuous spectral domain! This gives us motivation for using working in the Fourier transform domain!</p>
<p><img src="images/gaussian_process_water_filling.png" alt="" /></p>
<p>Finally, for $D \leq \min_{\omega} S(\omega)$, we can show that $$R(D) = \left[\frac{1}{2} \log \frac{\sigma^2}{D}\right]$$
where $\sigma^2$ is the variance of the innovations of $\mathbf{X}$. This can be used to justify predictive coding ideas.</p>
<h2 id="reference"><a class="header" href="#reference">Reference</a></h2>
<p>For more details on this, you can read the survey paper "Lossy source coding" by Berger and Gibson available at <a href="https://ieeexplore.ieee.org/document/720552">https://ieeexplore.ieee.org/document/720552</a>.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../lossy/rd.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../resources.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../lossy/rd.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../resources.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="../mermaid.min.js"></script>
        <script type="text/javascript" src="../mermaid-init.js"></script>


    </body>
</html>
