<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Rate-Distortion Theory</title>


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../././mdbook-admonish.css">
        <link rel="stylesheet" href=".././mdbook-admonish.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "light" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../contents.html">EE274: Data Compression, course notes</a></li><li class="chapter-item expanded "><a href="../lossless_iid/coverpage.html"><strong aria-hidden="true">1.</strong> Lossless data compression: basics</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../lossless_iid/intro.html"><strong aria-hidden="true">1.1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="../lossless_iid/prefix_free_codes.html"><strong aria-hidden="true">1.2.</strong> Prefix Free Codes</a></li><li class="chapter-item expanded "><a href="../lossless_iid/kraft_ineq_and_optimality.html"><strong aria-hidden="true">1.3.</strong> Kraft Inequality</a></li><li class="chapter-item expanded "><a href="../lossless_iid/entropy.html"><strong aria-hidden="true">1.4.</strong> Entropy and Neg-log likelihood thumb rule</a></li><li class="chapter-item expanded "><a href="../lossless_iid/huffman.html"><strong aria-hidden="true">1.5.</strong> Huffman coding</a></li><li class="chapter-item expanded "><a href="../lossless_iid/aep.html"><strong aria-hidden="true">1.6.</strong> Asymptotic Equipartition Property</a></li><li class="chapter-item expanded "><a href="../lossless_iid/arithmetic_coding.html"><strong aria-hidden="true">1.7.</strong> Arithmetic coding</a></li><li class="chapter-item expanded "><a href="../lossless_iid/ans.html"><strong aria-hidden="true">1.8.</strong> Asymmetric Numeral Systems</a></li><li class="chapter-item expanded "><a href="../lossless_iid/non_iid_sources.html"><strong aria-hidden="true">1.9.</strong> Non IID Sources and Entropy Rate</a></li><li class="chapter-item expanded "><a href="../lossless_iid/context_based_coding.html"><strong aria-hidden="true">1.10.</strong> Context-based coding</a></li><li class="chapter-item expanded "><a href="../lossless_iid/lz77.html"><strong aria-hidden="true">1.11.</strong> Universal Compression with LZ77</a></li><li class="chapter-item expanded "><a href="../lossless_iid/practical_tips.html"><strong aria-hidden="true">1.12.</strong> Practical Tips on Lossless Compression</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="../lossy/coverpage.html"><strong aria-hidden="true">2.</strong> Lossy data Compression</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../lossy/quant.html"><strong aria-hidden="true">2.1.</strong> Basics and Quantization</a></li><li class="chapter-item expanded "><a href="../lossy/rd.html" class="active"><strong aria-hidden="true">2.2.</strong> Rate-Distortion Theory</a></li><li class="chapter-item expanded "><a href="../lossy/transform_coding_theory.html"><strong aria-hidden="true">2.3.</strong> Transform Coding Theory</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="../resources.html"><strong aria-hidden="true">3.</strong> Resources</a></li><li class="chapter-item expanded affix "><li class="spacer"></li><li class="chapter-item expanded "><a href="../homeworks/coverpage.html"><strong aria-hidden="true">4.</strong> Homeworks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../homeworks/HW1.html"><strong aria-hidden="true">4.1.</strong> HW1</a></li><li class="chapter-item expanded "><a href="../homeworks/HW1_sol.html"><strong aria-hidden="true">4.2.</strong> HW1 Solution</a></li><li class="chapter-item expanded "><a href="../homeworks/HW2.html"><strong aria-hidden="true">4.3.</strong> HW2</a></li><li class="chapter-item expanded "><a href="../homeworks/HW2_sol.html"><strong aria-hidden="true">4.4.</strong> HW2 Solution</a></li><li class="chapter-item expanded "><a href="../homeworks/HW3.html"><strong aria-hidden="true">4.5.</strong> HW3</a></li><li class="chapter-item expanded "><a href="../homeworks/HW3_sol.html"><strong aria-hidden="true">4.6.</strong> HW3 Solution</a></li><li class="chapter-item expanded "><a href="../homeworks/HW4.html"><strong aria-hidden="true">4.7.</strong> HW4</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="../projects.html"><strong aria-hidden="true">5.</strong> Project</a></li><li class="chapter-item expanded affix "><li class="spacer"></li><li class="chapter-item expanded "><a href="../scl_tutorial/SCL_tutorial.html"><strong aria-hidden="true">6.</strong> SCL Tutorial</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../scl_tutorial/basics.html"><strong aria-hidden="true">6.1.</strong> SCL Basics</a></li><li class="chapter-item expanded "><a href="../scl_tutorial/exercise.html"><strong aria-hidden="true">6.2.</strong> SCL Exercise</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="../quiz_problems_2023.html"><strong aria-hidden="true">7.</strong> Quiz Problems (2023)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="rate-distortion-theory"><a class="header" href="#rate-distortion-theory">Rate Distortion Theory</a></h1>
<h2 id="entropy-conditional-entropy-recap"><a class="header" href="#entropy-conditional-entropy-recap">Entropy, Conditional entropy Recap:</a></h2>
<p>In case of lossless compression we saw information theoretic quantities such as entropy: $H(X)$, conditional entropy: $H(X|Y)$ etc.</p>
<p>To recap:</p>
<ol>
<li>
<p><strong>Entropy</strong>: Let $X$ be a random variable, with alphabet ${1, 2, \ldots, k}$ and discrete probability distribution $P = { p_1, p_2, \ldots, p_k}$. i.e. one can imagine the samples generate by the random variable $X$ to be independent and identically distributed as per distribution $P$.</p>
<p>Then the entropy $H(X)$ of the random variable is defined as:
$$ H(X) = \sum_{i=1}^k p_i \log_2 \frac{1}{p_i} $$</p>
</li>
<li>
<p><strong>Joint Entropy</strong>: The joint entropy of discrete random variables $X,Y$ with distribution $P(x,y)$ is simply the entropy of the joint random variable $X,Y$.</p>
</li>
</ol>
<p>$$ H(X,Y) = \sum_{x,y} p(x,y) \log_2 \frac{1}{p(x,y} $$</p>
<ol start="3">
<li><strong>Conditional Entropy/Relative Entropy</strong> The conditional entropy between two discrete random variables $X,Y$ is defined as:</li>
</ol>
<p>$$ H(Y|X) = \sum_{x,y} p(x,y) \log_2 \frac{1}{p(y|x)}$$</p>
<p>Note that $H(Y|X)$ has alternative definitions:</p>
<p>$$H(Y|X) = \sum_x P(x)H(Y|X=x)$$</p>
<p>i.e. it is the average of the entropies $H(Y|X=x)$</p>
<h2 id="mutual-information"><a class="header" href="#mutual-information">Mutual Information</a></h2>
<p>For today's discussion another important information theoretic quantity which would be interesting to us is the mutual information $I(X;Y)$</p>
<div id="admonition-mutual-information" class="admonition admonish-note" role="note" aria-labelledby="admonition-mutual-information-title">
<div class="admonition-title">
<div id="admonition-mutual-information-title">
<p>Mutual Information</p>
</div>
<a class="admonition-anchor-link" href="#admonition-mutual-information"></a>
</div>
<div>
<p>Let $X,Y$ be two random variables with joint distribution $p(x,y)$. Then we define the mutual information between $X,Y$ as:</p>
<p>$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$</p>
</div>
</div>
<p>One intuitive explanation for mututal information is that it is the difference between sum of individial entropies and the joint entropy between two random variables $X,Y$, and so in a way capture how much information is common between $X,Y$.</p>
<p>Mutual information has some nice properties, which we will use:</p>
<ol>
<li><strong>property-1: $I(X;Y) = I(Y;X)$</strong>: It is clear from the symmetry that mutual information between $X,Y$ is equal to mutual information between $Y,X$</li>
</ol>
<p>$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$</p>
<ol start="2">
<li>
<p><strong>property-2: $I(X;Y) = H(X) - H(X|Y)$</strong>:
This can be shown using the property of joint entropy: $H(U,V) = H(U) + H(V|U)$.
$$
\begin{align*}
I(X;Y) &amp;= H(X) + H(Y) - H(X,Y) \
&amp;= H(X) + H(Y) - H(Y) - H(X|Y) \
&amp;= H(X) - H(X|Y)
\end{align*}
$$</p>
</li>
<li>
<p><strong>property-3: $I(X;Y) = D_{KL}(p(x,y)||p(x)p(y))$</strong>:</p>
<p>$$
\begin{align*}
I(X;Y) &amp;= H(X) + H(Y) - H(X,Y) \
&amp;= \sum_{x} p(x) \log_2 \frac{1}{p(x)} +  \sum_{y} p(y) \log_2 \frac{1}{p(y)} + \sum_{x,y} p(x,y) \log_2 \frac{1}{p(x,y)} \
&amp;= \sum_{x,y} p(x,y) \log_2 \frac{p(x)p(y)}{p(x,y)} \
&amp;= D_{KL}(p(x,y)||p(x)p(y))
\end{align*}
$$</p>
</li>
<li>
<p><strong>property-4: $I(X;Y) \geq 0$</strong>: This follows from the non-negativity of the $D_{KL}$ and the <em>property-3</em>.</p>
</li>
</ol>
<p>Mutual Information has a very important connection to lossy compression, but even beyond that in Information theory in general. For more information look at the <a href="https://web.stanford.edu/class/ee276/files/lec/Ee276-lec9.pdf">EE276 course notes on communication capacity</a></p>
<h2 id="lossy-compression-setup"><a class="header" href="#lossy-compression-setup">Lossy Compression setup</a></h2>
<p>Let us recap the lossy compression setup, and now that we are going to discuss this in detail, let us define it more concretely.</p>
<p>Lets say we are given a sequence of random variables $X_1, X_2, \ldots, X_k$, Our goal is to encode this sequence $X_1^k$ to $n=\log_2(N)$ bits, using a <em>lossy encoder</em>. We also decode back the $N$ bits to reconstruction $Y_1, Y_2, \ldots, Y_k$, using a <em>lossy decoder</em>.</p>
<pre><code>## Encoding
X_1,X_2, ..., X_k ====&gt; [LOSSY ENCODER] ==&gt;  0100011...1 (n = log2(N) bits) 

## Decoding
0100011...1 (N bits) ======&gt; [LOSSY DECODER] ==&gt;
Y_1, Y_2, \ldots, Y_k
</code></pre>
<p>At this point, we need a few more terms to understand the performance of our <em>lossy compressor</em>:</p>
<ol>
<li><strong>Rate $R$</strong>: the compression rate $R$ is defined as, the average number of bits used per source symbols:</li>
</ol>
<p>$$ R = \frac{\log_2(N)}{k} = n/k$$</p>
<ol start="2">
<li><strong>distortion $D$</strong>: As the compression is not lossless, we also need to know how far the reconstruction $Y_1^k$ is from the input $X_1^k$.
$$ \text{distortion} \rightarrow d(X_1^k, Y_1^k)$$</li>
</ol>
<p>For simplicity lets stick to per-symbol distortion like mean square error, or hamming distortion.
Thus,</p>
<p>$$ d(X_1^k, Y_1^k) = \sum_k d(X_i, Y_i)$$</p>
<p>For example:</p>
<p>$$ \begin{align*}
\text{hamming distortion} &amp;\rightarrow d(x,y) = \mathbb{1}(x\neq y) \
\text{mse distortion} &amp;\rightarrow d(x,y) = (x-y)^2
\end{align*}
$$</p>
<ol start="3">
<li><strong>Average Distortion</strong> We mainly care about the <em>average distortion</em>:
$$ \text{expected distortion} \rightarrow \bar{D} = \mathbb{E}[d(X_1^k, Y_1^k)] $$
over the random variables $X_1^k, Y_1^k$.</li>
</ol>
<h2 id="rate-distortion-function"><a class="header" href="#rate-distortion-function">Rate-distortion function</a></h2>
<p>One interesting question to answer is:
<em>"What is the best rate R we can achieve for distortion at max D</em>"? i.e. If the target distortion of our lossy compressor is $D$, what is the best we can compress the data $X_1^k$ to? We define this optimal rate to be $R(D)$, the rate-distortion function.</p>
<p>Thus:</p>
<p>$$ R(D) \triangleq \min_{\text{all lossy compressors s.t.} \bar{D} \leq D}  R$$</p>
<p>This is the precise problem Shannon solved.</p>
<div id="admonition-shannons-lossy-compression-theorem" class="admonition admonish-note" role="note" aria-labelledby="admonition-shannons-lossy-compression-theorem-title">
<div class="admonition-title">
<div id="admonition-shannons-lossy-compression-theorem-title">
<p>Shannon's lossy compression theorem</p>
</div>
<a class="admonition-anchor-link" href="#admonition-shannons-lossy-compression-theorem"></a>
</div>
<div>
<p>Let $X_1,X_2,\ldots$ be data generated i.i.d. Then, the optimal rate $R(D)$ for a given maximum distortion $D$ is:</p>
<p>$$ R(D) = \min_{\mathbb{E}d(X,Y) \leq D} I(X;Y)$$</p>
<p>where the expectation in the minimum is over distributions $q(x,y) = p(x)q(y|x)$, where $q(y|x)$ are any arbitrary conditional distributions.</p>
</div>
</div>
<p>Here are some examples:</p>
<h3 id="example-1-rd-for-bernoulli-rv"><a class="header" href="#example-1-rd-for-bernoulli-rv">Example-1: $R(D)$ for bernoulli r.v:</a></h3>
<p>Let $X \sim Bern(0.5)$. and let $d(x,y) = \mathbb{1}(x\neq y)$ i.e the Hamming distortion. Then:</p>
<p>$$ R(D) = \begin{cases}
1 - h(D) &amp; 0 \leq D \leq 0.5\
0 &amp; D &gt; 0.5
\end{cases}
$$</p>
<p>where $h(D)$ -&gt; binary entropy function of $Bern(p) = h(p)$.</p>
<p>The $R(D)$ formula is not very intuitive. But, it is clear that:</p>
<ol>
<li>
<p>$R(D)$ decreases as $D$ increases; this should be expected as the bits required should reduce if the allowed distortion $D$ is increasing.</p>
</li>
<li>
<p>$R(D) = 0$ if $D &gt; 0.5$; This is also quite intuitve as if allowed distortion is $D &gt; 0.5$, we can always just decode all zeros $Y_1^k = 000000...00$. For all zeros, the average distortion is $0.5$.</p>
</li>
</ol>
<h3 id="example-2-rd-for-gaussian-rv"><a class="header" href="#example-2-rd-for-gaussian-rv">Example-2: $R(D)$ for gaussian r.v.:</a></h3>
<p>Let's take a look at another example: Let $X \sim \mathcal{N}(0,1)$, i.e. the data samples $X_1, X_2, \ldots$ are distributed as unit gaussians. Also, lets consider the distortion to be the mean square distortion: $d(x,y) = (x-y)^2$ i.e the mse distortion. Then:</p>
<p>$$ R(D) = \begin{cases}
\frac{1}{2} \log_2 \frac{1}{D} &amp; 0 \leq D \leq 1.0\
0 &amp; D &gt; 1
\end{cases}
$$</p>
<p>Let's try to intuitively understand why this is the case:</p>
<p><img src="https://user-images.githubusercontent.com/1708665/200032503-e0b7b247-4e4c-4ea2-844e-e7864cf35186.jpg" alt="Note Nov 3, 2022-1" /></p>
<ul>
<li>If $X_i$'s are i.i.d $\mathcal{N}(0,1)$, then it is clear that with high probability:</li>
</ul>
<p>$$ \sqrt {\sum_{i=1}^k (X_i)^2} \lesssim \sqrt{k}$$</p>
<ul>
<li>This mainly follows from the law of large numbers, and that the variance of $X_i$ is $1$. Thus, we can say that $X_1^k$ will lie in a k-dimensional sphere of radius $\sqrt{k}$, with high probability.</li>
</ul>
<p>For the remaining discussion, we can thus focus our attention on vectors $X_1^k$, only lying inside this sphere of radius $\sqrt{k}$.</p>
<ul>
<li>Now, lets say we want to cover this k-dimensional sphere of radius $\sqrt{k}$ with k-dimensional spheres of radius $\sqrt{Dk}$. How many spheres do we need?</li>
</ul>
<p>We can approximate this number based on the volumes of the spheres:</p>
<p>$$
\begin{align*}
N &amp;\geq \frac{Volume(\sqrt{k})}{Volume(\sqrt{Dk})} \
&amp;= \sqrt{1/D}^k
\end{align*}
$$</p>
<p>Although this approximation might feel very loose, as the dimension $k$ increases, it can be shown that the number of spheres of radius $\sqrt{Dk}$ required to cover the sphere of radius $\sqrt{k}$, is indeed approximately equal to:</p>
<p>$$ N \approx \left(\frac{1}{D}\right)^{k/2} $$</p>
<ul>
<li>
<p>Note that we can use these $N$ spheres of radius $\sqrt{Dk}$, as centroids for vector quantization, as we saw in the last lecture, and we would get a distortion of at max $D$, as the squared distance between any point in the $\sqrt{k}$ sized circle is at max $D$ with one of the $N$ centroids.</p>
</li>
<li>
<p>Thus our $R(D)$, the rate for distortion at maximum $D$ is:</p>
</li>
</ul>
<p>$$
\begin{align*}
R(D) \leq \frac{\log_2 N}{k} \approx \frac{1}{2} \log_2 \frac{1}{D}
\end{align*}
$$</p>
<p>Hope this hand-wavey "proof" gives an intuition for the $R(D)$ function for unit gaussian. The proof logic can however be made more precise.</p>
<p>NOTE: Similar idea proof holds for general distributions, using typical sequences balls. We won't be able to go much into the details of the Shannon's lossy compression theorem in the course unfortunately, but here are lecture notes in case you are interested: <a href="https://web.stanford.edu/class/ee376a/files/2017-18/lecture_12.pdf">EE376a Lossy compression notes</a></p>
<p>We can also experiment with this idea, here is the R-D curve for unit gaussian and the practical performance in $k=2$. We see that the R-D performance even with $k=2$ is quite reasonable.</p>
<p><img src="https://user-images.githubusercontent.com/1708665/200065773-94db5436-f2d4-4422-b0ee-4c7ab673731c.png" alt="RD" /></p>
<p>We can also see the convergence as $k$ increases:</p>
<pre><code class="language-py">## Rate: 1, optimal_mse: 0.25
k: 1, N: 2,   Rate: 1, mse_loss: 0.37
k: 2, N: 4,   Rate: 1, mse_loss: 0.36
k: 4, N: 16,  Rate: 1, mse_loss: 0.33
k: 8, N: 256, Rate: 1, mse_loss: 0.29
...
</code></pre>
<h2 id="achieving-the-rd-in-general"><a class="header" href="#achieving-the-rd-in-general">Achieving the R(D) in general</a></h2>
<p>We saw briefly how we can achieve the $R(D)$ optimal function using vector quantization for data distributed as i.i.d unit gaussians.</p>
<p>The Broad idea of using vector quantization can be actually shown to asymptotically optimal for any data distribution. i.e. as the dimension of data $k$ increases, using vector quantization, we can achieve optimal $R(D)$ performance.</p>
<p>Although the convergence w.r.t $k$ can be slow. In the next lecture we will see how we can accelerate this convergence.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../lossy/quant.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../lossy/transform_coding_theory.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../lossy/quant.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../lossy/transform_coding_theory.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="../mermaid.min.js"></script>
        <script type="text/javascript" src="../mermaid-init.js"></script>


    </body>
</html>
