<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="././mdbook-admonish.css">
        <link rel="stylesheet" href="./mdbook-admonish.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "light" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="contents.html">EE274: Data Compression, course notes</a></li><li class="chapter-item expanded "><a href="lossless_iid/coverpage.html"><strong aria-hidden="true">1.</strong> Lossless data compression: basics</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lossless_iid/intro.html"><strong aria-hidden="true">1.1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="lossless_iid/prefix_free_codes.html"><strong aria-hidden="true">1.2.</strong> Prefix Free Codes</a></li><li class="chapter-item expanded "><a href="lossless_iid/kraft_ineq_and_optimality.html"><strong aria-hidden="true">1.3.</strong> Kraft Inequality</a></li><li class="chapter-item expanded "><a href="lossless_iid/entropy.html"><strong aria-hidden="true">1.4.</strong> Entropy and Neg-log likelihood thumb rule</a></li><li class="chapter-item expanded "><a href="lossless_iid/huffman.html"><strong aria-hidden="true">1.5.</strong> Huffman coding</a></li><li class="chapter-item expanded "><a href="lossless_iid/aep.html"><strong aria-hidden="true">1.6.</strong> Asymptotic Equipartition Property</a></li><li class="chapter-item expanded "><a href="lossless_iid/arithmetic_coding.html"><strong aria-hidden="true">1.7.</strong> Arithmetic coding</a></li><li class="chapter-item expanded "><a href="lossless_iid/ans.html"><strong aria-hidden="true">1.8.</strong> Asymmetric Numeral Systems</a></li><li class="chapter-item expanded "><a href="lossless_iid/non_iid_sources.html"><strong aria-hidden="true">1.9.</strong> Non IID Sources and Entropy Rate</a></li><li class="chapter-item expanded "><a href="lossless_iid/context_based_coding.html"><strong aria-hidden="true">1.10.</strong> Context-based coding</a></li><li class="chapter-item expanded "><a href="lossless_iid/lz77.html"><strong aria-hidden="true">1.11.</strong> Universal Compression with LZ77</a></li><li class="chapter-item expanded "><a href="lossless_iid/practical_tips.html"><strong aria-hidden="true">1.12.</strong> Practical Tips on Lossless Compression</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="lossy/coverpage.html"><strong aria-hidden="true">2.</strong> Lossy data Compression</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lossy/quant.html"><strong aria-hidden="true">2.1.</strong> Basics and Quantization</a></li><li class="chapter-item expanded "><a href="lossy/rd.html"><strong aria-hidden="true">2.2.</strong> Rate-Distortion Theory</a></li><li class="chapter-item expanded "><a href="lossy/transform_coding_theory.html"><strong aria-hidden="true">2.3.</strong> Transform Coding Theory</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="resources.html"><strong aria-hidden="true">3.</strong> Resources</a></li><li class="chapter-item expanded affix "><li class="spacer"></li><li class="chapter-item expanded "><a href="homeworks/coverpage.html"><strong aria-hidden="true">4.</strong> Homeworks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="homeworks/HW1.html"><strong aria-hidden="true">4.1.</strong> HW1</a></li><li class="chapter-item expanded "><a href="homeworks/HW1_sol.html"><strong aria-hidden="true">4.2.</strong> HW1 Solution</a></li><li class="chapter-item expanded "><a href="homeworks/HW2.html"><strong aria-hidden="true">4.3.</strong> HW2</a></li><li class="chapter-item expanded "><a href="homeworks/HW2_sol.html"><strong aria-hidden="true">4.4.</strong> HW2 Solution</a></li><li class="chapter-item expanded "><a href="homeworks/HW3.html"><strong aria-hidden="true">4.5.</strong> HW3</a></li><li class="chapter-item expanded "><a href="homeworks/HW3_sol.html"><strong aria-hidden="true">4.6.</strong> HW3 Solution</a></li><li class="chapter-item expanded "><a href="homeworks/HW4.html"><strong aria-hidden="true">4.7.</strong> HW4</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="projects.html"><strong aria-hidden="true">5.</strong> Project</a></li><li class="chapter-item expanded affix "><li class="spacer"></li><li class="chapter-item expanded "><a href="scl_tutorial/SCL_tutorial.html"><strong aria-hidden="true">6.</strong> SCL Tutorial</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="scl_tutorial/basics.html"><strong aria-hidden="true">6.1.</strong> SCL Basics</a></li><li class="chapter-item expanded "><a href="scl_tutorial/exercise.html"><strong aria-hidden="true">6.2.</strong> SCL Exercise</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><a href="quiz_problems_2023.html"><strong aria-hidden="true">7.</strong> Quiz Problems (2023)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="ee274-data-compression-course-notes"><a class="header" href="#ee274-data-compression-course-notes">EE274: Data Compression, course notes</a></h1>
<p>Welcome! This e-book serves as lecture notes for the Stanford EE course, <a href="https://stanforddatacompressionclass.github.io/"><strong>EE274: Data Compression</strong></a>. This set of lecture notes is WIP, so please file an issue at <a href="https://github.com/stanfordDataCompressionClass/notes/issues">https://github.com/stanfordDataCompressionClass/notes/issues</a> if you find any typo/mistake.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lossless-data-compression"><a class="header" href="#lossless-data-compression">Lossless data compression</a></h1>
<p>The first part of the lecture notes pertain with lossless data compression</p>
<ol>
<li><a href="lossless_iid/./intro.html">Introduction</a></li>
<li><a href="lossless_iid/./prefix_free_codes.html">Prefix Free Codes</a></li>
<li><a href="lossless_iid/./kraft_ineq_and_optimality.html">Kraft Inequality</a></li>
<li><a href="lossless_iid/./entropy.html">Entropy and Neg-log likelihood thumb rule</a></li>
<li><a href="lossless_iid/./huffman.html">Huffman coding</a></li>
<li><a href="lossless_iid/./aep.html">Asymptotic Equipartition Property</a></li>
<li><a href="lossless_iid/./arithmetic_coding.html">Arithmetic coding</a></li>
<li><a href="lossless_iid/./ans.html">Asymmetric Numeral Systems</a></li>
<li><a href="lossless_iid/./non_iid_sources.html">Non IID Sources and Entropy Rate</a></li>
<li><a href="lossless_iid/./context_based_coding.html">Context-based coding</a></li>
<li><a href="lossless_iid/./lz77.html">Universal Compression with LZ77</a></li>
<li><a href="lossless_iid/./practical_tips.html">Practical Tips on Lossless Compression</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-to-lossless-coding"><a class="header" href="#introduction-to-lossless-coding">Introduction to lossless coding</a></h1>
<p>Data compression, is one area of EECS which we are aware of and unknowingly make use of thousands of times a day.
But, not many engineers are aware of how things work <em>under the hood</em>.
Each text, image and video is compressed before it gets transmitted and shared across devices using compression
schemes/formats such as <code>GZIP, JPEG, PNG, H264 ...</code> and so on. A lot of these compression schemes are somewhat domain
specific, for example typically use <code>GZIP</code> for text files, while we have <code>JPEG</code> for images.
While there is significant domain expertise involved in designing these schemes,
there are lots of commonalities and fundamental similarities.</p>
<p>Our focus in the first half of the course is going to be to understand these common techniques part of all of these
compression techniques, irrespective of the domain. The goal is to also build an <em>intuition</em> for these fundamental
ideas, so that we can appropriately tweak them when eventually designing real-life and domain-specific compression
techniques. With that in mind, we are going to start with very simple hypothetical but relevant scenarios, and try
to tackle these. As we will see, understanding these simpler cases will help us understand the fundamental details of
compression.</p>
<p>Let's start with a simple example:</p>
<blockquote>
<p>Let's say we are given a large random text file consisting of <code>4</code> letters $A, B, C, D$. To be more precise, each
letter
of the text file is generated randomly by picking a letter from the set $\mathcal{S} = {A, B, C, D} $ independently
and with uniform probability. i.e.</p>
</blockquote>
<p>$$P(A) = P(B) = P(C) = P(D) = 1/4 $$</p>
<p>An example of such a file is given below. Let's say we have such a file containing $1,000,000$ letters.</p>
<pre><code>ACABDADCBDDCABCDADCBDCADCBDACDACDBAAAAACDACDA...
</code></pre>
<p>Now if one examines this file, then the size of the file is going to be <code>1,000,000</code> bytes, as can be seen below.</p>
<pre><code>âžœ ls -l sample_ABCD_uniform.txt 
-rw-rw-r-- 1 kedar kedar 1000000 Jun 23 19:27 sample_ABCD_uniform.txt
</code></pre>
<p>This is quite obvious as we are using 1 byte (or <code>8 bits/symbol</code>) to represent each alphabet. In fact, if we look at the
bits in the file, we see that each symbol has a unique <em>codeword</em>, given by the ASCII table:</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>ASCII code</th></tr></thead><tbody>
<tr><td>A</td><td>1000001</td></tr>
<tr><td>B</td><td>1000010</td></tr>
<tr><td>C</td><td>1000011</td></tr>
<tr><td>D</td><td>1000100</td></tr>
</tbody></table>
</div>
<p>Every time we write text to a file, the programming language <em>encodes</em> the text file using ASCII encoding table -- to
map the characters and other punctuations to binary data. Also, when we read the file back, the binary data is
read <code>8 bits</code> at a time and then <em>decoded</em> back to the character.</p>
<p>Now, the question we want to answer is "<em>Can we reduce the size of the file?</em>"</p>
<p>To be more precise, we will restrict ourselves to <em>Symbol codes</em>, which are encoding schemes where we have a unique
binary string (called a <em>codeword</em>) for every symbol.</p>
<div id="admonition-quiz-1-fixed-bitwidth-encoding" class="admonition admonish-question" role="note" aria-labelledby="admonition-quiz-1-fixed-bitwidth-encoding-title">
<div class="admonition-title">
<div id="admonition-quiz-1-fixed-bitwidth-encoding-title">
<p>Quiz-1: Fixed Bitwidth Encoding</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/intro.html#admonition-quiz-1-fixed-bitwidth-encoding"></a>
</div>
<div>
<p>For the example above where our file contains random letters from the set $\mathcal{S} = {A, B, C, D}$,
the ASCII code gives us a compression of <code>8 bits/symbol</code>. Can we design a <em>symbol code</em> which
achieves better compression and is <em>lossless</em>, i.e. a <em>lossless compression</em> scheme?</p>
</div>
</div>
<h2 id="fixed-bitwidth-encoding"><a class="header" href="#fixed-bitwidth-encoding">Fixed Bitwidth encoding</a></h2>
<p>If we look at the ASCII code table, we see that all the codes start <code>1000</code>, which is kind of redundant. It is useful if
we have other letters in the file, but in this case we only have 4 letters. Thus, we don't need all the 8-bits! In
fact, we can just use <code>2 bits/symbol</code> to represent them all as follows:</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>ASCII code</th></tr></thead><tbody>
<tr><td>A</td><td>00</td></tr>
<tr><td>B</td><td>01</td></tr>
<tr><td>C</td><td>10</td></tr>
<tr><td>D</td><td>11</td></tr>
</tbody></table>
</div>
<p>Note that this is lossless compression, as during decoding as well we can just read <code>2</code> bits at a time and map them
to <code>A,B,C,D</code>. Thus, this simple idea has given us a <em>compression</em> of <strong>4x</strong>!</p>
<p>In general, if we have $k = |\mathcal{S}|$ different symbols in our text file, then using this simple idea we can
represent/<em>encode</em> the data using $\lceil \log_2 k \rceil$ bits.</p>
<h2 id="going-beyond-fixed-bitwidth-encoding"><a class="header" href="#going-beyond-fixed-bitwidth-encoding">Going beyond Fixed Bitwidth encoding</a></h2>
<p>Now, this <code>2 bits/symbol</code> compression is all well and good, but works only if some letters are fully absent from our
text file. For example, if now our file the letters $C, D$ were absent and only the letters ${A,B}$ were present, then
we could go from <code>2 bits/symbol</code> to <code>1 bit/symbol</code>, by assigning codes a follows:</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>ASCII code</th></tr></thead><tbody>
<tr><td>A</td><td>0</td></tr>
<tr><td>B</td><td>1</td></tr>
</tbody></table>
</div>
<p>But, what if $C,D$ were present, but in a smaller proportion? Is it possible to achieve better compression?
Here is a more precise question:</p>
<div id="admonition-quiz-2-beyond-fixed-bitwidth-encoding" class="admonition admonish-question" role="note" aria-labelledby="admonition-quiz-2-beyond-fixed-bitwidth-encoding-title">
<div class="admonition-title">
<div id="admonition-quiz-2-beyond-fixed-bitwidth-encoding-title">
<p>Quiz-2: Beyond Fixed Bitwidth encoding</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/intro.html#admonition-quiz-2-beyond-fixed-bitwidth-encoding"></a>
</div>
<div>
<p>Lets say that our random text file contains the letters $S = {A, B, C, D }$, such that each letter is sampled
independently with probability: $$P(A) = 0.49, P(B) = 0.49, P(C) = P(D) = 0.01 $$
Then can we design a <em>symbol code</em> which achieves lossless compression better than <code>2 bits/symbol</code>?</p>
</div>
</div>
<p>Clearly, if we stick with fixed bitwidth encoding, then irrespective of the distribution of letters in our file, we are
always going to get compression of <code>2 bits/symbol</code> for all the <code>4</code> symbols, and so of course we can't achieve
compression better than <code>2 bits/symbol</code>. But, intuitively we know that most of the time we are only going to encounter
symbol $A$ or $B$, and hence we should be able to compress our file closer to <code>1 bit/symbol</code>. To improve compression,
the idea here is to go for <em>variable length codes</em>, where the code length of each symbol can be different.</p>
<p>But, designing <em>variable length codes</em> which are also <em>lossless</em> is a bit tricky. In the fixed bitwidth scenario, the
decoding was simple: we always looked at <code>2 bits</code> at a time, and just performed a reverse mapping using the code table
we had. In case of <em>variable length codes</em>, we don't have that luxury as we don't really know how many bits does the
next codeword consists of.</p>
<p>Thus, the <em>variable length codes</em> need to have some special properties which makes it both lossless and also
convenient to decode.</p>
<p>Instead of just mentioning the property, we are going to try to discover and reverse engineer this property! With that
in mind, I am going to present the solution to the <strong>Quiz-2</strong>, but what you have to do is figure out the decoding
procedure! So here is one possible code:</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>codewords</th></tr></thead><tbody>
<tr><td>A</td><td>0</td></tr>
<tr><td>B</td><td>10</td></tr>
<tr><td>C</td><td>110</td></tr>
<tr><td>D</td><td>111</td></tr>
</tbody></table>
</div>
<p>Some questions for you to answer:</p>
<div id="admonition-quiz-3-variable-length-symbol-code" class="admonition admonish-question" role="note" aria-labelledby="admonition-quiz-3-variable-length-symbol-code-title">
<div class="admonition-title">
<div id="admonition-quiz-3-variable-length-symbol-code-title">
<p>Quiz-3: Variable length symbol code</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/intro.html#admonition-quiz-3-variable-length-symbol-code"></a>
</div>
<div>
<p>For the variable code above, answer the following questions:</p>
<ol>
<li>Are we doing better than <code>2 bits/symbol</code> in terms of the average compression?</li>
<li>Is the code really lossless?</li>
<li>How can we perform the decoding?</li>
</ol>
</div>
</div>
<p>Even before we try to answer these questions, lets try to first eyeball the designed code and see if we can
understand something! We see that the code for $A$ is now just <code>1</code> bit instead of <code>2</code>. At the same time, the code
lengths for $C,D$ are <code>3</code> instead of <code>2</code>. So, in a way, we are reducing the code length for symbols which are more
frequent, while increasing the code length for symbols which occur less frequently, which intuitively makes sense if
we want to reduce the average code length of our file!</p>
<p><strong>Compression ratio</strong>: Lets try to answer the first question: "<em>Are we doing better than <code>2 bits/symbol</code> in terms of the
average compression?</em>" To do this, we are going to compute $\mathbb{E}(L)$, the average code length of the code:</p>
<p>$$ \begin{aligned}
\mathbb{E}(L) &amp;= p(A)<em>1 + p(B)<em>2 + p(C)<em>3 + p(4)<em>3 \
&amp;= 0.49</em>1 + 0.49</em>2 + 0.01</em>3 + 0.01</em>3 \
&amp;= 1.53
\end{aligned} $$</p>
<p>So, although in the worst case this increases the symbol code length from <code>2</code> to <code>3</code>, we have been able to reduce the
average code length from <code>2</code> to <code>1.53</code>, which although is not as good as <code>1 bit/symbol</code> we see when $C, D$ were
completely absent, it is still an improvement from <code>2 bits/symbol</code>. Great!</p>
<p><strong>Lossless Decoding</strong>: Now that we have answered the first question, lets move to the second one: "<em>Is the code really
lossless?</em>"</p>
<p>Okay, lets get back to the code we have:</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>codewords</th></tr></thead><tbody>
<tr><td>A</td><td>0</td></tr>
<tr><td>B</td><td>10</td></tr>
<tr><td>C</td><td>110</td></tr>
<tr><td>D</td><td>111</td></tr>
</tbody></table>
</div>
<p>Let's take an example string and see how the encoding/decoding proceeds:</p>
<pre><code>input_string = ABCADBBA
</code></pre>
<p>The encoding proceeds in the same way as earlier: for every symbol, we look at the lookup table and write the
corresponding bits to the file. So, the encoding for our sample string looks like:</p>
<pre><code class="language-python">input_string = ABCADBBA
encoding = concat(0, 10, 110, 0, 111, 10, 10, 0)
= 010110011110100
</code></pre>
<p>Now, lets think about how we can decode this bit sequence: <code>010110011110100</code>. The issue with using a <em>variable length
code</em>, unlike before where we used <code>2</code> bits for each symbol is that we don't know where each code ends beforehand. For
example, we don't know how to decode symbol no. <code>7</code> directly. but let's see how we can decode the input sequentially,
one symbol at a time.</p>
<ol>
<li>
<p>The first bit of <code>010110011110100</code> is <code>0</code>. We see that only one codeword (corresponding to $A$) starts with <code>0</code>, so
we can be confident that the first letter was an $A$! At this time we have consumed <code>1</code> bit, and so to decode the
second symbol we have to start reading from the second bit.</p>
</li>
<li>
<p>Let's now focus our attention on the remaining bits: <code>10110011110100</code>. Now, the first bit is a <code>1</code>. Unfortunately
$B,C,D$ have codewords which start with a <code>1</code>, so lets read another bit: <code>10</code>. Okay! we have a match with the symbol
$B$. We can also be confident that the second symbol is indeed $B$, as no other codeword begins with <code>10</code>. Thus, our
decoded string until now is <code>A,B</code></p>
</li>
<li>
<p>We can continue in the same way and decode the remaining symbols one at a time, by following the following procedure,
to recover the input string: <code>ABCADBBA</code>.</p>
</li>
</ol>
<p>Below is a pseudocode to decode a single symbol using the matching scheme described above.</p>
<pre><code class="language-python">codewords_table = {A: 0, B: 10, C: 110, D: 111}


def decode_symbol(encoded_bitstring, codewords_table):
    temp_bits = ""

    # read until we find a match with a codeword in the codewords_table
    while not find_match(temp_bits, codewords_table):
        temp_bits += encoded_bitstring.read_next_bit()

    decoded_symbol = find_match(temp_bits, codewords_table)
    return decoded_symbol
</code></pre>
<p>Note, that one reason why our decoding procedure works is because no codeword is an initial segment of another codeword,
i.e. no two codewords are prefixes of each other. Thus, if
codeword for $B$ is <code>10</code>, then no other codeword can begin with <code>10</code>. This kind of <em>Symbol codes</em> are known as
<strong>prefix-free codes</strong> (or simply <strong>Prefix Codes</strong>). As we just saw, we can confidently say that if a code is *
prefix-free*, then it indeed leads to lossless compression, and a convenient decoding procedure.</p>
<p>Note that <em>prefix-free codes</em> aren't the only way of designing <em>symbol codes</em> which are lossless. For example, consider
the following code</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>codewords</th></tr></thead><tbody>
<tr><td>A</td><td>0</td></tr>
<tr><td>B</td><td>01</td></tr>
<tr><td>C</td><td>011</td></tr>
<tr><td>D</td><td>111</td></tr>
</tbody></table>
</div>
<p>Notice that codeword for $C$ starts with <code>01</code> which is in fact the codeword for $B$ (similarly for $B$ and $A$). Thus,
this code is not <em>prefix-free</em> and our decoding procedure described above will fail. For example if we encode a $C$
-&gt; <code>011...</code>, our decoding procedure will always output a $B$, as it would find a match at <code>01</code> itself. Note that
although our simple decoding scheme above fails, lossless decoding is still possible. Although, the decoding scheme
might be more complicated as, we have to read "future bits" to determine if a $B$ or a $C$ was encoded here.</p>
<p>As a fun exercise, can you try to figure out how the decoding works for this code?</p>
<div id="admonition-quiz-4-non-prefix-free-code-example" class="admonition admonish-question" role="note" aria-labelledby="admonition-quiz-4-non-prefix-free-code-example-title">
<div class="admonition-title">
<div id="admonition-quiz-4-non-prefix-free-code-example-title">
<p>Quiz-4: Non-prefix free code example</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/intro.html#admonition-quiz-4-non-prefix-free-code-example"></a>
</div>
<div>
<p>Can you think of a decoding procedure which works for this code? (<em>Hint: It sometimes helps to think in the
reverse direction</em>)</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>codewords</th></tr></thead><tbody>
<tr><td>A</td><td>0</td></tr>
<tr><td>B</td><td>01</td></tr>
<tr><td>C</td><td>011</td></tr>
<tr><td>D</td><td>111</td></tr>
</tbody></table>
</div></div>
</div>
<p>To summarize: the key idea to learn from the design of our code is that</p>
<div id="admonition-key-idea-1-prefix-free-codes" class="admonition admonish-info" role="note" aria-labelledby="admonition-key-idea-1-prefix-free-codes-title">
<div class="admonition-title">
<div id="admonition-key-idea-1-prefix-free-codes-title">
<p>Key-Idea 1: Prefix-free codes</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/intro.html#admonition-key-idea-1-prefix-free-codes"></a>
</div>
<div>
<p>If no codeword in a <em>symbol code</em> is a prefix of each other, then the <em>symbol code</em> is a <em>prefix-free code</em>.</p>
<p>Prefix-free codes by design are lossless compression schemes which allow for a convenient decoding scheme.</p>
</div>
</div>
<p>Another name for <em>prefix-free</em> codes is <em>instantaneous codes</em> as the decoding procedure is in a sense "instantaneous",
i.e. we don't have to read bits written by future symbols to decode the current symbols.</p>
<h2 id="next-lecture"><a class="header" href="#next-lecture">Next Lecture</a></h2>
<p>In this lecture, we discussed the design of a simple <em>prefix-free code</em> which improves upon the compression of a
non-uniform source from <code>2 bits/symbol</code> to <code>1.53 bits/symbol</code> by using variable length codewords. We also briefly
discussed how the decoding might proceed.</p>
<p>In the next lecture we will take this discussion further and try to answer the following questions:</p>
<ol>
<li>
<p><strong>Implementing the decoding</strong>: We will see how to transform our simple decoding procedure into an efficient
algorithm, which we can implement in practice</p>
</li>
<li>
<p><strong>Designing prefix-free codes</strong>: We will discuss how to design prefix-free codes to obtain better compression.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prefix-free-codes"><a class="header" href="#prefix-free-codes">Prefix(-Free) Codes</a></h1>
<p>As a recap, in the previous lecture, we discussed a simple example of a prefix-free code. We also discussed a simple
procedure for decoding data encoded using the prefix-free code. In this lecture we will be thinking about how to
actually go about implementing the decoding, and also how to design prefix-free code itself.</p>
<p>Lets start by thinking about the decoding. As a recap, here is our prefix-free code and the decoding procedure</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>Codewords</th></tr></thead><tbody>
<tr><td>A</td><td>0</td></tr>
<tr><td>B</td><td>10</td></tr>
<tr><td>C</td><td>110</td></tr>
<tr><td>D</td><td>111</td></tr>
</tbody></table>
</div>
<pre><code class="language-python">### Prefix-free decoding procedure
codewords_table = {A: 0, B: 10, C: 110, D: 111}

def decode_symbol(encoded_bitstring, codewords_table):
    temp_bits = ""

    # read until we find a match with a codeword in the codewords_table
    while not find_match(temp_bits, codewords_table):
        temp_bits += encoded_bitstring.read_next_bit()

    decoded_symbol = find_match(temp_bits, codewords_table)
    return decoded_symbol
</code></pre>
<p>The procedure is as follows:</p>
<ul>
<li>We start from the encoded bitstream, and try to find a match with either of the codewords. We stop when we get our
first match</li>
<li>Note that the decoding procedure above is "correct" because we have a <em>prefix-free</em> code, and so we can stop searching
when <code>find_match</code> obtains its first match.</li>
</ul>
<p>Let's think about how we can implement the <code>find_match</code> function.</p>
<p><strong>Idea-1: Hash-tables</strong>: One simple way to implement the <code>find_match</code> function in the decoding is via a hash table. We
can just create a hash table using the <code>codewords_table</code>. We can then just query the hash table with the <code>temp_bits</code>,
until we get a match.</p>
<p>Can we think of a simple data-structure which will make this procedure faster?</p>
<h2 id="prefix-free-tree"><a class="header" href="#prefix-free-tree">Prefix-free Tree</a></h2>
<p>Another approach towards prefix-free code decoding is to construct a <em>prefix-free tree</em> and use this tree for decoding.
Lets get back to our <code>prefix-free</code> code example to see what I mean by that:</p>
<p>Given the codewords table, we can represent it as a binary tree follows:</p>
<pre class="mermaid">graph TD
  *(Root) --&gt;|0| A:::endnode
  *(Root) --&gt;|1| n1(.)
  n1 --&gt;|10| B:::endnode
  n1 --&gt;|11| n2(.)
  n2 --&gt;|110| C:::endnode
  n2 --&gt;|111| D:::endnode
</pre>
<ol>
<li>The idea behind the <em>prefix-free tree</em> construction is simple. For each codeword, we add a node at depth <code>len(codeword)</code> from root node, taking the right path or the left path from previous node in the tree depending on whether the corresponding bit in the codeword is <code>1</code> or <code>0</code>. E.g. in our codewords table <code>B -&gt; 10</code>. In this case we add a node to the binary tree at depth <code>2 = len(10)</code>, corresponding to the path <code>10 -&gt; right, left</code> from the root node. Similarly, for <code>C -&gt; 110</code>, we add a node at depth <code>3</code> corresponding to the path <code>110 -&gt; right, right, left</code>.</li>
<li>Notice that for <em>prefix-free</em> codes, the codewords correspond to the leaf nodes. This can be shown using contradiction. If there was another node <code>n2</code>
corresponding to codeword <code>c2</code> sprouting out of node <code>n1</code> (with codeword <code>c1</code>), then based on the construction of <em>prefix-free tree</em> defined in the previous step, <code>c1</code> is a prefix of <code>c2</code>. This is a contradiction because we're violating the <code>prefix-free</code> property of our code. In fact, the property that prefix-free codes correspond to the leaf nodes of <em>prefix-free tree</em> is another way to define <em>prefix-free codes</em>!</li>
</ol>
<p>Okay, now that we have understood how to create the <em>prefix-free tree</em> data structure for any <em>prefix-free code</em>, can we
use this tree structure to improve the decoding algorithm? Take a look!</p>
<pre><code class="language-python">## Prefix-free code decoding using a prefix-free tree

prefix_tree = ...  # construct the tree here using codewords

def decode_symbol(encoded_bitarray, prefix_tree):
    # start from the root node and read bits until you read a leaf node
    node = prefix_tree.root
    while not node.is_leaf_node:
        bit = encoded_bitarray.read_next_bit()
        node = node.right if bit else node.left
    # return the symbol corresponding to the node, once we reached a leaf node
    return node.symbol
</code></pre>
<p>Some observations:</p>
<ol>
<li>This decoding scheme is similar in logic to the previous one, but quite a bit more efficient, as we are not querying
the hash table multiple times.</li>
<li>The key idea is that for <em>prefix-free codes</em> the codewords correspond to leaf nodes. Hence, we just parse the tree
with the bits from the output until we reach one of the leaf nodes.</li>
<li>Note that we need to create the <em>prefix-free tree</em> during the pre-processing step of our decoding as a one-time cost,
which typically gets amortized if the data we are encoding is quite big.</li>
</ol>
<p>As we will see later the <em>prefix-free tree</em> is not just useful for efficient decoding, but is a great way to think and
visualize <em>prefix-free codes</em>. We will in fact use the structure when we learn about how to design a good prefix-free
code.</p>
<h2 id="how-to-design-good-prefix-free-codes"><a class="header" href="#how-to-design-good-prefix-free-codes">How to design good <em>prefix-free codes</em>?</a></h2>
<p>Okay, now that we have convinced that prefix-free codes are indeed lossless and that they have an efficient decoding
scheme, lets revisit our code and think again about <em>why the scheme leads to better compression</em></p>
<p>To recap: we wanted to design a code for the skewed non-uniform distribution: $$ P(A) = 0.49, P(B) = 0.49, P(C) = P(D) =
0.01$$</p>
<p>we started with the following <em>Fixed Bitwidth code</em>.</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>Codewords</th></tr></thead><tbody>
<tr><td>A</td><td>00</td></tr>
<tr><td>B</td><td>01</td></tr>
<tr><td>C</td><td>10</td></tr>
<tr><td>D</td><td>11</td></tr>
</tbody></table>
</div>
<p>Then we were able to construct a <em>variable-length code</em>, which is indeed lossless and improves the compression
from <code>2 bits/symbol</code> to <code>1.53 bits/symbol</code> on an average.</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>Codewords</th></tr></thead><tbody>
<tr><td>A</td><td>0</td></tr>
<tr><td>B</td><td>10</td></tr>
<tr><td>C</td><td>110</td></tr>
<tr><td>D</td><td>111</td></tr>
</tbody></table>
</div>
<p>How did this improvement come about?</p>
<ol>
<li>
<p>The <em>Fixed Bitwidth code</em> is assigning equal importance to all the symbols $A, B, C, D$, as the code-lengths are the
same ( <code>2 bits/symbol</code>). This seems all good, in case they are equiprobable. i.e. if the probabilities are:
$$ p(A) = p(B) = p(C) = p(D) = 0.25 $$</p>
</li>
<li>
<p>But in case of the skewed probability distribution, clearly $A, B$ are more important as they occur much more
frequently. So, we should try to assign a shorter codeword to $A, B$ and could afford to assign longer codewords to
$C,D$ as they occur much less frequently.</p>
</li>
</ol>
<p>Thus we have a simple and somewhat obvious thumb rule for a code:</p>
<div id="admonition-key-idea-2" class="admonition admonish-info" role="note" aria-labelledby="admonition-key-idea-2-title">
<div class="admonition-title">
<div id="admonition-key-idea-2-title">
<p>Key-Idea 2</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/prefix_free_codes.html#admonition-key-idea-2"></a>
</div>
<div>
<p>Given a distribution, it is better (in terms of compression) to assign shorter codewords to symbols with higher probability.</p>
<p>$$ p(s_1) &gt; p(s_2) =&gt; l(s_1) &lt;= l(s_2)$$</p>
</div>
</div>
<p>Even though we want to assign shorter codes to symbols with higher probability, it is not clear what the proportionality
should be.</p>
<p>For example, the <em>prefix-free code</em> we have works well for distribution $$ \begin{aligned}
p(A) = p(B) = 0.49 \ p(C) = p(D) = 0.01
\end{aligned} $$ and gives us the average codelength of <code>1.53 bits/symbol</code>. But, the same code doesn't work well for a less
skewed distribution like:</p>
<p>$$ \begin{aligned}
p(A) = p(B) = 0.3 \ p(C) = p(D) = 0.2
\end{aligned} $$
as in this case the average codelength is <code>2.1 bits/symbol</code> (which is even higher than <em>Fixed Bitwidth code</em>!).</p>
<p>This problem, (and much more) was definitively analyzed in the brilliant work by Claude Shannon in his 1948 paper <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">A Mathematical theory of Communication</a>. Shannon's work laid the theoretical foundations of not just the field of data compression, but more so of the area of
error-free data communication. (The paper remains surprisingly accessible and short for its legendary nature. Do take a look!)</p>
<p>We will definitely come back in the future lectures to understand the work in more detail, but here is another thumb rule from Shannon's work for the optimal code-lengths:</p>
<p>$$ l_{optimal}(symbol) \approx \log_2 \frac{1}{p(symbol)} $$</p>
<p>Note that this is still a thumb rule, until we <em>prove</em> it and show convincingly in which scenarios it holds true. But,
lets try to verify this rule in a couple of simple cases:</p>
<ol>
<li>
<p><strong>Uniform Distribution</strong>: Let's consider the case of $$P(A) = P(B) = P(C) = P(D) = 1/4$$
We know that as we have 4 equiprobable symbols, we can encode data using <code>2 bits/symbol</code>. This matches the thumb rule
above: $$
\begin{aligned}
l_{optimal}(symbol) &amp;\approx \log_2 \frac{1}{p(symbol)} \
&amp;= \log_2 4 \
&amp;= 2
\end{aligned} $$
In general, we see that if the probability is $1/k$ for all the $k$ symbols of a distribution, then the optimal codelength is
going to be $log_2 k$, according to the thumb rule. Now, in cases where $k$ is not a power of $2$, and if we have a
single unique codeword per symbol, then the best we can achieve is $$ \lceil log_2 k \rceil \geq log_2 k $$</p>
</li>
<li>
<p><strong>Dyadic distribution</strong>: Based on the thumb rule, we need $\log_2 \frac{1}{p(symbol)}$ to be an integer, which
is only possible if $p(symbol)$ is a negative power of $2$ for all symbols. Such distributions are known as <em>dyadic
distributions</em></p>
<blockquote>
<p><strong>Dyadic distribution</strong> A distribution $P$ is called dyadic if $$ \exists l_i \in \mathbb{N}, P(s_i) = \frac{1}{2^{l_i}}, \forall s_i \in \mathcal{S}$$</p>
</blockquote>
<p>Based on the thumb rule, it is unambiguously clear that for dyadic distribution, the symbol $s_i$ should have codeword of length $l_i$. For
example: for the following dyadic distribution $$P(A) = 1/2, p(B) = 1/4, P(C) = P(D) = 1/8$$ the code-lengths are going
to be:</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>Prob</th><th>Optimal codelength</th></tr></thead><tbody>
<tr><td>A</td><td>1/2</td><td>1</td></tr>
<tr><td>B</td><td>1/4</td><td>2</td></tr>
<tr><td>C</td><td>1/8</td><td>3</td></tr>
<tr><td>D</td><td>1/8</td><td>3</td></tr>
</tbody></table>
</div></li>
<li>
<p><strong>General Distributions</strong>: For generic distributions, $l_{optimal}(symbol) = \log_2 \frac{1}{p(symbol)}$, might not
be achievable in general. But, a good goal to aim for is: $$ l(symbol) = \left\lceil \log_2 \frac{1}{p(symbol)}
\right\rceil $$ Is this possible? Lets look at this problem in the next section</p>
</li>
</ol>
<h2 id="designing-prefix-free-codes"><a class="header" href="#designing-prefix-free-codes">Designing prefix-free codes</a></h2>
<p>Okay! So, now that we know that the code-lengths to shoot for are: $$ l(symbol) = \left\lceil \log_2 \frac{1}{p(symbol)}
\right\rceil $$ let's try to think how.</p>
<p>Let's take a simple example (see below) and see if we can try to come up with a prefix-free code with the prescribed lengths.</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>Prob</th><th>Optimal codelength</th></tr></thead><tbody>
<tr><td>A</td><td>0.55</td><td>1</td></tr>
<tr><td>B</td><td>0.25</td><td>2</td></tr>
<tr><td>C</td><td>0.1</td><td>4</td></tr>
<tr><td>D</td><td>0.1</td><td>4</td></tr>
</tbody></table>
</div>
<p>We know that all prefix-free codes have a corresponding <em>prefix-free tree</em>. So, essentially we want to come up with a
binary tree with the leaf nodes at a distance equal to the code-lengths from the root node. For example, in the above
example, we want to construct a binary tree with leaf nodes at distance <code>1, 2, 4, 4</code> from the root node. Note that there
can be some additional leaf nodes to the binary tree which are not assigned to any codeword.</p>
<ol>
<li>Let's start with nodes at distance <code>1</code>. We know a binary tree has <code>2^1 = 2</code> nodes at distance <code>1</code> from the root node.
These correspond to codewords <code>0</code> (left node) and <code>1</code> (right node). Let's make the left node a leaf node corresponding
to symbol $A$ (with codelength = <code>1</code> as needed)</li>
</ol>
<pre class="mermaid">graph TD
     *(Root) --&gt;|0| A:::endnode
     *(Root) --&gt;|1| n1(.)
</pre>
<ol start="2">
<li>Now we have the right node (corresponding to codeword <code>1</code>), which we can split further into two nodes corresponding
to <code>10</code> and <code>11</code>. As we are needed to assign a codeword with length <code>2</code>, lets assign node corresponding to <code>10</code> to
symbol $B$, and make it a leaf node.</li>
</ol>
<pre class="mermaid">graph TD
     *(Root) --&gt;|0| A:::endnode
     *(Root) --&gt;|1| n1(.)
     n1 --&gt;|10| B:::endnode
     n1 --&gt;|11| n2(.)
</pre>
<ol start="3">
<li>We are now again left with the right node <code>11</code>, which we can split further into two nodes <code>110, 111</code> at distance
= <code>3</code> from the root node. Now, looking at the table of code-lengths, we do not have any code to be assigned to
length <code>3</code>, so lets split node <code>110</code> further into nodes corresponding to <code>1100</code> and <code>1101</code>. We can now assign these
two nodes to symbols $C, D$ respectively.</li>
</ol>
<pre class="mermaid">graph TD
     *(Root) --&gt;|0| A:::endnode
     *(Root) --&gt;|1| n1(.)
     n1 --&gt;|10| B:::endnode
     n1 --&gt;|11| n2(.)
     n2 --&gt; |110| n3(.)
     n2 --&gt; |111| n4(.)
     n3 --&gt; |1100| C:::endnode
     n3 --&gt; |1101| D:::endnode
</pre>
<p>Thus, our final codewords are:</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>Prob</th><th>Optimal codelength</th><th>codewords</th></tr></thead><tbody>
<tr><td>A</td><td>0.55</td><td>1</td><td>0</td></tr>
<tr><td>B</td><td>0.25</td><td>2</td><td>10</td></tr>
<tr><td>C</td><td>0.1</td><td>4</td><td>1100</td></tr>
<tr><td>D</td><td>0.1</td><td>4</td><td>1101</td></tr>
</tbody></table>
</div>
<p>Notice that the leaf node corresponding to <code>111</code> was not linked to any symbol. That is absolutely ok! It just implies
that we could have squeezed in another symbol here with length <code>3</code>, and points to the slight inefficiency in using the
approximation $ l(symbol) = \left\lceil \log_2 \frac{1}{p(symbol)} \right\rceil $.</p>
<p>The procedure we followed seems general enough that we can follow it for any distribution:</p>
<ol>
<li>Sort the distribution in descending order and compute codelengths as $ l(symbol) = \left\lceil \log_2 \frac{1}{p(symbol)} \right\rceil $</li>
<li>Assign the first lexicographically available codeword to the next symbol which is not a prefix of the previous ones and is of the stipulated length.</li>
</ol>
<p>We still need to argue the
correctness of the procedure and that it will work in all cases! For example, how can we be sure that we will not run
out of nodes?</p>
<p>We will look at all these problems in the next lecture.</p>
<h2 id="summary--next-lecture"><a class="header" href="#summary--next-lecture">Summary &amp; Next Lecture</a></h2>
<p>To summarize: here are the key takeaway's for this lecture:</p>
<ol>
<li>
<p><strong>Prefix-free codes</strong>: Among the symbol codes, <em>prefix-free</em> codes allow for efficient and convenient
encoding/decoding. In fact as we will see, given any <em>uniquely-decodable</em> code (or <em>lossless</em> code), we can construct
a <em>prefix-free</em> code with exact same code-lengths and thus the same compression performance. Thus, we restrict
ourselves to the analysis of <em>prefix-free</em> codes among the symbol codes.</p>
</li>
<li>
<p><strong>Prefix-free tree</strong>: Any prefix-free code has a <em>prefix-free binary tree</em> associated with it, which has the leaf
nodes corresponding to the codewords.</p>
</li>
<li>
<p><strong>Compression thumb rule</strong>: One, somewhat obvious thumb rule we learnt for symbol codes was that if $$ P(s_1) \geq P(
s_2) \implies l(s_1)  \leq l(s_2)$$ else we could swap the codewords for symbols $s_1, s_2$ and achieve lower average
codelength.</p>
</li>
<li>
<p><strong>Neg Log-likelihood</strong>: Another thumb-rule we learnt was that the optimal code-lengths for a prefix-free code are $$l_
{optimal}(symbol) \approx \log_2 \frac{1}{p(symbol)}$$
We will understand why this rule is true in the next lecture.</p>
</li>
<li>
<p><strong>Prefix-free code construction</strong>: We looked at a procedure for constructing <em>prefix-free codes</em> for any
distribution. In the next lecture, we will justify the correctness of the procedure and also discuss a couple of more
ideas of constructing prefix-free codes</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="designing-prefix-free-codes-1"><a class="header" href="#designing-prefix-free-codes-1">Designing prefix-free codes</a></h1>
<p>As a recap, in the previous lecture we discussed how to design a prefix free code given the code lengths. We discussed
a simple procedure which constructs a <em>prefix-free tree</em> given the code-lengths.</p>
<p>We also saw a simple thumb rule $l_{optimal}(symbol) \approx \log_2 \frac{1}{p(symbol)}$ which tells us what the
code-lengths of the prefix-free code should be. In this lecture we are going to discuss two things:</p>
<ol>
<li>Justify that <em>correctness</em> of our prefix-free tree construction with lengths $ l(symbol) = \left\lceil \log_2
\frac{1}{p(symbol)} \right\rceil $</li>
<li>Look at a few more prefix-free code constructions</li>
</ol>
<h2 id="kraft-inequality--converse"><a class="header" href="#kraft-inequality--converse">Kraft Inequality &amp; converse</a></h2>
<p>With the goal of proving the <em>correctness</em> of the <em>prefx-free tree</em> construction, we will first look at a simple but
fundamental property of binary trees, called the <em>Kraft-Mcmillan Inequality</em> (or simply the <em>Kraft Inequality</em>)</p>
<div id="admonition-theorem-1-kraft-inequality" class="admonition admonish-example" role="note" aria-labelledby="admonition-theorem-1-kraft-inequality-title">
<div class="admonition-title">
<div id="admonition-theorem-1-kraft-inequality-title">
<p>Theorem-1: Kraft Inequality</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/kraft_ineq_and_optimality.html#admonition-theorem-1-kraft-inequality"></a>
</div>
<div>
<p>Consider a binary tree, where the leaf nodes $n_1,n_2,\ldots,n_k$ are at depths $l_1,l_2,\ldots,l_k$ from the root node respectively.</p>
<p>Then the node depths $l_1,l_2,\ldots,l_k$ satisfy the inequality:</p>
<p>$$ \sum_{i=1}^k 2^{-l_i} \leq 1$$</p>
</div>
</div>
<p>The inequality is quite elegant, and so is its proof. Any thoughts on how the proof might proceed? Here is a hint:</p>
<p><strong>Hint</strong>: Let $l_{max} = \max_{i=1}^k l_i$. Then, the Kraft inequality can be written as: $$ \sum_{i=1}^k 2^{l_
{max}-l_i} \leq 2^{l_{max}} $$ All we have done here is multiply both sides by $2^{l_{max}}$, but this simple
transformation will help make the inequality more interpretable! Can you see the proof now? Here is a proof sketch:</p>
<ul>
<li>Let's try to interpret the RHS, $2^{l_{max}}$ are the number of nodes of the binary tree at depth $l_{max}$.</li>
<li>The LHS also has a natural interpretation: Given a leaf node at depth $l_i$, one can imagine that it corresponds to
$2^{l_{max} - l_i}$ nodes at depth $l_{max}$.</li>
</ul>
<pre class="mermaid">graph TD
  *(Root) --&gt;|0| A:::endnode
  A -.-&gt; n6(.):::fake
  A -.-&gt; n7(.):::fake
  n6-.-&gt; n8(.):::fake
  n6-.-&gt; n9(.):::fake
  n7-.-&gt; m1(.):::fake
  n7-.-&gt; m2(.):::fake
  *(Root) --&gt;|1| n1(.)
  n1 --&gt;|10| B:::endnode
  B -.-&gt; m3(.):::fake
  B -.-&gt; m4(.):::fake
  n1 --&gt;|11| n2(.)
  n2 --&gt; |110| C:::endnode
  n2 --&gt; |111| D:::endnode
  classDef fake fill:#ddd;
</pre>
<p>For example in the tree example above, node $A$ has <code>4</code> nodes corresponding to it at <code>depth = 3</code>, while node $B$ has <code>2</code>
nodes.</p>
<ul>
<li>It is clear that the nodes at depth $l_{max}$ are distinct for each of the leaf nodes $n_i$ (Think why?). As the "total number of nodes at depth $l_{max}$", is larger than "the sum of nodes at depth $l_{max}$ corresponding to leaf nodes
$n_i$, we get the inequality</li>
</ul>
<p>$$ \sum_{i=1}^k 2^{l_{max}-l_{i}} \leq 2^{l_{max}} $$</p>
<ul>
<li>This completes the proof sketch for the Kraft Inequality:</li>
</ul>
<p>$$ \sum_{i=1}^k 2^{-l_i} \leq 1$$</p>
<p>Well, that was a short and simple proof! It is also clear that the equality is true, if and only if there is no leaf
node left unaccounted for.</p>
<p>We can use the Kraft inequality to now show the <em>correctness</em> of the prefix-free tree construction with code-lengths $l_{i}$, as we discussed in last lecture.</p>
<h2 id="prefix-tree-construction-correctness"><a class="header" href="#prefix-tree-construction-correctness">Prefix-tree construction <em>correctness</em></a></h2>
<p>To recap, our <em>prefix-free tree</em> construction proceeds as follows:</p>
<ol>
<li>
<p>We are given probability distribution $p_1, p_2, \ldots, p_k$ for symbols $s_1, s_2, \ldots,
s_k$. <a href="https://en.wikipedia.org/wiki/Without_loss_of_generality">WLOG</a> assume that: $$ p_1 \geq p_2 \geq ... \geq
p_k$$ Now, compute code-lengths $l_1, l_2, \ldots, l_k$ such that: $$ l_i = \left\lceil \log_2 \frac{1}{p_i}
\right\rceil $$ Thus, the lengths, $l_1, l_2, \ldots, l_k$ satisfy $$ l_1 \leq l_2 \leq ... \leq l_k$$</p>
</li>
<li>
<p>The <em>prefix-free tree</em> construction follows by starting with an empty binary tree, and then recursively adding a leaf
node at depth $l_i$ to the binary tree.</p>
</li>
</ol>
<p>We want to argue the <em>correctness</em> of the tree construction at each step, i.e. we want to show that when we are adding node $n_k$, there will always be a node available for us to do so.</p>
<p>Let's proceed towards showing this inductively.</p>
<ul>
<li>In the beginning, we just have the root node, so we can safely add the node $n_1$ with length $l_1$. To do so, we need
to create a binary tree with $2^{l_1}$ leaf nodes, and just assign one of the leaf nodes to node $n_1$</li>
<li>Now, let's assume that we already have a binary tree $\mathcal{T}<em>{r-1}$ with nodes $n_1, n_2, \ldots, n</em>{r-1}$ and
that we want to add node $n_r$. We want to argue that there will always be a leaf node available with depth $l_r$ in
the binary tree $\mathcal{T}_{r-1}$. Let's see how can we show that:</li>
<li>If we look at the code-lengths, i.e. the depths of the nodes $l_i$, we see that they follow the Kraft inequality $$
\begin{aligned}
\sum_{i=1}^k 2^{-l_i} &amp;= \sum_{i=1}^k 2^{-\left\lceil \log_2 \frac{1}{p_i} \right\rceil} \
&amp;= \sum_{i=1}^k 2^{\left\lfloor \log_2 p_i \right\rfloor} \
&amp;\leq \sum_{i=1}^k 2^{ \log_2 p_i} \
&amp;\leq \sum_{i=1}^k p_i \
&amp;= 1
\end{aligned} $$ Now as $\sum_{i=1}^k 2^{-l_i} \leq 1$ it implies that the node depths of the tree $\mathcal{T}<em>{r-1}$
satisfies $\sum</em>{i=1}^{r-1} 2^{-l_i} &lt; 1$ (for $r &lt;= k$)</li>
<li>We know from the Kraft inequality that if the inequality is not tight, then there will be a leaf node available at $\max_{i=1}^{r-1}
l_i = l_{r-1}$ depth. Now, as $l_r \geq l_{r-1}$, we can safely say that the node $n_k$ can be added to the binary
tree $\mathcal{T}_{r-1}$.</li>
<li>This completes the <em>correctness</em> proof.</li>
</ul>
<p>In fact if you look at the proof, all we have used is the fact that $\sum_{i=1}^k 2^{-l_i} \leq 1$. Thus, the same proof
also gives us the following <em>converse theorem</em> of the Kraft inequality:</p>
<div id="admonition-theorem-2-converse-of-kraft-inequality" class="admonition admonish-example" role="note" aria-labelledby="admonition-theorem-2-converse-of-kraft-inequality-title">
<div class="admonition-title">
<div id="admonition-theorem-2-converse-of-kraft-inequality-title">
<p>Theorem-2: Converse of Kraft Inequality</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/kraft_ineq_and_optimality.html#admonition-theorem-2-converse-of-kraft-inequality"></a>
</div>
<div>
<p>Let $l_i \in \mathbb{N}$ such that:
$$ \sum_{i=1}^k 2^{-l_i} \leq 1$$</p>
<p>then, we can always construct a binary tree with $k$ leaf nodes at depths $l_i$ from the root node for $i \in {1,\dots, k}$.</p>
</div>
</div>
<h2 id="kraft-inequality-and-the-thumb-rule"><a class="header" href="#kraft-inequality-and-the-thumb-rule">Kraft inequality and the thumb rule</a></h2>
<p>In the last chapter we introduced the thumb rule $l_{optimal}(symbol) \approx \log_2 \frac{1}{p(symbol)}$ without any justification. Since then
we have seen a code construction that gets close to this thumb rule. Here we briefly sketch how Kraft inequality can be shown to justify the thumb rule. Details can be found in section 5.3 of extremely popular book by Cover and Thomas "<a href="https://searchworks.stanford.edu/?search_field=search&amp;q=elements+of+information+theory">Elements of Information Theory</a>". The idea is to consider the optimization problem
$$
\min \sum_{i=1}^k p_i l_i \
\mathrm{subject\  to}  \sum_{i=1}^k 2^{-l_i} \leq 1
$$
This is simply the optimization problem to minimize the expected code lengths for a prefix code (Kraft's inequality gives a mathematical way to express the constraint). While this is a integer optimization problem (due to the code lengths being integral) and hence is hard to solve, we can relax the integer constraint and try to solve this for any positive $l_i$. Then the method of Lagrange multipliers from convex optimization can be used to sovle this problem and obtain the optimal code lengths $l_i = - \log p_i$. In the next chapter, we will look again into this and derive the thumb rule in a different way (though the Kraft inequality still plays a crucial role).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="entropy--neg-log-likelihood-thumb-rule"><a class="header" href="#entropy--neg-log-likelihood-thumb-rule">Entropy &amp; Neg-log likelihood thumb rule</a></h1>
<p>In this lecture, our goal is to understand where the thumb rule  $l_{optimal}(symbol) \approx \log_2 \frac{1}{p(symbol)}$ for prefix-free code-lengths came from. To understand this, we are going to take a brief detour and get a sneak-peek into the area of Information Theory.</p>
<p>Information theory is the <em>science of information</em>, and is an area which tries to understand the fundamental nature of what is <em>information</em>, how do we store it, how do we communicate it etc. The whole field (along with some of the most fundamental results in the field) was laid down in a single seminal paper <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">A Mathematical theory of Communication</a> by Claude Shannon in 1948.</p>
<p>Rather than talking about information in philosophical terms, we are going to understand information theory mathematically. We are going to first define some quantities and properties central to information theory. As we will see, understanding the mathematical properties of these quantities is what will lead us to the <em>intuition</em> and understanding of information theory.</p>
<h2 id="entropy"><a class="header" href="#entropy">Entropy</a></h2>
<p>So, lets get started: the first property, or <em>information-theoretic measure</em> we are going to define is the <em>Entropy</em>. The entropy, although a term overloaded with a lot of meaning in the (math/science) pop culture, has a precise definition in information theory.</p>
<div id="admonition-entropy" class="admonition admonish-note" role="note" aria-labelledby="admonition-entropy-title">
<div class="admonition-title">
<div id="admonition-entropy-title">
<p>Entropy</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/entropy.html#admonition-entropy"></a>
</div>
<div>
<p>Let $X$ be a random variable, with alphabet ${1, 2, \ldots, k}$ and discrete probability distribution $P = { p_1, p_2, \ldots, p_k}$. i.e. one can imagine the samples generate by the random variable $X$ to be independent and identically distributed as per distribution $P$.</p>
<p>Then the entropy $H(X)$ of the random variable is defined as:</p>
<p>$$ H(X) = \sum_{i=1}^k p_i \log_2 \frac{1}{p_i} $$</p>
</div>
</div>
<p>Some comments:</p>
<ul>
<li>As you can see, $H(X)$ is a simple function of the distribution $P$. So, some works also denote $H(X)$ as a $H(P)$. We will use both of these notations interchangeably; but note that that the $H(X)$ notation is more precise and will be more helpful when we deal with more complex distributions (like Markov distributions).</li>
<li>Another way to look at $H(X)$ is as expectation over the negative-log likelihood function $NLL(X) = \log_2 \frac{1}{P(X)}$
$$
\begin{aligned}
H(X) &amp;= \mathbb{E}_P\left[ NLL(X) \right] \
H(X) &amp;= \mathbb{E}<em>P\left[ \log_2 \frac{1}{P(X)} \right] \
&amp;= \sum</em>{i=1}^k p_i \log_2 \frac{1}{p_i} \
\end{aligned}
$$
Although, this feels a bit weird, the neg-log likelihood function is a completely legitimate function of the random variable $X$. All the different viewpoints are useful in different scenarios</li>
</ul>
<h2 id="properties-of-entropy"><a class="header" href="#properties-of-entropy">Properties of Entropy</a></h2>
<p>Now that we have defined <em>Entropy</em>, lets take a look at some properties of $H(X)$. To prove most of these properties, we are going to need a simple but a very profound property of convex functions: the <em>Jensen's inequality</em>.</p>
<h3 id="jensens-inequality"><a class="header" href="#jensens-inequality">Jensen's inequality</a></h3>
<div id="admonition-jensens-inequality" class="admonition admonish-note" role="note" aria-labelledby="admonition-jensens-inequality-title">
<div class="admonition-title">
<div id="admonition-jensens-inequality-title">
<p>Jensen's inequality</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/entropy.html#admonition-jensens-inequality"></a>
</div>
<div>
<p>Let $f(x)$ be a convex function. Let $X$ be a random variable on ${1, 2, \ldots, k}$ with discrete probability distribution $P = {p_1, p_2, \ldots, p_k}$ be some probability distribution. Then:
$$
\mathbb{E} [f(X)] \geq f(\mathbb{E}[X])
$$</p>
<p>Similarly if $g(X)$ is a concave function, then the reverse inequality is true:
$$
\mathbb{E} [g(X)] \leq g(\mathbb{E}[X])
$$</p>
<p>In both cases, equality holds iff (if and only if) $x_1 = x_2 = \ldots = x_k$</p>
</div>
</div>
The Jensen's inequality can be also understood succinctly using the figure below. for clarity lets assume that $X$ takes values in $\{x_1, x_2\}$. 
<p><img src="lossless_iid/images/jensen.svg" alt="Jensen&#39;s inequality" /></p>
<ul>
<li>If one looks at the LHS: $ \mathbb{E} [f(X)] = p_1 f(x_1) + p_2f(x_2) $, then it can be understood as simply a weighted average with weights $p_1, p_2$ over the points $f(x_1), f(x_2)$. This weighted average corresponds to a point somewhere on the segment joining points $f(x_1)$ and $f(x_2)$</li>
<li>If one looks at the RHS: $f(\mathbb{E}[X]) = f(p_1x_1 + p_2x_2)$. Thus, the RHS corresponds to the value of the convex function $f$ at the weighted averaged point $p_1x_1 + p_2x_2$. It is clear from the figure that because of the "bowl-shaped* nature of $f$, $\mathbb{E} [f(X)] \geq f(\mathbb{E}[X])$.</li>
<li>It is also clear from the figure why the equality condition holds true iff $x_1 = x_2$</li>
</ul>
<p>We won't prove the Jensen's inequality here, but it is useful to understand and visualize the inequality. For a rigorous proof look at <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">the wiki article on Jensen's Inequality</a>. In the simple discrete case, it can be proved using induction.</p>
<p>Okay, we are all set to look at some properties of <em>Entropy</em>. As you can guess, in our case, we will be specifically using Jensen's inequality on the concave function $\log_2(.)$.</p>
<h4 id="1-non-negativity-of-entropy-hx-geq-0"><a class="header" href="#1-non-negativity-of-entropy-hx-geq-0"><strong>1. Non-negativity of Entropy: $H(X) \geq 0$</strong>:</a></h4>
<p>It is easy to see that $H(X) \geq 0$. The reason is that: $\log_2 \frac{1}{p_i} \geq 0$, as $0 \leq p_i \leq 1$
$$
\begin{aligned}
H(X) &amp;= \sum_{i=1}^k p_i \log_2 \frac{1}{p_i} \
&amp;\geq  \sum_{i=1}^k p_i \times 0 \
&amp;= 0
\end{aligned}
$$</p>
<h4 id="2-upper-bound-on-entropy-hx-leq-log_2-k"><a class="header" href="#2-upper-bound-on-entropy-hx-leq-log_2-k"><strong>2. Upper bound on Entropy: $H(X) \leq log_2 k$</strong>:</a></h4>
<p>We know that $H(X) \geq 0$. We can also compute an upper bound on $H(X)$:
$$
\begin{aligned}
H(X) &amp;= \mathbb{E}_P \log_2 \frac{1}{p(X)} \
&amp;\leq \log_2 \mathbb{E}<em>P \frac{1}{p(X)} \
&amp;= \log_2 \sum</em>{i=1}^k p_i * \frac{1}{p_i} \
&amp;= \log_2 k
\end{aligned}
$$
The first step of the proof follows from Jensen's inequality for the Concave function $\log_2(.)$. Also, note that the equality $H(X) = \log_2 k$ is possible only if: $$ \log_2 \frac{1}{p_1} = \log_2 \frac{1}{p_2} = \ldots = \log_2 \frac{1}{p_k}$$
which implies that $$p_1=p_2=\ldots=p_k \iff H(X) = \log_2 k $$</p>
<h4 id="3-joint-entropy-of-independent-random-variables"><a class="header" href="#3-joint-entropy-of-independent-random-variables"><strong>3. Joint entropy of independent random variables</strong>:</a></h4>
<p>If we have random variables $X_1, X_2$, we can define their entropy simply as the entropy of the pair $(X_1,X_2)$. The joint entropy is denoted
as $H(X_1, X_2)$ and can be expanded as
$$H(X_1, X_2) = \sum_{x_1, x_2} P(X_1=x_1, X_2=x_2) \log_2 \frac{1}{P(X_1=x_1, X_2=x_2)}$$</p>
<p>Now if $X_1$ and $X_2$ are independent random variables, then we have
$$
\begin{align}
H(X_1, X_2) &amp;= \sum_{x_1, x_2} P(X_1=x_1, X_2=x_2) \log_2 \frac{1}{P(X_1=x_1, X_2=x_2)}\
&amp;= \sum_{x_1, x_2} P(X_1=x_1) P(X_2=x_2) \log_2 \frac{1}{P(X_1=x_1) P(X_2=x_2)}\
&amp;= \sum_{x_1, x_2} P(X_1=x_1) P(X_2=x_2) \log_2 \frac{1}{P(X_1=x_1)} + \sum_{x_1, x_2} P(X_1=x_1) P(X_2=x_2) \log_2 \frac{1}{P(X_2=x_2)}\
&amp;= \sum_{x_1} P(X_1=x_1)  \log_2 \frac{1}{P(X_1=x_1)} \sum_{x_2}P(X_2=x_2) + \sum_{x_2}P(X_2=x_2) \log_2 \frac{1}{P(X_2=x_2)}\sum_{x_1}P(X_1=x_1) \
&amp;= \sum_{x_1} P(X_1=x_1)  \log_2 \frac{1}{P(X_1=x_1)}+ \sum_{x_2}P(X_2=x_2) \log_2 \frac{1}{P(X_2=x_2)}\
&amp;= H(X_1) + H(X_2)\
\end{align}
$$</p>
<p>Thus, the joint entropy of independent random variables is simply the sum of their individual entropies. This generalizes to $n$ independent random variables using induction. In particular, if the $n$-tuple $X^n = (X_1,\dots,X_n)$ consists of i.i.d. random variables distributed like $X$, then $H(X^n) = nH(X)$. The general case of dependent random variables and their joint entropy is studied in a later chapter.</p>
<h4 id="4-entropy-as-a-lower-bound--hx-leq-mathbbe_p-log_2-frac1qx--for-any-distribution-qx"><a class="header" href="#4-entropy-as-a-lower-bound--hx-leq-mathbbe_p-log_2-frac1qx--for-any-distribution-qx"><strong>4. Entropy as a lower bound: $ H(X) \leq \mathbb{E}_P \log_2 \frac{1}{q(X)} $ for any distribution $q(X)$:</strong></a></h4>
<p>Let us spell out this property in a bit more detail. Given any probability distribution $q(X)$, the following is true: $$  H(X) \leq \mathbb{E}_P \log_2 \frac{1}{q(X)} $$ the equality holds iff $q(X) = p(X)$, i.e. we want to show that
$$
\begin{aligned}
0 &amp;\leq \mathbb{E}_P \log_2 \frac{1}{q(X)} - H(X)\
&amp;= \mathbb{E}_P \log_2 \frac{1}{q(X)} - \mathbb{E}_P \log_2 \frac{1}{p(X)}\
&amp;= \mathbb{E}_P \log_2 \frac{p(X)}{q(X)}
\end{aligned}
$$
Okay, so proving the lower bound property is equivalent to showing that $\mathbb{E}_P \log_2 \frac{p(X)}{q(X)} \geq 0$. Any idea how can we do this?</p>
<p>The idea is again to use <em>Jensen's inequality</em>:</p>
<p>$$
\begin{aligned}
\mathbb{E}_P \log_2 \frac{p(X)}{q(X)} &amp;= - \mathbb{E}<em>P \log_2 \frac{q(X)}{p(X)} \
&amp;\geq - \log_2 \mathbb{E}<em>P \frac{q(X)}{p(X)} \
&amp;\geq - \log_2 \sum</em>{i=1}^k p_i * \frac{q_i}{p_i} \
&amp;\geq - \log_2 \sum</em>{i=1}^k q_i \
&amp;\geq - \log_2 1 \
&amp;= 0
\end{aligned}
$$</p>
<p>Note that we used <em>Jensen's inequality</em> in step (2), but the sign of the inequality is reversed because of the negation.
We also know that equality holds iff
$$
\frac{q_1}{p_1} = \frac{q_2}{p_2} = \ldots  = \frac{q_k}{p_k}
$$
i.e. if $q(X) = p(X)$!</p>
<p>Alright! This proves our property as
$$ \mathbb{E}_P \log_2 \frac{q(X)}{p(X)} \geq 0 \rightarrow H(X) \leq \mathbb{E}_P \log_2 \frac{1}{q(X)} $$</p>
<p>Let's pause a bit and think about what this property is telling:</p>
<ul>
<li>Essentially we are telling that $H(X)$ is the solution to the minimization problem:
$$ H(X) = \min_{q(X)} \mathbb{E}_P \log_2 \frac{1}{q(X)}  $$
We will see that this characterization is often quite useful.</li>
<li>Another corollary we proved is that $$ \mathbb{E}<em>P \log_2 \frac{q(X)}{p(X)} = \sum</em>{i=1}^k p_i \log_2 \frac{p_i}{q_i} \geq 0 $$
for any two distributions $p, q$. The quantity on the right is also known as the <em>KL-Divergence</em> $D_{KL}(p||q)$ between the two distributions.</li>
</ul>
<div id="admonition-kl-divergence" class="admonition admonish-example" role="note" aria-labelledby="admonition-kl-divergence-title">
<div class="admonition-title">
<div id="admonition-kl-divergence-title">
<p>KL-Divergence</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/entropy.html#admonition-kl-divergence"></a>
</div>
<div>
<p>Let $P = { p_1, p_2, \ldots, p_k}$ and $Q = { q_1, q_2, \ldots, q_k}$ be two given probability distributions. Then the KL-Divergence is defined as:</p>
<p>$$ D_{KL}(p||q) = \sum_{i=k} p_i \log_2 \frac{p_i}{q_i} $$</p>
<p>The following property holds for $D_{KL}(p||q)$:</p>
<p>$$ D_{KL}(p||q) \geq 0$$</p>
<p>Equality holds iff $p=q$</p>
</div>
</div>
<p>The KL-Divergence is a very important information theoretic quantity, which we will revisit!</p>
<hr />
<p>Okay, now that we have defined and got a bit familiar with some of the information theoretic measures, we are all set to understand where the thumb rule  $l_{optimal}(symbol) \approx \log_2 \frac{1}{p(symbol)}$ comes from!</p>
<p>To do so, let's try to answer the question:</p>
<div id="admonition-question-optimal-prefix-free-code" class="admonition admonish-question" role="note" aria-labelledby="admonition-question-optimal-prefix-free-code-title">
<div class="admonition-title">
<div id="admonition-question-optimal-prefix-free-code-title">
<p>Question: Optimal prefix-free code</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/entropy.html#admonition-question-optimal-prefix-free-code"></a>
</div>
<div>
<p>Given a probability distribution $p_1, p_2, \ldots, p_k$,</p>
<ol>
<li>What is the optimal prefix-free code for this distribution?</li>
<li>What is the best compression it can achieve?</li>
</ol>
</div>
</div>
<h2 id="optimal-prefix-free-codes"><a class="header" href="#optimal-prefix-free-codes">Optimal prefix-free codes</a></h2>
<p>The question seems quite daunting! How can we even go about approaching this one? The space of all prefix-free codes is quite large, and not very structured.</p>
<p>Luckily, based on the converse of Kraft's inequality, we know that the space of all prefix-free codes can be characterized by the inequality:</p>
<p>$$ \sum_{i=1}^k 2^{-l_i} \leq 1$$
where $l_i$ represent the code-lengths (i.e the depths of the nodes in the prefix-free tree)</p>
<p>Thus, the problem of finding the <em>optimal prefix-free code</em> given a probability distribution $p_1, p_2, \ldots, p_k$ can be formulated as a concise optimization problem.</p>
<div id="admonition-optimal-prefix-free-code" class="admonition admonish-note" role="note" aria-labelledby="admonition-optimal-prefix-free-code-title">
<div class="admonition-title">
<div id="admonition-optimal-prefix-free-code-title">
<p>Optimal prefix-free code</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/entropy.html#admonition-optimal-prefix-free-code"></a>
</div>
<div>
<p>The problem of finding <em>optimal prefix-free code</em> given a probability distribution $p_1, p_2, \ldots, p_k$ can be formulated as:</p>
<p>Given probabilities $p_1, p_2, \ldots, p_k$, solve for code-lengths $l_i \in \mathbb{N}$, such that:
$$ \begin{aligned}
\text{minimize   } &amp;\sum_{i=1}^k p_i l_i \
\text{under the constraint } &amp;\sum_{i=1}^k 2^{-l_i} \leq 1
\end{aligned} $$</p>
</div>
</div>
<p>Quite elegant! We have disassociated the problem of finding optimal prefix-free codes from unclear search spaces over trees and so on. Instead, we now just have this concrete optimization problem. Unfortunately we have integers $l_i$ to optimize over; and integer optimization problems might not be a feasible problem in general. In this case however, we can indeed solve this problem! We will look at a solution in the next lecture.</p>
<p>For this lecture, our goal is to see what best average compression performance we can achieve, i.e what is the solution (or a lower bound) to the minimization objective $\sum_{i=1}^k p_i l_i = \mathbb{E}_P [L]$</p>
<h3 id="lower-bounding-the-average-codelength"><a class="header" href="#lower-bounding-the-average-codelength">Lower bounding the average codelength</a></h3>
<p>Okay, lets try to see how we can obtain a lower bound for $\sum_{i=1}^k p_i l_i$, given the constraint that $\sum_{i=1}^k 2^{-l_i} \leq 1$.</p>
<p>Let's start by adding a <em>dummy</em> length $l_{k+1}$, so that:
$$ \sum_{i=1}^{k+1} 2^{-l_i} = 1$$
The benefit of this formulation is that we can now imagine $2^{-l_i} = q_i$ as probability values, and use the plethora of properties we proved earlier! Note that $l_{k+1}$ might not be an integer, but that is okay.</p>
<p>Let's also define $p_{k+1} = 0$, so that the new objective is equal to the average codelength.</p>
<p>$$ \mathbb{E}<em>P [L] = \sum</em>{i=1}^k p_i l_i = \sum_{i=1}^{k+1} p_i l_i $$</p>
<p>Okay, now that we have the setup ready, let us try to analyze the average codelength:</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{k+1} p_i l_i   &amp;= \sum_{i=1}^{k+1} p_i \log_2  2^{l_i}\</p>
<p>&amp;= \sum_{i=1}^{k+1} p_i \log_2 \frac{1}{2^{-l_i}} \
&amp;= \sum_{i=1}^{k+1} p_i \log_2 \frac{1}{q_i} \
\end{aligned}
$$</p>
<p>All we have done until now is some basic math yoga, but the outcome is that we have transformed the objective $\sum_{i=1}^{k+1} p_i l_i$ into a form more amenable for our purpose. Do you now see the lower bound?</p>
<p>Using the <strong>property 3</strong> of entropy for distribution $P$ and $Q$ proved above, we can now show that:</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{k+1} p_i l_i  &amp;= \sum_{i=1}^{k+1} p_i \log_2 \frac{1}{2^{-l_i}} \
&amp;= \sum_{i=1}^{k+1} p_i \log_2 \frac{1}{q_i} \
&amp;\geq \sum_{i=1}^{k+1} p_i \log_2 \frac{1}{p_i}  \
&amp;=  \sum_{i=1}^{k} p_i \log_2 \frac{1}{p_i}  \
&amp;= H(X)
\end{aligned}
$$</p>
<p><strong>Thus, the lower bound on average codelength of any prefix-free code is the Entropy $H(X)$!</strong> It is also easy to see that the equality holds if and only if:
$p_i = q_i = 2^{-l_i}$ for all $i \in {1, 2, \ldots, k}$.</p>
<p>To summarize the important theorem:</p>
<div id="admonition-theorem-3-entropy-as-a-lower-bound-on-avg-codelength" class="admonition admonish-example" role="note" aria-labelledby="admonition-theorem-3-entropy-as-a-lower-bound-on-avg-codelength-title">
<div class="admonition-title">
<div id="admonition-theorem-3-entropy-as-a-lower-bound-on-avg-codelength-title">
<p>Theorem-3: Entropy as a lower bound on avg. codelength</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/entropy.html#admonition-theorem-3-entropy-as-a-lower-bound-on-avg-codelength"></a>
</div>
<div>
<p>Given a distribution $p_1, p_2, ..., p_k$ for data. Any prefix-free code with code-lengths $l_1, l_2, \ldots, l_k$ will have average codelength lower bounded by the entropy $H(X)$ of the source distribution $P$:</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{k} p_i l_i<br />
&amp;\geq  \sum_{i=1}^{k} p_i \log_2 \frac{1}{p_i}  \
&amp;= H(X)
\end{aligned}
$$</p>
<p>The equality holds if and only if:
$p_i = 2^{-l_i}$ for all $i \in {1, 2, \ldots, k}$, i.e:</p>
<p>$$ l_i = \log_2 \frac{1}{p_i} $$</p>
</div>
</div>
<p>Some observations/comments:</p>
<h4 id="1-thumb-rule-justification"><a class="header" href="#1-thumb-rule-justification"><strong>1. Thumb-rule justification</strong></a></h4>
<p>Notice that the equality condition tells us that optimal compression can be achieved by the prefix-free code iff:</p>
<p>$$ l_i = \log_2 \frac{1}{p_i} $$</p>
<p>and, this is the justification for the thumb rule $l_{optimal}(symbol) \approx \log_2 \frac{1}{p(symbol)}$, we discussed in the previous lecture and used for construction of Shannon code.</p>
<h4 id="2-shannon-code-performance"><a class="header" href="#2-shannon-code-performance"><strong>2. Shannon code performance</strong></a></h4>
<p>Another point to note is that, we can never achieve avg codelength equal to entropy $H(X)$, unless $p_i = 2^{-l_i}$, i.e. all the probabilities $p_i$ are powers of $2$. This can be quite unlikely in real life, which is a bit of a bummer.</p>
<p>The positive news is that we can in fact get quite close to $H(X)$.</p>
<p>Let's analyze the performance of Shannon code. We know that Shannon code has code-lengths $l_i = \left\lceil \log_2 \frac{1}{p_i} \right\rceil$, thus the average codelength is:</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{k} p_i l_i<br />
&amp;\leq  \sum_{i=1}^{k} p_i  \left\lceil \log_2 \frac{1}{p_i} \right\rceil \
&amp;\leq \sum_{i=1}^{k} p_i  ( \log_2 \frac{1}{p_i} + 1 ) \
&amp;= \sum_{i=1}^{k} p_i  \log_2 \frac{1}{p_i} + 1 \
&amp;= H(X) + 1
\end{aligned}
$$</p>
<p>Thus, Shannon code guarantees that we can always achieve average code-lengths within $1$ bit of the optimal. This is quite promising!</p>
<p>Essentially, unless the entropy $H(X)$ is very low, we are not missing much by using Shannon codes.</p>
<h4 id="3-coding-in-blocks"><a class="header" href="#3-coding-in-blocks"><strong>3. Coding in blocks</strong></a></h4>
<p>As discussed above Shannon codes only guarantee performance up to 1 bit of entropy, and this gap is too much in some scenarios, especially with highly compressible data. In such cases, one common strategy is to code in blocks. This simply means we can treat an input $n$-tuple $X^n = (X_1,\dots,X_n)$ as a symbol in a bigger alphabet, and apply Shannon code to this alphabet. So we split the input stream into blocks of size $n$ symbols, encode each block and concatenate the results. Here again, Shannon codes get us to within 1 bit of entropy and hence we have (denoting the expected code length by $\bar{l}$):
$$\bar{l}(X^n) \leq H(X^n) + 1$$</p>
<p>Using the properties of joint entropy for i.i.d. random variables, and dividing by $n$ we get
$$\frac{1}{n}\bar{l}(X^n) \leq H(X) + \frac{1}{n}$$
which means coding in blocks of $n$ brings us within $\frac{1}{n}$ bits of entropy. This means we can get arbitrary close to entropy using block coding and hence entropy is not only the lower bound, but it is also achievable. In the next chapter, we will look at some examples of this phenomenon where we'll see how the average code length improves as we code with bigger and bigger blocks. But there is no free lunch with block coding! Note that a naive implementation would lead to a complexity exponential in $n$ as if the alphabet size is $k$ per-symbol, with block-coding the number of symbols in the alphabet an encoder needs to handle expands to a size of $k^n$. Moreover, we now have a decoding delay of $n$ since we cannot just go and decode a single symbol until we get the entire block. In later chapters, we will look at clever implementations that successfully get the benefits of block coding while reducing this penalty.</p>
<h4 id="4-entropy-as-a-lower-bound-for-uniquely-decodable-codes"><a class="header" href="#4-entropy-as-a-lower-bound-for-uniquely-decodable-codes"><strong>4. Entropy as a lower bound for uniquely decodable codes</strong></a></h4>
<p>We showed that $H(X)$ lower bounds the average code-lengths for prefix-free codes. But, what about general symbol codes which are uniquely decodable (or <em>lossless</em>)?</p>
<p>We saw that even for general uniquely decodable symbol codes, the Kraft inequality holds true. i.e:
$$ \sum_{i=1}^k 2^{-l_i} \leq 1$$
Notice that this is precisely the only property we use for proving the lower bound on $H(X)$, and so, entropy is in fact a lower bound on average code-lengths for general uniquely decodable codes.</p>
<h4 id="5-mismatched-coding-and-kl-divergence"><a class="header" href="#5-mismatched-coding-and-kl-divergence"><strong>5. Mismatched coding and KL-divergence</strong></a></h4>
<p>We saw above that the code optimized of a source $P$ will have code lengths $l_i \approx \log_2 \frac{1}{p_i}$. This achieves expected code length close to entropy:
$$\bar{l} = \sum p_i l_i \approx \sum p_i  \log_2 \frac{1}{p_i} = H(P)$$</p>
<p>Now, consider a scenario where we optimize a code for source distribution $Q$ but then use it for a source with distribution $P$. In this case, the code lengths will be  $l_i \approx \log_2 \frac{1}{q_i}$ and the average code length will be
$$
\begin{align}
\bar{l} &amp;= \sum p_i l_i \
&amp;\approx \sum p_i  \log_2 \frac{1}{q_i} \
&amp;= \sum p_i  \log_2 \frac{1}{p_i} + \sum p_i  \log_2 \frac{p_i}{q_i} \
&amp;= H(P) + D(P||Q)
\end{align}
$$
In this scenario, the optimal code length was $H(P)$ but the actual average code length exceeds it by $D(P||Q)$. Thus the KL-divergence is the penalty for mismatched coding for using the incorrect distribution for designing the code. Later in the class we will study universal codes which imply existence of distributions that can closely approximate any distribution from a large class, meaning that a single code (over a block) is able to perform well irrespective of the actual source distribution.</p>
<h2 id="entropy-beyond-data-compression"><a class="header" href="#entropy-beyond-data-compression">Entropy beyond data compression</a></h2>
<p>Given a random variable $X$ over ${1, 2, \ldots, k}$ with distribution $P = { p_1, p_2, \ldots, p_k }$, we know that the fundamental limit on compression of the random variable $X$ using symbol codes is $H(X)$.</p>
<p>$H(X)$ is sometimes also referred to as the self-information of the random variable $X$, as in a way, we are saying that the amount of information (on an average) in each instantiation of the random variable $X$ is $H(X)$ bits. This is quite natural, as we should not be able to compress an input better than the amount of information it is worth.</p>
<p>The discussion, although a bit hand-wavy is quite fundamental. Due to this, we see $H(X)$ popping up at lots of other places as a fundamental limit of a problem, and not just in case of compression.</p>
<p>For example, here are a few scenarios, for which the answer is related to $H(X)$. Try to solve them for fun!</p>
<p><strong>1. Simulating non-uniform random variables:</strong> Lets say you want to simulate a random variable $X$ over ${1, 2, \ldots k}$ with distribution $P = { p_1, p_2, \ldots, p_k }$. To simulate the random variable, you are only allowed to use fair coin tosses. In that case, what is fundamental limit on the average number of coins you would have to toss to simulate $X$?</p>
<p><strong>2. Simulating a fair coin using a biased coin:</strong> Lets now say you have a biased coin with probability of <code>H/T</code> to be $p, 1-p$ respectively. The goal is to use this coin to simulate a fair coin. One simple way to do this is to toss the coin twice, and associate <code>(H,T)</code> as heads, and <code>(T,H)</code> as tails. If we get <code>(H,H), (T,T)</code>, then we discard the result and toss again. Is this the optimal strategy though? Can you do better? What is the fundamental limit on the average number of biased coins you would have to toss do to obtain a fair coin?</p>
<p><strong>3. Biased binary search:</strong> We are familiar that given a sorted array of size $k$, we can locate a query $q$ in one of the $n$ bins using $\lceil \log_2 k \rceil$ comparisons. However, a lot of times, we already have a good idea which bins the query might lie in. Can we accelerate the binary-search in this case? To be more precise, lets say you have a bias that the query will lie in each of the bins with probability $p_1, p_2, \ldots, p_k$. (usually this bias is uniform, i.e $p_i = 1/n$). In such a scenario what is the fundamental limit on the average number of comparisons you have to do to locate the query $q$?</p>
<p>Although all these scenarios seem very different, they are essentially related to <em>Entropy</em> $H(X)$, and use the property that the self information, the amount of bits corresponding to random variable $X$ is $H(X)$. This key intuition led to a very diverse and vibrant area of mathematics called <em>Information Theory</em>. What we saw in today's lecture is just a small glimpse into information theory, but if you are interested please take a look at the course <a href="https://web.stanford.edu/class/ee276/">EE 276</a>, and the resources listed there.</p>
<h2 id="summary-and-next-lecture"><a class="header" href="#summary-and-next-lecture">Summary and next lecture</a></h2>
<p>To summarize this lecture:</p>
<ol>
<li><strong>Entropy:</strong> We took a detour into information theory and understood some properties of entropy $H(X)$.</li>
<li><strong>Entropy as a limit of compression:</strong> We derived the properties of $H(X)$ and used them to show that $H(X)$ is the fundamental limit on the compression, i.e. the average codelength of any prefix-free code designed for compressing a given source $X$ is always lower bounded by $H(X)$.</li>
</ol>
<p>Although we were able to show that $H(X)$ is the fundamental limit on average codelength, and that we can always come within 1 bit of the fundamental limit using Shannon codes, it is a bit unsatisfactory as we don't know if Shannon Code is the optimal prefix code.</p>
<p>In the next lecture, we are doing to discuss <strong>Huffman Code</strong>, which is the answer to the question:</p>
<blockquote>
<p>Given a random variable with instances sampled i.i.d with probability distribution $P = {p_1, p_2, \ldots, p_k}$, what is the best prefix free code for this source?</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="huffman-code"><a class="header" href="#huffman-code">Huffman Code</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>As a recap, in the last lecture we formulated the problem of finding the <em>optimal prefix-free code</em> given a probability distribution $p_1, p_2, \ldots, p_k$ as a concrete optimization problem as follows:</p>
<div id="admonition-optimal-prefix-free-code" class="admonition admonish-note" role="note" aria-labelledby="admonition-optimal-prefix-free-code-title">
<div class="admonition-title">
<div id="admonition-optimal-prefix-free-code-title">
<p>Optimal prefix-free code</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/huffman.html#admonition-optimal-prefix-free-code"></a>
</div>
<div>
<p>The problem of finding <em>optimal prefix-free code</em> given a probability distribution $p_1, p_2, \ldots, p_k$ can be formulated as:</p>
<p>Given probabilities $p_1, p_2, \ldots, p_k$, solve for code-lengths $l_i \in \mathbb{N}$, such that:
$$ \begin{aligned}
\text{minimize   } &amp;L(p) = \sum_{i=1}^k p_i l_i \
\text{under the constraint } &amp;\sum_{i=1}^k 2^{-l_i} \leq 1
\end{aligned} $$</p>
</div>
</div>
<p>We also showed that the optimal average codelength $L(p)$ is lower bounded by $H(p)$, the entropy corresponding to source $p$. (i.e. $L(p) \geq H(p)$). In this lecture we are going to discuss the famous <strong>Huffman codes</strong>, which is in fact (one of) the solutions to the optimization problem above!</p>
<hr />
<p>The problem of finding optimal prefix-free codes eluded researchers for quite some time; until David Huffman, then a mere graduate student in the EE dept at MIT solved it as his course project! Here is an anecdote about the discovery of Huffman codes, borrowed from the <a href="https://www.maa.org/press/periodicals/convergence/discovery-of-huffman-codes">maa webpage</a>.</p>
<blockquote>
<p><em>"The story of the invention of Huffman codes is a great story that demonstrates that students can do better than professors. David Huffman (1925-1999) was a student in an electrical engineering course in 1951. His professor, Robert Fano, offered students a choice of taking a final exam or writing a term paper. Huffman did not want to take the final so he started working on the term paper. The topic of the paper was to find the most efficient (optimal) code. What Professor Fano did not tell his students was the fact that it was an open problem and that he was working on the problem himself. Huffman spent a lot of time on the problem and was ready to give up when the solution suddenly came to him. The code he discovered was optimal, that is, it had the lowest possible average message length. The method that Fano had developed for this problem did not always produce an optimal code. Therefore, Huffman did better than his professor. Later Huffman said that likely he would not have even attempted the problem if he had known that his professor was struggling with it."</em></p>
</blockquote>
<h2 id="huffman-code-construction"><a class="header" href="#huffman-code-construction">Huffman code construction</a></h2>
<p>Even before we get started, it is important to note that <strong>Huffman codes</strong> are <em>one of the</em> optimal prefix-free codes for a given distribution, and that there are multiple of those. It is easy to see why: for example, given codewords <code>0, 10, 110, ...</code> from <strong>Huffman code</strong>, we can just switch <code>0</code> with <code>1</code> and that is another legit optimal prefix-free code  (<code>1, 01, 001, ...</code>). So, we are going to discuss one of the many possible optimal prefix free constructions.</p>
<p>Let's first discuss the construction of the Huffman codes, and then we will discuss the optimality.
Let's start with an example probability distribution:</p>
<pre><code class="language-python">probs_dict = {"A": 0.35, "B": 0.25, "C": 0.2, "D": 0.12, "E": 0.08}
</code></pre>
<p><strong>STEP 0:</strong> The initial step is to first build singleton nodes from these probabilities.</p>
<pre class="mermaid">graph TD
  N1[&quot;A&lt;br/&gt;(p=0.35)&quot;]:::endnode
  N2(&quot;B&lt;br/&gt;(p=0.25)&quot;):::endnode
  N3(&quot;C&lt;br/&gt;(p=0.2)&quot;):::endnode
  N4(&quot;D&lt;br/&gt;(p=0.12)&quot;):::endnode
  N5(&quot;E&lt;br/&gt;(p=0.08)&quot;):::endnode
</pre>
<p>Note that each Node has the following structure.</p>
<pre><code>class Node:
    str symbol_name
    float prob
    Node left = None
    Node right = None
</code></pre>
<p>Each node has a <code>symbol_name</code> and a <code>prob</code> fields.
The node also has <code>left</code> and <code>right</code> fields, which are pointers to its children.</p>
<pre><code class="language-python">node_list = [Node(A, 0.35), Node(B, 0.25), Node(C, 0.2), Node(D, 0.12), Node(E, 0.08)]
</code></pre>
<p>Essentially we now have 5 nodes (or binary trees with a single node). The Huffman tree construction works by joining these nodes in a recursive fashion using the next 2 steps, to construct a single tree.</p>
<p><strong>Step 1:</strong> We pop out the two nodes with the smallest probability from the <code>node_list</code>.
In our example, these are <code>Node(D, 0.12)</code> and <code>Node(E, 0.08)</code></p>
<p>Thus, the <code>node_list</code> now looks like:</p>
<pre><code class="language-python">node_1, node_2 = Node(D, 0.12), Node(E, 0.08)
node_list = [Node(A, 0.35), Node(B, 0.25), Node(C, 0.2)]
</code></pre>
<p><strong>Step 2:</strong> In the next step, we join the two popped nodes and create a new node as a parent of these two nodes. The probability of the new node is equal to the sum of the two children nodes.  In our example, the new node is node <code>N1</code> and has <code>prob = 0.12 + 0.08 = 0.2</code>.</p>
<p>This new node is  re-added to the <code>node_list</code>.<br />
So, now our node list is:</p>
<pre><code class="language-python">## Iter 1
node_list = [Node(A, 0.35), Node(B, 0.25), Node(C, 0.2), Node(N1, 0.2)]
</code></pre>
<pre class="mermaid">graph TD
  N1[&quot;A&lt;br/&gt;(p=0.35)&quot;]:::endnode
  N2(&quot;B&lt;br/&gt;(p=0.25)&quot;):::endnode
  N3(&quot;C&lt;br/&gt;(p=0.2)&quot;):::endnode
  N4(&quot;D&lt;br/&gt;(p=0.12)&quot;):::endnode
  N5(&quot;E&lt;br/&gt;(p=0.08)&quot;):::endnode
  N6(&quot;N1&lt;br/&gt;(p=0.2)&quot;) --&gt; N4
  N6 --&gt; N5

  style N6 fill:#dddddd
</pre>
<p>Let's pause and see what the two steps above did: Step 1,2 reduced the length of <code>node_list</code> by <code>1</code> (as we removed 2 nodes and added 1). We can thus apply <strong>Step 1</strong>, <strong>Step 2</strong> recursively, to obtain a single node/tree. The resulting final tree is our Huffman tree!</p>
<pre><code class="language-python">## Iter 2
node_list = [Node(A, 0.35), Node(B, 0.25), Node(N2, 0.4)]
</code></pre>
<pre class="mermaid">graph TD
  N1[&quot;A&lt;br/&gt;(p=0.35)&quot;]:::endnode
  N2(&quot;B&lt;br/&gt;(p=0.25)&quot;):::endnode
  N3(&quot;C&lt;br/&gt;(p=0.2)&quot;):::endnode
  N4(&quot;D&lt;br/&gt;(p=0.12)&quot;):::endnode
  N5(&quot;E&lt;br/&gt;(p=0.08)&quot;):::endnode
  N6(&quot;N1&lt;br/&gt;(p=0.2)&quot;) --&gt; N4
  N6 --&gt; N5
  N7(&quot;N2&lt;br/&gt;(p=0.4)&quot;) --&gt; N6
  N7 --&gt; N3

  style N6 fill:#dddddd
  style N7 fill:#dddddd
</pre>
<pre><code class="language-python">## Iter 3
node_list = [Node(N2, 0.4), Node(N3, 0.6)]
</code></pre>
<pre class="mermaid">graph TD
  N1[&quot;A&lt;br/&gt;(p=0.35)&quot;]:::endnode
  N2(&quot;B&lt;br/&gt;(p=0.25)&quot;):::endnode
  N3(&quot;C&lt;br/&gt;(p=0.2)&quot;):::endnode
  N4(&quot;D&lt;br/&gt;(p=0.12)&quot;):::endnode
  N5(&quot;E&lt;br/&gt;(p=0.08)&quot;):::endnode
  N6(&quot;N1&lt;br/&gt;(p=0.2)&quot;) --&gt; N4
  N6 --&gt; N5
  N7(&quot;N2&lt;br/&gt;(p=0.4)&quot;) --&gt; N6
  N7 --&gt; N3
  N8(&quot;N3&lt;br/&gt;(p=0.6)&quot;) --&gt; N1
  N8 --&gt; N2

  style N6 fill:#dddddd
  style N7 fill:#dddddd
  style N8 fill:#dddddd
</pre>
<pre><code class="language-python">## Iter 4
node_list = [Node("*", 1.0)]
</code></pre>
<pre class="mermaid">graph TD
  N1[&quot;A&lt;br/&gt;(p=0.35)&quot;]:::endnode
  N2(&quot;B&lt;br/&gt;(p=0.25)&quot;):::endnode
  N3(&quot;C&lt;br/&gt;(p=0.2)&quot;):::endnode
  N4(&quot;D&lt;br/&gt;(p=0.12)&quot;):::endnode
  N5(&quot;E&lt;br/&gt;(p=0.08)&quot;):::endnode
  N6(&quot;N1&lt;br/&gt;(p=0.2)&quot;) --&gt; N4
  N6 --&gt; N5
  N7(&quot;N2&lt;br/&gt;(p=0.4)&quot;) --&gt; N6
  N7 --&gt; N3
  N8(&quot;N3&lt;br/&gt;(p=0.6)&quot;) --&gt; N1
  N8 --&gt; N2
  N9(&quot;*&lt;br/&gt;(p=1.0)&quot;) --&gt; N8
  N9 --&gt; N7

  style N6 fill:#dddddd
  style N7 fill:#dddddd
  style N8 fill:#dddddd
  style N9 fill:#dddddd
</pre>
<p>Thus, our procedure for constructing the Huffman tree can be described in pseudo-code as follows:</p>
<pre><code class="language-python">
# Input -&gt; given prob distribution
probs_dict = {A1: p_1, A2: p_2, ..., Ak: p_k} # p_1, .. are some floating point values

class Node:
    str symbol_name
    float prob
    Node left = None
    Node right = None

    
def build_huffman_tree(probs_array):
    ## STEP 0: initialize the node_list with singleton nodes
    node_list = [Node(s,prob) for s,prob in probs_array.items()]

    # NOTE: at each iter, we are reducing length of node_list by 1
    while len(node_list) &gt; 1:
        ## STEP 1: sort node list based on probability,
        # and pop the two smallest nodes 
        node_list = sort(node_list) # based on node.prob
        node_0, node_1 = node_list.pop(0, 1) # remove the smallest and second smallest elements

        ## STEP 2: Merge the two nodes into a single node
        new_node_prob = node_0.prob + node_1.prob
        new_node = Node(symbol_name="", prob=new_node_prob, left=node_0, right=node_1)
        node_list.append(new_merged_node)
    
    # finally we will have a single node/tree.. return the node as it points to the 
    # root node of our Huffman tree
    return node_list[0] 
</code></pre>
<p>Quite cool! We are essentially constructing the tree in by stitching one node at a time, until we have a single tree. Nodes with smaller probabilities join into a sub-tree earlier and thus, they will likely have higher depth in the final tree (and thus longer codelength). As we will see this greedy approach is in fact optimal!</p>
<p>Once the Huffman tree is constructed, we can use this tree as a prefix-free code for encoding/decoding data.
For example, in our case the table we get is:</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>Codewords</th></tr></thead><tbody>
<tr><td>A</td><td>00</td></tr>
<tr><td>B</td><td>01</td></tr>
<tr><td>C</td><td>10</td></tr>
<tr><td>D</td><td>110</td></tr>
<tr><td>E</td><td>111</td></tr>
</tbody></table>
</div>
<p>Here is another example below of the Huffman code constructed on a typical text of english language. Notice that vowels, punctuations have much shorter length as they occur quite frequently.</p>
<p><img src="https://liucs.net/cs101s19/data/69/d3381f-1f93-433a-a85c-851c43ba6832/mathemaniac-keyboard-histo.svg" alt="img2" /></p>
<p><img src="https://liucs.net/cs101s19/data/69/d3381f-1f93-433a-a85c-851c43ba6832/huffenc.svg" alt="img1" /></p>
<h3 id="optimizing-the-tree-construction"><a class="header" href="#optimizing-the-tree-construction">Optimizing the tree construction</a></h3>
<p>There are certain optimizations which can be performed in the Huffman tree construction:</p>
<p>For example, <code>STEP 1</code> in the pseudocode can be replaced with a priority queue, as we mainly need top two elements and nothing more. Here is the full code implementation in the Stanford Compression Library: <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/compressors/huffman_coder.py">https://github.com/kedartatwawadi/stanford_compression_library/blob/main/compressors/huffman_coder.py</a>.</p>
<h2 id="optimality-of-huffman-codes"><a class="header" href="#optimality-of-huffman-codes">Optimality of Huffman codes</a></h2>
<p>To get some intuition into the Huffman code optimality, let's think about what are the necessary conditions for a prefix-free code to be optimal? Let me list a couple of conditions, and we will see why they are true:</p>
<div id="admonition-theorem-8-necessary-conditions-for-prefix-free-code-optimality" class="admonition admonish-example" role="note" aria-labelledby="admonition-theorem-8-necessary-conditions-for-prefix-free-code-optimality-title">
<div class="admonition-title">
<div id="admonition-theorem-8-necessary-conditions-for-prefix-free-code-optimality-title">
<p>Theorem-8: Necessary conditions for prefix-free code optimality</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/huffman.html#admonition-theorem-8-necessary-conditions-for-prefix-free-code-optimality"></a>
</div>
<div>
<p>Any optimal prefix free code for a distribution $P= { p_1, p_2, \ldots, p_k }$ must satisfy the following conditions: <br>
<strong>Condition 1</strong> The code-lengths are ordered in inverse order of the probabilities, i.e.
$$ p_i &gt; p_j \Rightarrow l_i \leq l_j$$
<strong>Condition 2</strong> The two longest codewords have the same code-lengths</p>
</div>
</div>
<h4 id="condition-1-proof"><a class="header" href="#condition-1-proof"><strong>Condition 1</strong> proof</a></h4>
<p><strong>Condition 1</strong> is quite obvious but fundamental: it just says that if the probability of a symbol is higher than it necessarily has to have a shorter codelength. We have seen condition <strong>Condition 1</strong> before, as a thumb-rule. But, lets show that it is explicitly true in this case. Let's assume that our code $\mathcal{C}$ is optimal but for  $p_i &gt; p_j$ has $l_i &gt; l_j$.
Let the average codelength of code $\mathcal{C}$ be $L_1$. Now, lets consider a prefix-free code $\hat{\mathcal{C}}$ where we exchange the codewords corresponding to <code>i</code> and <code>j</code>. Thus, the new code-lengths $\hat{l}_i$ are:
$$
\begin{aligned}
\hat{l}_r &amp;= l_r, \text{ if } r \neq i \text{ and } r \neq j \
\hat{l}_i &amp;= l_j \
\hat{l}_j &amp;= l_i
\end{aligned}
$$</p>
<p>The new average codelength $L_2$ is:
$$
\begin{aligned}
L_2 &amp;= \sum_{i=1}^k p_r \hat{l}_r \
&amp;= L_1 + p_i (\hat{l}_i - l_i)  + p_j (\hat{l}_j - l_j) \
&amp;= L_1 + p_i (l_j - l_i)  + p_j (l_i - l_j) \
&amp;= L_1 - (l_i - l_j)(p_i - p_j) \
\end{aligned}
$$</p>
<p>As $(l_i - l_j)(p_i - p_j) &gt; 0$, it implies that $L_2 &lt; L_1$, which is in contradiction to our assumption that code $\mathcal{C}$ is optimal. This proves the <strong>Condition 1</strong>.</p>
<h4 id="condition-2-proof"><a class="header" href="#condition-2-proof"><strong>Condition 2</strong> proof</a></h4>
<p>The <strong>Condition 2</strong> is also equally simple to prove. Let's show it by contradiction again. For example, let $c_k = 0110101$ be the unique longest codeword. Then, as there is no other codeword of length $7$, we can be sure that we can drop one bit from $c_k$, and the resulting code will still be a valid prefix-free code. Thus, we can assign $\hat{c}_k = 011010$, and we get a shorter average codelength!</p>
<p>As we can see, these two properties are satisfied by the construction procedure of Huffman coding.</p>
<ol>
<li>The symbols with higher probability are chosen later, so their code-lengths are lower.</li>
<li>We always choose the two symbols with the smallest probability and combine them, so the <strong>Condition 2</strong> is also satisfied.</li>
</ol>
<p>Note that these are just observations about Huffman construction and don't prove its optimality. The optimality proof is a bit more technical, and we refer you to the <a href="http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf">Cover, Thomas text, section 5.8</a>.</p>
<p>The main property is that after the <em>merge</em> step of merging two smallest probability nodes of probability distribution $(p_1, p_2, \ldots, p_k)$, the remaining tree is still optimal for the distribution $(p_1, p_2, \ldots, p_{k-1} + p_k)$.</p>
<h2 id="practical-prefix-free-coding"><a class="header" href="#practical-prefix-free-coding">Practical prefix-free coding</a></h2>
<p>Huffman codes are actually used in practice due to their optimality and relatively convenient construction! Here are some examples:</p>
<ul>
<li>
<p>http/2 header compression: <a href="https://www.rfc-editor.org/rfc/rfc7541#appendix-B">https://www.rfc-editor.org/rfc/rfc7541#appendix-B</a></p>
</li>
<li>
<p>ZLIB, DEFLATE, GZIP, PNG compression: <a href="https://www.ietf.org/rfc/rfc1951.txt">https://www.ietf.org/rfc/rfc1951.txt</a></p>
</li>
<li>
<p>JPEG huffman coding tables: <a href="https://www.w3.org/Graphics/JPEG/itu-t81.pdf">https://www.w3.org/Graphics/JPEG/itu-t81.pdf</a></p>
</li>
</ul>
<p>A lot of times in practice, the data is split into streams and after multiple transformations needs to be losslessly compressed. This is the place where Huffman codes are used typically.</p>
<h3 id="fast-prefix-free-decoding"><a class="header" href="#fast-prefix-free-decoding">Fast Prefix-free decoding</a></h3>
<p>In practice, Huffman coders lie at the end of any compression pipeline and need to be extremely fast. Let's look at the typical encoding/decoding and see how we can speed it up.</p>
<p>For simplicity and clarity we will work with a small exmaple. Let's say this is our given code:</p>
<pre><code class="language-py">encoding_table = {"A": 0, "B": 10, "C": 110, "D": 111}
</code></pre>
<ol>
<li>The encoding itself is quite simple and involves looking at a lookup table.</li>
</ol>
<pre><code class="language-py">
## Prefix-free code encoding
encoding_table = {"A": 0, "B": 10, "C": 110, "D": 111}
def encode_symbol(s):
    bitarray = encoding_table[s]
    return bitarray, len(bitarray)
</code></pre>
<p>The typical prefix-free code decoding works by parsing through the prefix-free tree, until we reach a leaf node, e.g. see the code in SCL: <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/compressors/prefix_free_compressors.py">https://github.com/kedartatwawadi/stanford_compression_library/blob/main/compressors/prefix_free_compressors.py</a>. Here is a pseudo-code for the decoding algorithm:</p>
<pre><code class="language-py">## Prefix-free code decoding: 
prefix_free_tree = ... # construct a tree from the encoding_table
def decode_symbol_tree(encoded_bitarray):
    # initialize num_bits_consumed
    num_bits_consumed = 0

    # continue decoding until we reach leaf node
    curr_node = prefix_free_tree.root_node
    while not curr_node.is_leaf_node:
        bit = encoded_bitarray[num_bits_consumed]
        if bit == 0:
            curr_node = curr_node.left_child
        else:
            curr_node = curr_node.right_child
        num_bits_consumed += 1

    # as we reach the leaf node, the decoded symbol is the id of the node
    decoded_symbol = curr_node.id
    return decoded_symbol, num_bits_consumed
</code></pre>
<p>Although this is quite simple, there are a lot of branching instructions (e.g. <code>if, else</code> conditions). These instructions are not the fastest instructions on modern architectures. So, here is an alternative decoding procedure for this example:</p>
<pre><code class="language-py">## faster decoding
state_size = 3
decode_state = {
    000: "A",
    001: "A",
    010: "A",
    011: "A",
    100: "B",
    101: "B",
    110: "C",
    111: "D",
}
codelen_table = {
    "A": 1,
    "B": 2,
    "C": 3,
    "D": 3,
}
def decode_symbol_tree(encoded_bitarray):
    state = encoded_bitarray[:state_size] 
    decoded_symbol = decode_state[state]
    num_bits_consumed = codelen_table[decoded_symbol]
    return decoded_symbol, num_bits_consumed
</code></pre>
<p>Looking at the new decoding pseudo-code, we see following observations:</p>
<ol>
<li>The <code>state_size=3</code> corresponds to the maximum depth of the prefix-free tree.</li>
<li>The <code>state</code> is the first <code>3</code> bits of the <code>encoded_bitarray</code>. We use the <code>state</code> as an index to query thr <code>decode_state</code> lookup table to obtain the <code>decoded_symbol</code>.</li>
<li>As the <code>state</code> is sometimes reading in more bits that what we wrote, we need to output the <code>num_bits_consumed</code> correctly to be <code>codelen_table[decoded_symbol]</code>.</li>
</ol>
<p>One natural question is why should the <code>decoded_symbol = decode_state[state]</code> operation output the correct state? The reason is because of the prefix-free nature of the codes. So, if we look at the entries in <code>decode_state</code> table corresponding to <code>B</code>, we see that these are <code>100, 101</code> both of which start with <code>10</code> which is the codeword corresponding to <code>B</code>.</p>
<p>Based on first look, the decoding is indeed simpler with this new decoding procedure, but we do have to pay some cost. The cost is in terms of memory consumption. For example, we now need to maintain a table <code>decode_state</code> which has size $2^{|maxdepth|}$. This is clearly more memory than a typical prefix-free tree. In a way we are doing some caching. But for caching to work properly, the table sizes need to be small. The bigger the lookup table, the slower the table lookup is. So, practically the maximum leaf depth needs to be kept in check.</p>
<p>Thus, practically we want to get a Huffman tree with a maximum leaf depth constraint. Typically, the depth is also set to be <code>16</code> or <code>24</code> so that it is a byte multiple. For more details on efficient Huffman Coding, we encourage the reader to have a look at this blog entry by Fabian Giesen on Huffman coding: <a href="https://fgiesen.wordpress.com/2021/08/30/entropy-coding-in-oodle-data-huffman-coding/?hmsr=joyk.com&amp;utm_source=joyk.com&amp;utm_medium=referral">https://fgiesen.wordpress.com/2021/08/30/entropy-coding-in-oodle-data-huffman-coding/?hmsr=joyk.com&amp;utm_source=joyk.com&amp;utm_medium=referral</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="asymptotic-equipartition-property"><a class="header" href="#asymptotic-equipartition-property">Asymptotic Equipartition Property</a></h1>
<p>We have been looking at the problem of data compression, algorithms for the same as well as fundamental limits on the compression rate. In this chapter we will approach the problem of data compression in a very different way. The basic idea of asymptotic equipartition property is to consider the space of all possible sequences produced by a stochastic (random) source, and focusing attention on the "typical" ones. The theory is asymptotic in the sense that many of the results focus on the regime of large source sequences. This technique allows us to recover many of the results for compression but in a non-constructive manner. Beyond compression, the same concept has allowed researchers to obtain powerful achievability and impossibility results even before a practical algorithm was developed. Our presentation here will focus on the main concepts, and present the mathematical intuition behind the results. We refer the reader to Chapter 3 in Cover and Thomas and to Information Theory lecture notes (available <a href="https://web.stanford.edu/class/ee376a/files/2017-18/lecture_4.pdf">here</a>) for some details and proofs.</p>
<p>Before we get started, we define the notation used throughout this chapter:</p>
<ol>
<li><strong>Alphabet</strong>: $\mathcal{U} = {1,2,\dots,r}$ specifies the possible values that the random variable of interest can take.</li>
<li><strong>iid source</strong>: An independent and identically distributed (iid) source produces a sequence of random variables that are independent of each other and have the same distribution, e.g. a sequence of tosses of a coin. We use $U_1, U_2, \dots \mathrm{iid} \sim U$ notation for iid random variables from distribution $U$.</li>
<li><strong>Source sequence</strong>: $U^n = (U_1,\dots,U_n)$ denotes the $n$-tuple representing $n$ source symbols, and $\mathcal{U}^n$ represents the set of all possible $U^n$. Note that we use lowercase $u^n$ for a particular realization of $U^n$.</li>
<li><strong>Probability</strong>: Under the iid source model, we simply have $P(U^n) = \Pi_{i=1}^n P(U_i)$ where we slightly abuse the notation by using $P$ to represent both the probability function of the $n$-tuple $U^n$ and that for a given symbol $U_i$.</li>
</ol>
<p>As an example, we could have alphabet $\mathcal{U} = {0, 1}$ and a source distributed iid according to $P(U_i = 0) = 0.3 = 1- P(U_i = 1)$. Then the source sequence $u^3 = (1,0,0)$ has probability $0.7\times0.3\times0.3=0.063$.</p>
<p>Before we start the discussion of asymptotic equipartition property and typical sequences, let us recall the (weak) law of large numbers (LLN) that says the empirical average of a sequence of random variables converges towards their expectation (to understand the different types of convergence of random variables and the strong LLN, please refer to a probability textbook).</p>
<div id="admonition-theorem-1-weak-law-of-large-numbers" class="admonition admonish-example" role="note" aria-labelledby="admonition-theorem-1-weak-law-of-large-numbers-title">
<div class="admonition-title">
<div id="admonition-theorem-1-weak-law-of-large-numbers-title">
<p>Theorem-1: Weak Law of Large Numbers</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/aep.html#admonition-theorem-1-weak-law-of-large-numbers"></a>
</div>
<div>
<p>For $U_1, U_2, \dots \mathrm{iid} \sim U$ with alphabet $\mathcal{U} = {1,2,\dots,r}$, we have the following for any $\epsilon &gt; 0$:
$$\lim_{n\rightarrow \infty} \mathrm{P}\left(\left|\frac{1}{n}\sum_{i=1}^n {U_i} - E[U]\right| &lt; \epsilon \right) = 1$$</p>
</div>
</div>
<p>That is for arbitarily small $\epsilon &gt; 0$, as $n$ becomes large, the probability that the empirical average is within $\epsilon$ of the expectation is $1$.</p>
<p>With this background, we are ready to jump in!</p>
<h2 id="the-epsilon-typical-set"><a class="header" href="#the-epsilon-typical-set">The $\epsilon$-typical set</a></h2>
<div id="admonition-definition-1" class="admonition admonish-example" role="note" aria-labelledby="admonition-definition-1-title">
<div class="admonition-title">
<div id="admonition-definition-1-title">
<p>Definition-1</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/aep.html#admonition-definition-1"></a>
</div>
<div>
<p>For some $\epsilon &gt; 0$, the source sequence $U^n$ is <strong>$\epsilon$-typical</strong> if
$$\left|-\frac{1}{n}\log P(U^n) - H(U)\right| \leq \epsilon$$</p>
</div>
</div>
Recall that $H(U)$ is the entropy of the random variable $U$. It is easy to see that the condition is equivalent to saying that $2^{-n(H(U)+\epsilon)} \leq P(U^n) \leq 2^{-n(H(U)-\epsilon)}$. The set of all **$\epsilon$-typical** sequences is called the **$\epsilon$-typical set** denoted as $A_{\epsilon}^{(n)}$. Put in words, the **$\epsilon$-typical set** contains all $n$-length sequences whose probability is close to $2^{-nH(U)}$. 
<p>Next, we look at some probabilities of the typical set:</p>
<div id="admonition-theorem-2-properties-of-typical-sets" class="admonition admonish-example" role="note" aria-labelledby="admonition-theorem-2-properties-of-typical-sets-title">
<div class="admonition-title">
<div id="admonition-theorem-2-properties-of-typical-sets-title">
<p>Theorem-2: Properties of typical sets</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/aep.html#admonition-theorem-2-properties-of-typical-sets"></a>
</div>
<div>
<p>For any $\epsilon &gt; 0$,</p>
<ol>
<li>$\lim_{n\rightarrow \infty} P(U^n \in A_{\epsilon}^{(n)}) = 1$.</li>
<li>$\left|A_{\epsilon}^{(n)}\right| \leq 2^{n(H(U)+\epsilon)}$</li>
<li>For large enough $n$, $\left|A_{\epsilon}^{(n)}\right| \geq (1-\epsilon)2^{n(H(U)-\epsilon)}$</li>
</ol>
<p>Simply put, this is saying that for large $n$, the typical set has probability close to $1$ and size roughly $2^{n(H(U)}$</p>
</div>
</div>
<h3 id="proof-sketch-for-theorem-2"><a class="header" href="#proof-sketch-for-theorem-2">Proof sketch for Theorem-2</a></h3>
<ol>
<li>This follows directly from the Weak LLN by noting the definition of typical sequences and the following facts:
(i) $-\frac{1}{n}\log P(U^n) = \frac{1}{n}\sum - \log P(U_i)$ (since this is iid).
(ii) $H(U) = E[-\log P(U)]$ from the definition of entropy.
Thus applying the LLN on $-\log P(U)$ instead of $U$ directly gives the desired result.</li>
</ol>
<p>Both 2 &amp; 3 follow from the definition of typical sequences (which roughly says the probability of each typical sequence is close to $2^{-nH(U)}$) and the fact that the typical set has probability close to $1$ (less than $1$ just because total probability is atmost $1$ and close to $1$ due to property 1).</p>
<h3 id="intuition-into-the-typical-set"><a class="header" href="#intuition-into-the-typical-set">Intuition into the typical set</a></h3>
<p>To gain intuition into the typical set and its properties, let $|\mathcal{U}| = r$ and consider the set of all $n$ length sequences $U^n$ with size $r^n$. Then the AEP says that (for $n$ sufficiently large), all the probability mass is concentrated in an exponentially smaller subset of size $2^{nH(U)}$. That is,
$$\frac{|A_{\epsilon}^{(n)}|}{|\mathcal{U}^n|} \approx \frac{2^{nH(U)}}{r^n} = 2^{-n(\log r - H(U))}$$</p>
<p>Furthermore, all the elements in the typical set $A_{\epsilon}^{(n)}$ are roughly equiprobable, each with probability $2^{-nH(U)}$. Thus, $\mathcal{U}^n$ contains within itself a subset that contains almost the entire probability mass, and within the subset the elements are roughly uniformly distributed. It is important to note that these properties hold only for large enough $n$ since ultimately these are derived from the law of large numbers. The intuition into typical sets is illustrated in the figure below.</p>
<img src="lossless_iid/images/typical_set.png" style="border:1px solid black;" alt="Typical set">
<div id="admonition-quiz-1-intuition-into-the-typical-set" class="admonition admonish-question" role="note" aria-labelledby="admonition-quiz-1-intuition-into-the-typical-set-title">
<div class="admonition-title">
<div id="admonition-quiz-1-intuition-into-the-typical-set-title">
<p>Quiz-1: Intuition into the typical set</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/aep.html#admonition-quiz-1-intuition-into-the-typical-set"></a>
</div>
<div>
<p>What does the elements of the typical set look like?</p>
<ol>
<li>Consider a binary source with $P(0)=P(1)=0.5$. What is the size of the typical set $A_{\epsilon}^{(n)}$ (Hint: this doesn't depend on $\epsilon$!)? Which elements of ${0,1}^n$ are typical? After looking at this example, can you still claim the typical set is exponentially smaller than the set of all sequences <em>in all cases</em>?</li>
<li>Now consider a binary source with $P(0)=1-P(1)=0.2$. Recalling the fact that typical elements have probability around $2^{-nH(U)}$, what can you say about the elements of the typical set? (Hint: what fraction of zeros and ones would you expect a <em>typical</em> sequence to have? Check if your intuition matches with the $2^{-nH(U)}$ expression.)</li>
<li>In part 2, you can easily verify that the most probable sequence is the sequence consisting of all $1$s. Is that sequence typical (for small $\epsilon$)? If not, how do you square that off with the fact that the typical set contains almost all of the probability mass?</li>
</ol>
</div>
</div>
<p>Before we discuss the ramifications of the above for compressibility of sequences, we introduce one further property of subsets of $\mathcal{U}^n$. This property says that any set substantially smaller than the typical set has negligible probability mass. Intuitively this holds because all elements in the typical set have roughly the same probability, and hence removing a large fraction of them leaves very little probability mass remaining. In other words, very roughly the property says that the typical set is the smallest set containing most of the probability. This property will be very useful below when we link typical sets to compression. Here we just state the theorem and leave the proof to the references.</p>
<div id="admonition-theorem-2-sets-exponentially-smaller-than-the-typical-set" class="admonition admonish-example" role="note" aria-labelledby="admonition-theorem-2-sets-exponentially-smaller-than-the-typical-set-title">
<div class="admonition-title">
<div id="admonition-theorem-2-sets-exponentially-smaller-than-the-typical-set-title">
<p>Theorem-2: Sets exponentially smaller than the typical set</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/aep.html#admonition-theorem-2-sets-exponentially-smaller-than-the-typical-set"></a>
</div>
<div>
<p>Fix $\delta &gt; 0$ and $B^{(n)} \subseteq \mathcal{U}^n$ such that $|B^{(n)}| \leq 2^{n(H(U)-\delta)}$. Then
$$\lim_{n\rightarrow \infty} P(U^n \in B^{(n)}) = 0$$</p>
</div>
</div>
<h2 id="compression-based-on-typical-sets"><a class="header" href="#compression-based-on-typical-sets">Compression based on typical sets</a></h2>
<p>Suppose you were tasked with developing a compressor for a source with $2^k$ possible values, each of them equally likely to occur. It is easy to verify that a simple fixed-length code that encodes each of the value with $k$ bits is optimal for this source. But, if you think about the AEP property above, as $n$ grows, almost all the probability in the set of $n$-length sequences over alphabet $\mathcal{U}$ is contained in the typical set with roughly $2^k$ elements (where $k=nH(U)$). And the elements within the typical set are (roughly) equally likely to occur. Ignoring the non-typical elements for a moment, we can encode the typical elements with $nH(U)$ bits using the simple logic mentioned earlier. We have encoded $n$ input symbols with $nH(U)$ bits, effectively using $H(U)$ bits/symbol! This was not truly lossless because we fail to represent the non-typical sequences. But this can be considered near-lossless since the non-typical sequences have probability close to $0$, and hence this code is lossless for a given input with very high probability. Note that we ignored the $\epsilon$'s and $\delta$'s in the treatment above, but that shouldn't take away from the main conclusions that become truer and truer as $n$ grows.</p>
<p>On the flip side, suppose we wanted to encode the elements in $\mathcal{U}$ with $n(H(U)-\delta)$ bits for some $\delta &gt; 0$. Now, $n(H(U)-\delta)$ bits can represent $2^{n(H(U)-\delta)}$ elements. But according to theorem 2 above, the set of elements correctly represented by such a code has negligible probability as $n$ grows. This means a fixed-length code using less than $H(U)$ bits per symbol is unable to losslessly represent an input sequence with very high probability. Thus, using AEP we can prove the fact that any fixed-length <em>near-lossless</em> compression scheme must use at least $H(U)$ bits per symbol.</p>
<h3 id="lossless-compression-scheme"><a class="header" href="#lossless-compression-scheme">Lossless compression scheme</a></h3>
<p>Let us now develop a lossless compression algorithm based on the AEP, this time being very precise. As before, we focus on encoding sequences of length $n$. Note that a lossless compression algorithm aiming to achieve entropy must be variable length (unless the source itself is uniformly distributed). And the AEP teaches us that elements in the typical set should ideally be represented using $\approx nH(U)$ bits. With this in mind, consider the following scheme:</p>
<div id="admonition-lossless-compression-using-typical-sets" class="admonition admonish-info" role="note" aria-labelledby="admonition-lossless-compression-using-typical-sets-title">
<div class="admonition-title">
<div id="admonition-lossless-compression-using-typical-sets-title">
<p>Lossless compression using typical sets</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/aep.html#admonition-lossless-compression-using-typical-sets"></a>
</div>
<div>
<p>Fix $\epsilon &gt; 0$, and assign index $idx$ ranging from $1,\dots,|A_{\epsilon}^{(n)}|$ to the elements in $A_{\epsilon}^{(n)}$ (the order doesn't matter). In addition, define a fixed length code $fixed$ for $\mathcal{U}^n$ that uses $n\log_2 |\mathcal{U}|$ bits to encode any input sequence. Now the encoding of $u^n$ is simply:</p>
<ul>
<li>if $u^n \in A_{\epsilon}^{(n)}$, encode as $0$ followed by $idx(u^n)$</li>
<li>else, encode as $1$ followed by $fixed(u^n)$</li>
</ul>
</div>
</div>
<p>Let's calculate the expected code length (in bits per symbol) used by the above scheme. For $n$ large enough, we can safely assume that $P(U^n \in A_{\epsilon}^{(n)}) \geq 1-\epsilon$ by the AEP. Furthermore, we know that $idx(u^n)$ needs at most $H(U) + \epsilon$ bits to represent (theorem-1 part 2). Thus, denoting the code length by $l$, we have
$$E[l(U^n)] \leq (1-\epsilon)(1+n(H(U) + \epsilon)) + \epsilon (1+n\log_2 |\mathcal{U}|)$$
where the first term corresponds to the typical sequences and the second term to everything else (not that we use the simplest possible encoding for non-typical sequences since they don't really matter in terms of their probability). Also note that we add $1$ to each of the lengths to account for the $0$ or $1$ we prefix in the scheme above. Simplifying the above, we have
$$\frac{E[l(U^n)]}{n} = (1-\epsilon)(H(U) + \epsilon) + \epsilon \log_2 |\mathcal{U}| + \frac{1}{n}$$
$$\frac{E[l(U^n)]}{n} = H(U) + O(\epsilon) + \frac{1}{n}$$
where $O(\epsilon)$ represents terms bounded by $c\epsilon$ for some constant $c$ when $\epsilon$ is small. Thus we can achieve code lengths arbitrary close to $H(U)$ bits/symbol by selecting $\epsilon$ and $n$ appropriately!</p>
<div id="admonition-quiz-2-lossless-compression-based-on-typical-sets" class="admonition admonish-question" role="note" aria-labelledby="admonition-quiz-2-lossless-compression-based-on-typical-sets-title">
<div class="admonition-title">
<div id="admonition-quiz-2-lossless-compression-based-on-typical-sets-title">
<p>Quiz-2: Lossless compression based on typical sets</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/aep.html#admonition-quiz-2-lossless-compression-based-on-typical-sets"></a>
</div>
<div>
<ol>
<li>Describe the decoding algorithm for the above scheme and verify that the scheme is indeed lossless.</li>
<li>What is the complexity of the encoding and decoding procedure as a function of $n$? Consider both the time and memory usage. Is this a practical scheme for large values of $n$?</li>
</ol>
</div>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="arithmetic-coding"><a class="header" href="#arithmetic-coding">Arithmetic coding</a></h1>
<p>In this chapter, we discuss arithmetic coding, which is a major step in the direction of achieving entropy without sacrificing efficiency. While here we will discuss the case of data from known iid distributions, arithmetic coding is versatile and is most useful in cases of complex adaptive data distributions (to be discussed in the following chapters). In addition, as we will see later, arithmetic coding plays a crucial role in understanding the link between compression and prediction.</p>
<h2 id="recap---issues-with-symbol-codes"><a class="header" href="#recap---issues-with-symbol-codes">Recap -&gt; Issues with symbol codes:</a></h2>
<p>Lets consider the simple distribution below and think about what designing prefix-free codes for the same</p>
<pre><code class="language-py">P = {A: 0.1, B: 0.9}
H(P) = 0.47
</code></pre>
<p>It is clear that as there are just two symbols, the only reasonable prefix-free code design here is:</p>
<pre><code>A -&gt; 0, B -&gt; 1
Huffman code (or any other prefix-free code) = 1 bit
</code></pre>
<p>So the average codelength for huffman code for this data is 1 bit. If we compare it with the entropy $H(P) = 0.47$ of the distribution,
our average codelength (<code>1 bit</code>) is quite off. Ideally for a symbol $s$, ideally we want to use $l(s) = \log_2 \frac{1}{P(s)}$ bits (to achieve $H(P)$). But, as we are using a symbol code, we can't use fractional bits.
Thus, there is always going to an overhead of up to  <code>~1 bit</code> per symbol with symbol codes. This is consistent with the guarantees on the expected codelength $L \leq H(P) + 1$ with Huffman codes.</p>
<p>One solution to this problem is to use considering block codes. i.e. we consider tuples of symbols from the alphabet as a new "symbol" and construct a Huffman code using that.
For example, we can consider blocks of size 2: <code>P = {AA: 0.01, AB: 0.09, BA: 0.09, BB: 0.81}</code>. Here is the Huffman tree:</p>
<pre><code class="language-py">block_size: 1, entropy: 0.47, avg_codelen: 1.00 bits/symbol

  |--A
Â·-|  
  |--B
block_size: 2, entropy: 0.47, avg_codelen: 0.65 bits/symbol

       |--BA
  |--Â·-|  
  |    |    |--AA
  |    |--Â·-|  
  |         |--AB
Â·-|  
  |--BB
</code></pre>
<p>We see that <code>BB</code> has probability <code>0.81</code>, so it receives the shortest codelength of <code>1 bit</code>. The average codelength in this case is <code>0.65 bits/symbol</code> which is definitely an improvement over the <code>1 bit/symbol</code>.
The idea here is that, as we are using fixed codes per "symbol", as now the symbols are tuples of size 2, the overhead gets amortized to 2 symbols from the alphabet.</p>
<p>We can continue doing this by considering larger blocks of data. For example, with blocks of size $k$, the new alphabet size is $2^k$, and the overhead due to Huffman coding should be <code>~1/k</code> (or lower).
Here is an example of applying the Huffman coding on blocks of size 3,4,5:</p>
<pre><code class="language-py">block_size: 3, entropy: 0.47, avg_codelen: 0.53 bits/symbol

                      |--AAA
                 |--Â·-|  
                 |    |--BAA
            |--Â·-|  
            |    |    |--ABA
            |    |--Â·-|  
            |         |--AAB
       |--Â·-|  
       |    |--ABB
  |--Â·-|  
  |    |    |--BBA
  |    |--Â·-|  
  |         |--BAB
Â·-|  
  |--BBB

</code></pre>
<pre><code class="language-py">## Huffman code for blocks
block_size: 1, entropy: 0.47, avg_codelen: 1.00 bits/symbol
block_size: 2, entropy: 0.47, avg_codelen: 0.65 bits/symbol
block_size: 3, entropy: 0.47, avg_codelen: 0.53 bits/symbol
block_size: 4, entropy: 0.47, avg_codelen: 0.49 bits/symbol
block_size: 5, entropy: 0.47, avg_codelen: 0.48 bits/symbol
</code></pre>
<p>We already see that the with block size <code>4</code> we are already quite close to the entropy of <code>0.47</code>. So, this is quite great, and should help us solve our problem of reaching the entropy limit.
In general the convergence of Huffman codes on blocks goes as:</p>
<ol>
<li>Huffman codes</li>
</ol>
<p>$$ H(X) \leq \mathbb{E}[l(X)] \leq H(X) + 1 $$</p>
<ol start="2">
<li>Huffman codes on blocks of size B</li>
</ol>
<p>$$ H(X) \leq \frac{\mathbb{E}[l(X_1^B)]}{B} \leq H(X) + \frac{1}{B}$$</p>
<p>However there is a practical issue. Do you see it? As a hint (and also because it looks cool!) here is the huffman tree for block size <code>5</code>:</p>
<pre><code class="language-py">block_size: 5, entropy: 0.47, avg_codelen: 0.48 bits/symbol

            |--BABBB
       |--Â·-|  
       |    |              |--BBABA
       |    |         |--Â·-|  
       |    |         |    |--ABBBA
       |    |    |--Â·-|  
       |    |    |    |    |--BABBA
       |    |    |    |--Â·-|  
       |    |    |         |--AABBB
       |    |--Â·-|  
       |         |                        |--BBAAA
       |         |                   |--Â·-|  
       |         |                   |    |--AABAB
       |         |              |--Â·-|  
       |         |              |    |    |--AAABB
       |         |              |    |--Â·-|  
       |         |              |         |--ABBAA
       |         |         |--Â·-|  
       |         |         |    |         |--AABBA
       |         |         |    |    |--Â·-|  
       |         |         |    |    |    |              |--AAAAB
       |         |         |    |    |    |         |--Â·-|  
       |         |         |    |    |    |         |    |--AABAA
       |         |         |    |    |    |    |--Â·-|  
       |         |         |    |    |    |    |    |         |--AAAAA
       |         |         |    |    |    |    |    |    |--Â·-|  
       |         |         |    |    |    |    |    |    |    |--BAAAA
       |         |         |    |    |    |    |    |--Â·-|  
       |         |         |    |    |    |    |         |    |--ABAAA
       |         |         |    |    |    |    |         |--Â·-|  
       |         |         |    |    |    |    |              |--AAABA
       |         |         |    |    |    |--Â·-|  
       |         |         |    |    |         |--BAAAB
       |         |         |    |--Â·-|  
       |         |         |         |         |--BABAA
       |         |         |         |    |--Â·-|  
       |         |         |         |    |    |--ABABA
       |         |         |         |--Â·-|  
       |         |         |              |    |--BAABA
       |         |         |              |--Â·-|  
       |         |         |                   |--ABAAB
       |         |    |--Â·-|  
       |         |    |    |    |--ABBAB
       |         |    |    |--Â·-|  
       |         |    |         |--BABAB
       |         |--Â·-|  
       |              |         |--BBAAB
       |              |    |--Â·-|  
       |              |    |    |--BAABB
       |              |--Â·-|  
       |                   |    |--ABABB
       |                   |--Â·-|  
       |                        |--BBBAA
  |--Â·-|  
  |    |         |--BBBAB
  |    |    |--Â·-|  
  |    |    |    |--BBBBA
  |    |--Â·-|  
  |         |    |--BBABB
  |         |--Â·-|  
  |              |--ABBBB
Â·-|  
  |--BBBBB

</code></pre>
<p>The issue is that, as we increase the block size $B$, our codebook size increases exponentially as $|\mathcal{X}|^B$.
The larger the codebook size the more complicated the encoding/decoding becomes, the more memory we need, the higher the latency etc. For example, if we look at the tree above, the codebook size is $2^5 = 32$. This is quite manageable.
However, this wouldn't have been the case if our alphabet size was $256$ for a byte coder instead of $2$. In that case, block code of size <code>5</code> has a codebook = $2^{40}$, which is definitely unmanageable.</p>
<p>Thus, even though with block size $B$, we obtain compression as close as <code>1/B bits/symbol</code> to entropy $H(X)$, the idea doesn't hold ground practically.
That is, in fact the problem which Arithmetic coding solves.</p>
<h2 id="arithmetic-coding-introduction"><a class="header" href="#arithmetic-coding-introduction">Arithmetic coding: Introduction</a></h2>
<p>Arithmetic coding solves the problems we discussed with the block-based Huffman coder:</p>
<ol>
<li>
<p><strong>Entire data as a single block</strong>: Arithmetic coding encodes entire data as a single block: For data $x_1^n$, the <code>block_size = n</code>
i.e. the entire data is a single block!</p>
</li>
<li>
<p><strong>codewords are computed on a fly</strong>: As the block size for arithmetic coding is the entire data, the codebook size would have been massive ($|\mathcal{X}|^n$). The codeword is computed on <em>on the fly</em>.
No need to pre-compute the codebook beforehand!</p>
</li>
<li>
<p><strong>compression efficiency</strong>: Arithmetic coding is optimal in terms of compression. -&gt;  <em>theoretically</em> the performance can be shown to be:<br />
$$H(X) \leq \frac{\mathbb{E}[l(X_1^n)]}{n} \leq H(X) + \frac{2}{n}$$
i.e. <code>~2 bits</code> of overhead for the <em>entire</em> sequence</p>
</li>
</ol>
<p>Thus, we see that Arithmetic coding solves our problem of achieving average compression equal to $H(X)$, at the same time being practical.</p>
<h2 id="how-does-arithmetic-coding-work"><a class="header" href="#how-does-arithmetic-coding-work">How does Arithmetic coding work?</a></h2>
<h3 id="primer-the-number-line-in-binary"><a class="header" href="#primer-the-number-line-in-binary">Primer: the number line (in binary)</a></h3>
<p>Before we get to Arithmetic coding, lets take a brief detour towards revisiting the number line, as that will be helpful for the later discussions.</p>
<p>We are familiar with the unit interval <code>[0,1)</code> in decimal floating point. A decimal floating point number can also be represented in the binary alphabet.</p>
<p><img src="https://user-images.githubusercontent.com/1708665/195666574-59533bd7-b67c-4fe8-8a6f-3a04a29d9af5.png" alt="img" /></p>
<p>For example:</p>
<ol>
<li><code>0.5 = 2^{-1} = b0.1</code></li>
<li><code>0.75 = 2^{-1} + 2^{-2} = b0.11</code></li>
</ol>
<p>In fact this is similar to how computers represent floating point values. Here are some more examples below:</p>
<pre><code class="language-py"># floating point values in binary
from utils.bitarray_utils import float_to_bitarrays
_, binary_exp = float_to_bitarrays(0.3333333, 20)

0.3333.. = b0.010101...
0.6666.. = b0.101010...
</code></pre>
<p>As you can see, like the decimal case, the binary floating point representations need not be finite (or even repetitive). Getting familiar with thinking about floating point values in binary will be useful to us for Arithmetic coding discussion.</p>
<h3 id="arithmetic-encoding-theoretical"><a class="header" href="#arithmetic-encoding-theoretical">Arithmetic Encoding (theoretical)</a></h3>
<p>Okay, we are all set to discuss Arithmetic encoding. Lets consider a specific running example, as that will make it much easier:</p>
<p>We want to encode the sequence $x_1^n = BACB$ sampled from the distribution $P$.</p>
<pre><code class="language-py">P = ProbabilityDist({A: 0.3, B: 0.5, C: 0.2})
x_input = BACB
</code></pre>
<p>The Arithmetic encoding works in the following two steps:</p>
<ol>
<li><strong>STEP I</strong>: Find an <em>interval</em> (or a <em>range</em>) <code>[L,H)</code>, corresponding to the <em>entire sequence</em> $x_1^n$</li>
</ol>
<pre><code class="language-py">        x_input -&gt; |------------------[.....)------------|
                   0               low     high          1 
</code></pre>
<ol start="2">
<li><strong>STEP II</strong>: Communicate the <em>interval</em> <code>[L,H)</code> <em>efficiently</em>
(i.e. using less number of bits)</li>
</ol>
<pre><code class="language-py">         x_input -&gt; [L,H) -&gt; 011010
</code></pre>
<p>Before we discuss the steps in detail, let's discuss some desirable properties to gain further intuition. Consider the range <code>[0,1)</code> divided up into subintervals (known to both encoder and decoder). You want to convey a particular subinterval to a friend. For example you wanted to convey the interval <code>[0.3, 0.8)</code>. One way to do this is to just send over any point within this interval, for example <code>0.5</code>. As you can see, this can be done quite cheaply. Whereas if you wanted to convey the interval <code>[0.3, 0.30001)</code>, you would need to send over a much more precise value, for example <code>0.300005</code>. Thus the first piece of intuition is that:</p>
<ul>
<li><strong>Intuition 1: Bigger intervals need fewer bits to communicate</strong>.</li>
</ul>
<p>But recall from previous chatpers that a good coding scheme should assign shorter codewords to more probable sequences. Thus we would like to have the following property:</p>
<ul>
<li><strong>Intuition 2: More probable sequences should correspond to bigger intervals</strong>.</li>
</ul>
<p>The simplest logic is to have the size of the interval be proportional to the probability of the sequence, which is surprisingly what we will do!</p>
<p>With these combined intuitions, we get:</p>
<ul>
<li>More probable sequences =&gt; Bigger interval =&gt; Fewer bits to communicate</li>
</ul>
<p>With this in mind, let's study the steps and see how they satisfy this basic intuition.</p>
<h4 id="step-i-finding-the-interval-lh"><a class="header" href="#step-i-finding-the-interval-lh">STEP-I: finding the interval <code>[L,H)</code></a></h4>
<p>Lets see how the <strong>STEP I</strong> (finding the interval <code>[L,H)</code>) works.
We start with <code>[L,H) = [0,1)</code>, and the subdivide the interval as we see each symbol.</p>
<ol>
<li><code>x_input[0] = B</code></li>
</ol>
<p><img src="https://user-images.githubusercontent.com/1708665/195671855-1d048cea-ad31-4a20-8149-e9bfe68da2ac.jpg" alt="img" /></p>
<p>After we process the symbol <code>B</code>, our interval shrinks from <code>[0,1)</code> to <code>[0.3,0.8)</code>. Can you guess the shrinking rule?</p>
<p>The main idea is to find the cumulative probability values for the symbols, and then for symbol with index <code>i</code>, assign it the interval <code>[L,H) = [C(i), C(i+1))</code></p>
<pre><code class="language-py">alphabet = [A,B,C]
prob_array = [0.3, 0.5, 0.2]
cumul_array = [0.0, 0.3, 0.8, 1.0]
</code></pre>
<p><img src="https://user-images.githubusercontent.com/1708665/195671851-7fc549b8-d2a3-4fa9-b9c7-8bb311df5ed7.jpg" alt="img" /></p>
<p>One comment on the interval size is that the interval size is proportional to the probability of the symbol. (i.e. <code>C(i+1) - C(i) = P(i)</code>). We will see that a similar relationship holds as we encode more symbols.</p>
<ol start="2">
<li><code>x_input[1] = A</code>
Let's now see how the interval updates as we encode the second symbol. As you can see from the image below, we continue subdiving the interval <code>L,H</code>, based on the cumulative probability of the next symbol.</li>
</ol>
<p><img src="https://user-images.githubusercontent.com/1708665/195671847-0d550641-7da3-4127-8e92-dd4f251d7f3b.jpg" alt="img" /></p>
<p>Notice that the new interval <code>[L', H')</code> can be described as:</p>
<pre><code class="language-py">j -&gt; index of the next symbol
P(j), C(j) -&gt; prob and cumulative_prob of j

# update rule
L' = L + C(j)*[H-L]
H' = L' + P(j)*[H-L]
</code></pre>
<p>We can continue this way for the entire input.</p>
<p><img src="https://user-images.githubusercontent.com/1708665/195671842-0728b8a8-3718-4022-a594-b77b561f71aa.jpg" alt="img" /></p>
<pre><code class="language-py">prob = ProbabilityDist({A: 0.3, B: 0.5, C: 0.2})
x_input = BACB

# find interval corresp to BACB
ENCODE: B -&gt; [L,H) = [0.30000,0.80000)
ENCODE: A -&gt; [L,H) = [0.30000,0.45000)
ENCODE: C -&gt; [L,H) = [0.42000,0.45000)
ENCODE: B -&gt; [L,H) = [0.42900,0.44400)
</code></pre>
<p>Thus, the final interval is: <code>x_input -&gt; [0.429, 0.444)</code></p>
<p>For completeness, here is a pesudo-code of the <strong>STEP I</strong> or Arithmetic coding. Note that in the literature <em>interval</em> and <em>range</em> are terms used interchangeably to describe <code>[L,H)</code>.</p>
<pre><code class="language-py">class ArithmeticEncoder:
    ...

    def shrink_range(self, L, H, s):
        rng = H - L
        new_L = L + (rng * self.P.cumul[s])
        new_H = new_L + (rng * self.P.probs(s))
        return new_L, new_H

    def find_interval(self, x_input):
        L,H = 0.0, 1.0
        for s in x_input:
            L,H = self.shrink_range(L,H,s)
        return L,H

    def encode_block(self, x_input):
        # STEP1
        L,H = self.find_interval(x_input)

        # STEP-II
        ...
</code></pre>
<p><strong>Observation:</strong> Notice that each time we encode symbol $s$, we shrink the interval size by $P(s)$. For example: we started with interval <code>[L,H) = [0,1)</code> of size <code>1</code>. We continued shrinking this interval to <code>[0.3,0.8)</code> which has size <code>0.5 = P(B)</code>.</p>
<p><span style="color:purple;"> <strong>Quiz-1</strong>: What is the size of the interval (<code>H-L</code>) for the input $x_1^n$?</span></p>
<p><strong>Ans:</strong> It is easy to see that the interval size is equal to the probability of the input parsed until then. For example, when we see the first symbol <code>B</code>, the interval size is <code>0.8 - 0.3 = 0.5 = P(B)</code>, when we see the next symbol, the interval size is:
<code>0.45 - 0.3 = 0.15 = P(A)*P(B) = P(AB)</code>. This can be generalized to the entire sequence.</p>
<p>$$
\begin{align*}
(H - L) &amp;= p(x_1)<em>p(x_2)...p(x_n) \
&amp;= \prod_{i=1}^n p(x_i) \
&amp; = p(x_1^n)
\end{align</em>}$$</p>
<p>This is consistent with the intuition presented above!</p>
<h4 id="step-ii-communicating-the-interval-lh"><a class="header" href="#step-ii-communicating-the-interval-lh">STEP-II communicating the interval <code>[L,H)</code></a></h4>
<p>Until this point, we discussed the <strong>STEP I</strong> of the Arithmetic encoding, given the input <code>x_input</code>, find an interval corresponding to the input.</p>
<pre><code class="language-py">P = {A: 0.3, B: 0.5, C: 0.2}
x_input = BACB
L = [0.429, 0.444)
</code></pre>
<p>The <strong>STEP-II</strong> is logically quite simple. We want to communicate the interval $[L,H)$. We do this by communicating a value $Z \in [L,H)$.</p>
<p>For example: $Z = \frac{(L+H)}{2}$, i.e. the midpoint of the range/interval.
(in our example <code>Z = 0.4365</code>)</p>
<p><span style="color:purple;"><strong>Quiz-2:</strong></span> If the decoder knows:</p>
<ul>
<li><code>n=4</code></li>
<li><code>P = {A: 0.3, B: 0.5, C: 0.2}</code></li>
<li><code>Z = 0.4365</code></li>
</ul>
<p><span style="color:purple;">How can it decode the entire input sequence? $x_1^n$.</span></p>
<h3 id="arithmetic-decoding-theoretical"><a class="header" href="#arithmetic-decoding-theoretical">Arithmetic decoding (theoretical)</a></h3>
<p>Let's try to answer the quiz question above. Let's start by plotting out what all we know. We know the value of $Z=0.4365$, the midpoint of the final interval.
<img src="https://user-images.githubusercontent.com/1708665/195691372-900a32af-7f78-4e3b-8a83-a69e83cd7163.jpg" alt="h:550" /></p>
<p>How can we use $Z$ to decode the entire input <code>x_input</code>? Lets start by asking a simpler question: how can we decode the first symbol <code>x_input[0]</code> of the input?</p>
<ol>
<li><strong>Decoding <code>x_input[0]</code></strong></li>
</ol>
<p>The answer is quite simple:</p>
<pre><code class="language-py">alphabet = [A,B,C]
prob_array = [0.3, 0.5, 0.2]
cumul_array = [0.0, 0.3, 0.8, 1.0]
</code></pre>
<p>As the decoder knows the probability distribution, cumulative distribution etc, the decoder can form bins using the cumulative distribution values:</p>
<pre><code class="language-py"># cumulative bits:
bin_0 -&gt; [C(0), C(1)) = [0.0, 0.3) #A
bin_1 -&gt; [C(1), C(2)) = [0.3, 0.8) #B
bin_2 -&gt; [C(2), C(3)) = [0.8, 1.0) #C
</code></pre>
<p>As during the encoding the successive intervals (on encoding each symbol) are subsets of the original intervals, we can see decode the first symbol by checking which interval $Z$ lies in.</p>
<p><img src="https://user-images.githubusercontent.com/1708665/195691365-719c0536-a812-48f4-a836-d352fb837f0d.jpg" alt="h:550" /></p>
<p>In our case <code>Z = 0.4365</code> lies in the bin corresponding to symbol <code>B</code>, so the first input symbol is <code>B</code>.</p>
<ol start="2">
<li><strong>Decoding <code>x_input[1]</code></strong>
Having decoded the first symbol <code>B</code>, let's now see how we can decode the second symbol.</li>
</ol>
<p>The logic here is again similar, if you recall, during the encoding, we divided the current range further into sub-intervals, and then picked one of them based on the next symbol. For example in this case the sub=intervals are:</p>
<pre><code class="language-py"># sub-intervals of [0.3, 0.8)
bin_0 -&gt; = [0.3, 0.45) #BA
bin_1 -&gt; = [0.45, 0.7) #BB
bin_2 -&gt; = [0.7, 0.8) #BC
</code></pre>
<p>We can again find which bin does $Z$ belong do, and that will correspond to our next symbol. In our case, the next symbol is <code>A</code>.</p>
<p><img src="https://user-images.githubusercontent.com/1708665/195691359-5fbf1d74-09ed-43ca-a81a-0c2fc52f9d65.jpg" alt="h:550" /></p>
<p>We can continue this recursive process until we have decoded <code>n=4</code> symbols.</p>
<ol>
<li>
<p>Notice that the intervals are in exact sync during the encoding and decoding, and that is the key to the lossless-ness of the decoding algorithm.</p>
</li>
<li>
<p>Also note that we do need to mention what the number of encoded symbols is (<code>n=4</code> in our example), as the decoder can potentially decode infinite sequence using a single $Z$. So, we need to tell it when to stop the recursive process. In practice we can inform the decoder about when to stop in two common ways:</p>
<ol>
<li>Prepend the length <code>n</code> at the start of the bitstream using a fixed length encoding</li>
<li>Use a special <code>EOF</code> symbol in the alphabet, and inform the decoder to stop when it sees the <code>EOF</code> symbol. In this case, the <code>EOF</code> symbol should have a non-zero but typically small probability in the distribution.</li>
</ol>
</li>
</ol>
<pre><code class="language-py">Z = 0.4365
ENCODE: B -&gt; [L,H) = [0.30000,0.80000)
ENCODE: A -&gt; [L,H) = [0.30000,0.45000)
ENCODE: C -&gt; [L,H) = [0.42000,0.45000)
ENCODE: B -&gt; [L,H) = [0.42900,0.44400)
------------------------------
DECODE: B -&gt; [L,H) = [0.30000,0.80000)
DECODE: A -&gt; [L,H) = [0.30000,0.45000)
DECODE: C -&gt; [L,H) = [0.42000,0.45000)
DECODE: B -&gt; [L,H) = [0.42900,0.44400)
</code></pre>
<p>For completeness, the decoding algorithm given the $Z$ is described below.</p>
<pre><code class="language-py">class ArithmeticDecoder:
    ...
    def shrink_range(self, L, H, s):
        # same as the encoder
        ...
        return new_L, new_H

    def decode_symbol(self, L, H, Z):
        rng = H - L
        search_list = L + (self.P.cumul * rng)
        symbol_ind = np.searchsorted(search_list, Z)
        return self.P.alphabet[symbol_ind]

    def decode_block(self, Z, n):
        L,H = 0.0, 1.0
        for _ in range(n): #main decoding loop
            s = self.decode_symbol(L, H, Z)
            L,H = self.shrink_range(L,H,s)
</code></pre>
<h3 id="communicating-z"><a class="header" href="#communicating-z">Communicating $Z$</a></h3>
<p>To recap, the Arithmetic encoder we have does the following:</p>
<ol>
<li>
<p><strong>STEP-I</strong>: Find an <em>interval</em> (or a <em>range</em>) <code>[L,H)</code>
corresponding to the <em>entire sequence</em> $x_1^n$ (<code>[0.429, 0.444]</code>)</p>
</li>
<li>
<p><strong>STEP-II</strong>: Find the midpoint of the interval $[L,H)$, $Z = \frac{(L+H)}{2}$.  (<code>Z =0.4365</code>), and communicate $Z$ to the decoder.</p>
</li>
</ol>
<p>One simple way of communicating $Z$, is writing the binary expansion of $Z$ to the bitstream -&gt;
eg: <code>Z = 0.4365 = b0.01101111101...</code>
then the final <span style="color:red;"> <code>encoded_bitstream = 01101111101...</code> </span> and then just writing the binary expansion to a file.</p>
<p>However there is one problem:</p>
<p><span style="color:purple;"> <strong>Quiz-3:</strong> Although our method of writing the binary expansion of $Z$ to file is cute, it might not give us any compression as $Z$'s binary representation can be long, can also have infinite bits.
How can we fix this? </span></p>
<p>The solution to the problem is quite simple. Instead of communicating the entire binary expansion of $Z$, we truncate the expansion to $k$ bits and communicate this truncated binary string. Let's call the the floating point value corresponding to the truncated binary string as $\hat{Z}$.</p>
<p>Note that we have to be careful regarding how we choose $k$, if $k$ is too small the $\hat{Z}$ might be steer out of the interval <code>[L,H)</code>, which will be a problem for the decoding. Also, choosing $k$ too large will hamper our compression performance.</p>
<p><img src="https://user-images.githubusercontent.com/1708665/195694952-5a4772a0-7e17-4b67-a1c8-17526c9a1d66.jpg" alt="h:550" /></p>
<p>Thus, our full Arithmetic encoder can be described below:</p>
<div id="admonition-arithmetic-encoder" class="admonition admonish-note" role="note" aria-labelledby="admonition-arithmetic-encoder-title">
<div class="admonition-title">
<div id="admonition-arithmetic-encoder-title">
<p>Arithmetic Encoder</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/arithmetic_coding.html#admonition-arithmetic-encoder"></a>
</div>
<div>
<ol>
<li>
<p><strong>STEP-I</strong>: Find an <em>interval</em> (or a <em>range</em>) $[L,H)$
corresponding to the <em>entire sequence</em> $x_1^n$ (<code>[0.429, 0.444]</code>)</p>
</li>
<li>
<p><strong>STEP-II</strong>: Find the midpoint of the interval $[L,H)$, $Z = \frac{(L+H)}{2}$.  (<code>Z =0.4365</code>)</p>
</li>
<li>
<p><strong>STEP-III:</strong> Truncate $Z$ to $k$ bits ($\hat{Z}$)
e.g:</p>
</li>
</ol>
<pre><code>L,H = 0.429, 0.444
Z = 0.4365 = b0.01101111101...
Z_hat = b0.011011111 ~ 0.4296
</code></pre>
<p>Final Encoding = <span style="color:red;"> <code>encoded_bitstream = 011011111</code> </span></p>
</div>
</div>
<h3 id="determining-how-many-bits-to-truncate-z-to"><a class="header" href="#determining-how-many-bits-to-truncate-z-to">Determining how many bits to truncate $Z$ to</a></h3>
<p>Okay, we are almost done finalizing the Arithmetic encoder. The only final missing piece of the jigsaw puzzle is how do we determine $k$, the number of bits to truncate $Z$ to?</p>
<p>There are in fact two conditions we need to satisfy when choosing $k$:</p>
<ol>
<li>
<p><strong>Cond 1:</strong> Truncate $Z$ to $\hat{Z}$ with $k$ bits, so that $ \hat{Z} \in [L,H)$</p>
</li>
<li>
<p><strong>Cond 2:</strong> If $\hat{Z}$ has binary representation: <code>Z_hat = b0.011011111</code> for example, then we also need, any extension of it $Z_{ext} \in [L, H)$.</p>
</li>
</ol>
<p>For eg:</p>
<pre><code>Z_hat = b0.011011111
Z_ext = b0.01101111111011110101..
</code></pre>
<p>The <strong>Cond-1</strong> is somewhat obvious, we want the $\hat{Z}$ obtained on truncating $Z$ to $k$ bits, we still lie inside the interval <code>[L,H)</code>. Lets now see why the  <strong>Cond 2</strong> is imposed: Lets say Arithmetic coding leads to <code>Z_hat = b0.011011111</code>, and thus the final encoded bitarray is <code>011011111</code>. Now, in practice arithmetic coded bitstream is always going to be followed by some other bitstream, and so the decoder is going to see this stream like: <code>01101111111011110101..</code>. the second condition ensures that, even if the decoder reads in some bits not part of the arithmetic encoding, the resultant floating point value $Z_{ext}$ will still lie inside the interval <code>[L, H)</code>.</p>
<p>With some simple arithmetic, it can be shown that the two conditions together can be written as:</p>
<p>$$ [\hat{Z}, \hat{Z} + 2^{-k}) \in [L,H)$$</p>
<p>which gives us a simple bound on $k$:
$$ k \leq \left\lceil {log_2 \frac{1}{(H-L)}} \right \rceil + 1 $$</p>
<p><span style="color:purple;"> <strong>Quiz-4:</strong> Explain where the condition $ [\hat{Z}, \hat{Z} + 2^{-k}) \in [L,H)$ comes from. As a hint, think about a $k$-bit binary value like $b = 0.b_1b_2b_3\dots b_k$ - what can you say about a value $c=0.b_1b_2b_3\dots b_k c_1c_2\dots$? What is the maximum possible value of $c-b$? </span></p>
<p>The key observation here is that shorter the interval, $|H-L|$, the larger $k$ we need to use to truncate $Z$ (consistent with the basic intuition we presented at the very beginning!). Using the $k$ computation above, we finally have got our complete Arithmetic coder pseudocode:</p>
<pre><code class="language-py"># Arithmetic Encoding pseudo-code
class ArithmeticEncoder:
    def shrink_range(self, L, H, s):
        ...
    def find_interval(self, x_input):
        L,H = 0.0, 1.0
        for s in x_input:
            L,H = self.shrink_range(L,H,s)
        return L,H

    def encode_block(self, x_input):
        # STEP-1 find interval
        L,H = self.find_interval(x_input)

        # STEP-II,III communicate interval
        Z = (L+H)/2 
        num_bits = ceil(log2(1/(H-L))) + 1
        _, code = float_to_bitarray(Z, num_bits)
        return code

# Arithmetic decoding-pseudocode
class ArithmeticDecoder:
    ...
    def shrink_range(self, L, H, s):
        ...

    def decode_symbol(self, L, H, Z):
        ...

    def decode_block(self, code, n):
        Z = bitarray_to_float(code)

        # start decoding
        L,H = 0.0, 1.0
        for _ in range(n): #main decoding loop
            s = self.decode_symbol(L, H, Z)
            L,H = self.shrink_range(L,H,s)

        # add code to remove additional bits read
</code></pre>
<p>One point to note in the decoding is that, as the decoder might have read in more bits that what the encoder wrote, after decoding all the symbols, the decoder needs to backtrack a bit (otherwise the program processing the next stream is going to falter!). Since the decoder will know the decoded sequence and hence the value of $k$, it can easily backtrack the extra bits read.</p>
<h3 id="arithmetic-coding-compression-performance"><a class="header" href="#arithmetic-coding-compression-performance">Arithmetic coding compression performance:</a></h3>
<p>Now that we have discussed the full Arithmetic encoder/decoder, lets try to understand the compression performance of Arithmetic coder.</p>
<p>Let us start by summarizing what we know:</p>
<ul>
<li>Size of interval $H-L = p(x_1^n)$</li>
<li>$k \leq \log_2 {\frac{1}{H-L}} + 2$</li>
</ul>
<p>Based on these two properties, it is quite straightforward to see that the codelength for encoding an entire sequence $x_1^n$ using Arithmetic coding is:</p>
<p>$$ codelen = k \leq \log_2 \frac{1}{p(x_1^n)} + 2 $$</p>
<p>As the optimal number of bits to encode a sequence $x_1^n$ using a distribution $p(.)$ are $\log_2 \frac{1}{p(x_1^n)}$, we see that arithmetic coding is in fact within <code>2</code> bits of the optimal on the <em>ENTIRE</em> sequence! Thus, we can summarize by saying that:</p>
<div id="admonition-arithmetic-coder-compression-performance" class="admonition admonish-note" role="note" aria-labelledby="admonition-arithmetic-coder-compression-performance-title">
<div class="admonition-title">
<div id="admonition-arithmetic-coder-compression-performance-title">
<p>Arithmetic coder compression performance</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/arithmetic_coding.html#admonition-arithmetic-coder-compression-performance"></a>
</div>
<div>
<p>Given any model/distribution $p(x)$ for a sequence $x_1^n$, arithmetic coding achieves codelength of
$$ l(x_1^n) = k \leq \log_2 \frac{1}{p(x_1^n)} + 2 $$</p>
<p>Also, the average codelength achieved by Arithmetic coding is within $2/n$ bits of the entropy $H(X)$.</p>
<p>$$ H(X) \leq \frac{\mathbb{E}[l(X_1^n)]}{n} \leq H(X) + \frac{2}{n}$$</p>
</div>
</div>
<p>It is quite incredible that Arithmetic coding is basically optimal for <em>any</em> given distribution $P$. This in a way shifts the complexity in designing compression algorithms from designing codes to finding a distribution/model $P$ which matches the data! This property of Arithmetic coding is also called the <strong>model, entropy coding separation</strong>. This means we can just focus on building good probability models, which could even be adaptive (i.e., the distribution changes as we see more data), and then use Arithmetic coding as a black-box to convert the probabilities to bits efficiently. We will revisit this idea in the upcoming chapters.</p>
<h2 id="arithmetic-coding-in-practice"><a class="header" href="#arithmetic-coding-in-practice">Arithmetic coding in practice</a></h2>
<p>Now that we have concluded our discussion of the (theoretical) Arithmetic coding algorithm, lets think about implementing the pseudo-code in practice and what challenges we might face.</p>
<p>There is one clear practical issue which you might be able to spot from the hint below!</p>
<p><span style="color:purple;"> <strong>Quiz-5</strong>: What are the practical issues with our Arithmetic encoding/decoding?</span></p>
<p>Hint -&gt;</p>
<pre><code class="language-py">prob = ProbabilityDist({A: 0.3, B: 0.5, C: 0.2})
x_input = BACBBCCBA

# find interval corresp to BACB
ENCODE: B -&gt; [L,H) = [0.30000,0.80000)
ENCODE: A -&gt; [L,H) = [0.30000,0.45000)
ENCODE: C -&gt; [L,H) = [0.42000,0.45000)
ENCODE: B -&gt; [L,H) = [0.42900,0.44400)
ENCODE: C -&gt; [L,H) = [0.44100,0.44400)
ENCODE: C -&gt; [L,H) = [0.44340,0.44400)
ENCODE: B -&gt; [L,H) = [0.44358,0.44388)
ENCODE: A -&gt; [L,H) = [0.44358,0.44367)
</code></pre>
<p>The key idea is that every time we encode a symbol <code>s</code>, we are shrinking the interval <code>[L,H)</code>, by the probability of the symbol <code>P(s)</code>. Thus, we see that very quickly the interval becomes quite small. For example in the hint above, on encoding <code>8</code> symbols, we are already at a place where out interval is <code>[0.44358,0.44367)</code>. We quickly see that due to finite bit arithmetic, we are going to run out of bits to represent this interval. So we need to find a way to avoid the interval <code>[L,H)</code> from getting too small:</p>
<p><span style="color:purple;"> <strong>Quiz-6</strong>: What can we do to avoid the interval <code>[L,H)</code> from getting too small?</span></p>
<p>Hint -&gt;</p>
<pre><code class="language-py">L = 0.429 = b0.0110110...
H = 0.444 = b0.01110001...
</code></pre>
<p>The core idea is quite simple. Notice that in the example above both <code>L</code>, <code>H</code> start with <code>011</code>. Then, $Z$, or any value lying inside the interval also will start with <code>011</code>! Thus, we can pre-emptively output <code>011</code>, and then <em>rescale</em> the intervals <code>L,H</code>. For example:</p>
<pre><code class="language-py">L = 0.429 = b0.0110110...
H = 0.444 = b0.01110001...

Rescaled: L=0.8580, H=0.8880, bitarray='0'
Rescaled: L=0.7160, H=0.7760, bitarray='01'
Rescaled: L=0.4320, H=0.5520, bitarray='011'
</code></pre>
<p>Notice that after rescaling our intervals are <code>L=0.4320, H=0.5520</code> are much larger that what we started with: <code>0.429, 0.444</code>. This can be understood from the fact that, flushing out a <code>0</code> is equivalent to setting <code>L,H</code> as: <code>L,H = 2*L,2*H</code>; while flushing out a <code>1</code> is equivalend to <code>L,H = (L - 0.5)*2, (H - 0.5)*2</code>. i.e. we are expanding out either the left half (<code>[0,0.5)</code>) of the number line or the right half (<code>[0.5, 1)</code>) of the number line by <code>2x</code>. This is also illustrated in the figure below (credit: <a href="https://www.youtube.com/watch?v=t8_198HHSfI">YouTube</a>):</p>
<p><img src="lossless_iid/images/arith_rescaling.png" alt="img" /></p>
<p>Thus, our rescaling operation can be summarized below.</p>
<pre><code class="language-py"># Arithmetic encoding with rescaling of the range
class ArithmeticEncoder:
    def shrink_range(self, L, H, s):
        ...
    def rescale_range(self, L, H):
        bitarray = ""
        while (L &gt;= 0.5) or (H &lt; 0.5):
            if (L &lt; 0.5) and (H &lt; 0.5):
                bitarray+= "0"
                L,H = L*2, H*2
            elif ((L &gt;= 0.5) and (H &gt;= 0.5)):
                bitarray += "1"
                L,H = (L - 0.5)*2, (H - 0.5)*2    
        return L, H, bitarray
    def find_interval(self, x_input):
        L,H, bitarray = 0.0, 1.0, Bitarray("")
        for s in x_input:
            L,H = self.shrink_range(L,H,s)
            L,H, bits = self.rescale_range(L,H)
            bitarray += bits
        return L,H, bitarray
</code></pre>
<p>This largely mitigates the problem. However, it is possible that <code>L,H</code> are extremely close, but don't start with the same bits. For example:</p>
<pre><code class="language-py">L = 0.499 = 0.111..b, 
H = 0.501 = 0.100..b
</code></pre>
<p>In this case, we cannot premptively flush-out bits and rescale the range <code>[L,H)</code>. This <em>mid-range rescaling</em> issues is handled in different ways, by either:</p>
<ol>
<li>
<p>If <code>L=0.499, H = 0.501</code>, we can setting<code>H = 0.4999999...</code> for example, and then apply the usual re-scaling. In this case, we are losing a bit of compression, as we are reducing the interval size when it is not needed. The <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/compressors/range_coder.py">range coder variant</a> implemented in the SCL uses this simple approach.</p>
</li>
<li>
<p>An optimal way to handle the <em>mid-range rescaling</em> is to expand the middle interval <code>[0.25, 0.75)</code> by 2x <code>L,H &lt;- 2L - 0.5, 2H - 0.5</code>. This is a bit non-trivial, but is well explained in <a href="https://www.youtube.com/playlist?list=PLE125425EC837021F">this YouTube series</a> of lectures on Arithmetic coding. Note that this optimal rescaling is also implemented as part of the <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/compressors/arithmetic_coding.py">Arithmetic coder in the SCL</a></p>
</li>
<li>
<p>In practice, it is more efficient to flush out bytes or even words instead of bits. i.e if <code>L,H</code> start with the same 8 bits, only then those 8 bits are flushed out. These variants of Arithmetic coding are traditional called <em>range coding</em>. One variant of the <em>range coder</em> is impplemented in the SCL and can be <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/compressors/range_coder.py">accessed here</a>.</p>
</li>
</ol>
<p>We encourage the reader to study the implementations of both the Arithmetic coder and the Range coder in the SCL to get a better understanding of these practical issues. The implementations are written in a simple and clear manner, and are well commented to enable easy understanding. The code also links to various resources which can be used to understand the implementation better.</p>
<h3 id="arithmetic-coding-computational-performance"><a class="header" href="#arithmetic-coding-computational-performance">Arithmetic coding computational performance</a></h3>
<p>Let's start with some basic points:</p>
<ol>
<li>The encoding/decoding complexity is linear in the number of symbols.</li>
<li>We do not need to store any codebook, the codewords are computed on the fly. Hence the memory requirements are quite low. In fact observe we never even need to compute the probability table for the entire sequence $p(x_1^n)$, we can just work with the symbol probabilities $p(x_i)$.</li>
</ol>
<p>While this all sounds great, there are a few caveats to consider:</p>
<ol>
<li>The encoding/decoding as presented above involves floating point operations, which can be slow. In practice, integer arithmetic is used to speed up the operations but it is still significantly slower than the table lookup based implementations of Huffman coding.</li>
<li>For larger alphabet size, there is dependency on the alphabet size $|\mathcal{X}|$ especially during decoding since we need to identify the interval. In practice, this can be mitigated by using data structures like Fenwick trees or binary search, which can reduce the complexity to $O(\log |\mathcal{X}|)$ per symbol.</li>
<li>Another complication happens with large alphabet sizes and finite precision arithmetic, which can lead to underflows. This can be mitigated by using techniques like scaling or renormalization (discussed above) which further complicate the implementation. This is the reason many uses of arithmetic coding in practice (e.g., in image and video codecs) are limited to binary alphabets, such as <a href="https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding">CABAC</a> in H.264 and H.265 video coding standards. Of course, the actual alphabet needs to be mapped to binary symbols before encoding which can done in various ways.</li>
<li>As discussed above, in many cases byte-level flushing (range coder) is used instead of bit-level flushing, which can lead to some loss in compression performance while being faster.</li>
</ol>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>To summarize, Arithmetic encoding has had a profound impact on the compression world due to its optimality, and made it possible to work on much more complex models such as adaptive i.i.d models, and even non-iid k-markov models (as we will see in future lectures).</p>
<p>The only drawback of Arithmetic coding is its speed (especially when compared with Huffman coding)</p>
<div class="table-wrapper"><table><thead><tr><th>Codec</th><th>Encode speed</th><th>Decode speed</th><th>compression</th></tr></thead><tbody>
<tr><td>Huffman coding</td><td>252 Mb/s</td><td>300 Mb/s</td><td>1.66</td></tr>
<tr><td>Arithmetic coding</td><td>120 Mb/s</td><td>69 Mb/s</td><td>1.24</td></tr>
</tbody></table>
</div>
<p>NOTE -&gt; Speed numbers from: <a href="http://cbloomrants.blogspot.com/2014/02/02-01-14-understanding-ans-3.html">Charles Bloom's blog</a></p>
<p>In the next lecture we see how we can achieve compression performance similar to Arithmetic coding, but speeds closer to that of Huffman coding.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="asymmetric-numeral-systems"><a class="header" href="#asymmetric-numeral-systems">Asymmetric Numeral Systems</a></h1>
<p><strong>NOTE:</strong> trying out a <em>new</em> format (actually  really old, from the days of Galileo, see <em>Systema cosmicum</em>).
This is a dialogue between three friends <em>Brahma, Vishnu and Mahesh</em> who are trying to understand asymmetric numeral systems</p>
<hr />
<h2 id="the-pre-intro"><a class="header" href="#the-pre-intro">The Pre-intro</a></h2>
<p><strong>Vishnu:</strong> I have been trying to read the paper <a href="https://arxiv.org/abs/0902.0271">Asymmetric Numeral Systems</a> (ANS) by Jarek Duda, but have not made any progress. Have either of you made any progress?</p>
<p><strong>Mahesh:</strong> Yeah, it is driving me nuts, only if someone could tell me the TL;DR.</p>
<p><strong>Brahma:</strong> Well, I did read it. In fact I have been trying to understand it on-and-off since the past 4-5 years. Seems like I finally understood it. I have still not understood the original paper, but here are some blogs by some fantastic researchers in the area of compression. I found their blogs to be the most useful.
Here are links to their write-ups on ANS</p>
<ol>
<li>Fabian Giesen: https://fgiesen.wordpress.com/2014/02/02/rans-notes/</li>
<li>Charles Bloom: https://cbloomrants.blogspot.com/2014/02/02-18-14-understanding-ans-conclusion.html</li>
<li>Yann Collet: http://fastcompression.blogspot.com/2013/12/finite-state-entropy-new-breed-of.html</li>
</ol>
<p>I would recommend starting with Fabian's blog. He talks specifically about <code>rANS</code>, which is one of the many variants of compressors introduced by Jarek. I found the specificity simpler to start with.
All the best! I have to now go and <em>meditate</em>.</p>
<p><strong>Mahesh:</strong> Well hold on! I will look at these links in detail, but can you maybe give a short intro?</p>
<p><strong>Brahma:</strong> Hmm, alright! The ANS paper introduces a set of new compression algorithms, namely the <code>rANS</code> (range-ANS) and <code>tANS</code> (table-ANS) compression algorithm. To be more precise <code>rANS</code> and <code>tANS</code> are actually a family of compression methods which provide a good tradeoff between compression and speed.</p>
<p>I found it is easier to start with <code>rANS</code> and then develop <code>tANS</code> as a consequence of optimizations under certain parameter regimes. This is in-fact the path which Fabian and Charles choose in their blogs, Yann chooses the reverse path of directly tackling <code>tANS</code>, which is also interesting in itself!</p>
<p><strong>Vishnu:</strong> Sounds good! But even before we go there, can you explain why the name of the paper: <em>Asymmetric Numeral Systems</em>?</p>
<hr />
<h2 id="the-introduction"><a class="header" href="#the-introduction">The Introduction</a></h2>
<p><strong>Brahma:</strong> Okay, lets start the story from the beginning:
Lets say the problem is that we are given a sequence of digits in <code>[0,9]</code>, and we want to represent them with a single number <code>x</code>.
Lets say we are given the sequence <code>3,2,4,1,5</code>. Then how can we form a single number representing them?</p>
<p><strong>Vishnu:</strong> Umm, isn't this trivial? If we are given digits <code>3,2,4,1,5</code>, I can just form the number <code>x = 32415</code>
<strong>Brahma:</strong> Thats right! this is what we do everyday in our lives. Lets however try to understand what we just did in a different way. What we are actually doing is the following: We are starting with the <em>state</em> <code>x = 0</code>, and then updating it as we see each symbol:</p>
<pre><code class="language-python">x = 0  # &lt;-- initial state
x = x*10 + 3 # x = 3
x = x*10 + 2 # x = 32
...
x = x*10 + 5 # x = 32415
</code></pre>
<p>Thus, during the <em>encoding</em> we are start with state <code>x=0</code>, and then it increases as <code>x = 0, 3, 32, 324, 3214, 32145</code> as we <em>encode</em> the symbols.
Can either of you tell me how would you decode back the sequence if I give you the final state <code>32145</code>?</p>
<p><strong>Mahesh:</strong> Sure, then we can decode the sequence as:</p>
<pre><code class="language-python">symbols = []
x = 32145 # &lt;- final state

# repeat 5 times
s, x = x%10, x//10  # s,x = 5, 3214
symbols.append(s) 

</code></pre>
<p>at the end, you would get <code>symbols = [5,4,1,2,3]</code>.. which would need to be reversed to get your input sequence.</p>
<p><strong>Brahma:</strong> Precisely! Notice that during the decoding, we are just reversing the operations, i.e: the <code>encode_step</code> and <code>decode_step</code> are exact inverses of each other. Thus, the state sequence we see is <code>32145, 3214, 321, ..., 3</code> is the same as during encoding, but in the reverse order.</p>
<pre><code class="language-python">def encode_step(x,s):
   x = x*10 + s
   return x

def decode_step(x)
   s = x%10 # &lt;- decode symbol
   x = x//10 # &lt;- retrieve the previous state
   return (s,x)
</code></pre>
<p>We have until now spoken about representing an input sequence of digits using a single state <code>x</code>. But how can we turn this into a compressor? We need a way to represent the final state <code>x</code> using bits.</p>
<p><strong>Mahesh:</strong> That is again quite simple: we can just represent <code>x</code> in binary using <code>ceil(log_2[x]))</code> bits. For example in this case:
<code>32145 = 111110110010001</code>. Our "encoder/decoder" can be written as:</p>
<pre><code class="language-python">
## Encoding
def encode_step(x,s):
   x = x*10 + s
   return x

def encode(symbols):
    x = 0 # initial state
    for s in symbols:
        x = encode_step(x,s)
    
    return to_binary(x)    

## Decoding

def decode_step(x):
   s = x%10 # &lt;- decode symbol
   x = x//10 # &lt;- retrieve the previous state
   return (s,x)

def decode(bits, num_symbols):
   x = to_uint(bits) # convert bits to final state

   # main decoding loop
   symbols = []
   for _ in range(num_symbols):
       s, x = decode_step(x)
       symbols.append(s)
   return reverse(symbols) # need to reverse to get original sequence
</code></pre>
<p><strong>Brahma:</strong> Thats right Mahesh! Can you also comment on how well your "compressor" is compressing data?</p>
<p><strong>Mahesh:</strong>: Let me see: if we look at the <code>encode_step</code>, we are increasing it by a factor of 10 <code>x_next &lt;- 10*x</code>. There is the small added symbol <code>s</code>, but it is quite small, compared to the state <code>x</code> after some time. As we use <code>~ log_2(x)</code> bits to represent the state. Increasing it 10 times, we are using <code>~log_2(10)</code> bits per symbol.</p>
<p><strong>Vishnu:</strong> I see! So, we are effectively using around <code>log_2(10)</code> bits per symbol. I think that is <em>optimal</em> if all our digits <code>[0,9]</code> are equiprobable and have probability <code>1/10</code> each. But, what if the symbols are not equi-probable? I think in that case this "compressor" might do a bad job.</p>
<p><strong>Brahma:</strong> Good observation Vishnu! That is precisely correct. One thumb-rule we know from Information theory is that if a symbol occurs more frequently, it should use less bits than the symbol which occurs less frequently. In fact, we know that on average a symbol <code>s</code> with probability <code>prob[s]</code> should <em>optimally</em> use <code>log_2 (1/prob[s])</code> bits.
Let me even put this thumb-rule in a box to emphasize:</p>
<pre><code>avg. optimal bits for a symbol s = log_2 (1/prob[s])
</code></pre>
<p>Notice that our compressor is using <code>log_2(10) = log_2(1/10)</code>, which validates the fact that our compressor is optimal if our symbols have probability <code>1/10</code>, but otherwise it is not.</p>
<p><strong>Vishnu:</strong> Thats a very useful thumb rule. I think our compressor will be "fixed" if the <code>encode_step</code> increases the state <code>x</code> as:
<code>x_next ~ x* (1/prob[s])</code>. In that case, effectively our symbol s is using <code>log_2(1/prob[s])</code> bits.</p>
<p><strong>Brahma:</strong> Very good Vishnu! You have got the correct intuition here. In fact this is the core idea behind <code>rANS</code> encoding. Ohh, to answer Mahesh's question: what we were doing with our "compressor" was our usual <code>Symmetric Numeral System</code>, where the probability of all symbols is the same. We will now be discussing the <code>Asymmetric Numeral Systems</code> which will handle the case where the digits don't have the same probability.</p>
<p><strong>Mahesh:</strong> Ahh, cool! I now understand the title of the paper :)</p>
<hr />
<h2 id="theoretical-rans"><a class="header" href="#theoretical-rans">Theoretical rANS</a></h2>
<p><strong>Brahma:</strong> Okay, before we go into the details of <code>rANS</code>, lets define some notation: as <code>x</code> is an integer, lets represent <code>prob[s]</code> using rational numbers:</p>
<pre><code class="language-python">prob[s] = freq[s]/M
</code></pre>
<p>where <code>M = freq[0] + freq[1] + ... + freq[9]</code>, i.e. the sum of frequencies of all symbols (also called as Range Factor). Here <code>freq[s]</code> is also called the "Frequency" of symbol <code>s</code>. Lets also define <code>cumul[s]</code>, the cumulative frequency. i.e:</p>
<pre><code class="language-python">cumul[0] = 0
cumul[1] = freq[0]
cumul[2] = freq[0] + freq[1]
...
cumul[9] = freq[0] + freq[1] + ... + freq[8]
</code></pre>
<p>For example:  if we limit ourselves to only the digits <code>{0,1,2}</code>:</p>
<pre><code class="language-python">freq -&gt; [3,3,2], M = 8
cumul -&gt; [0,3,6]
prob -&gt; [3/8, 3/8, 2/8]
</code></pre>
<p>Is this setting clear?</p>
<p><strong>Vishnu:</strong> Yes, yes, basically you are just representing everything with integers. Lets go ahead!
<strong>Brahma:</strong> Haha allright! Okay, so the <code>rANS</code> encoding follows the exact same pattern we saw earlier. i.e.
We still start from state <code>x=0</code> and then update the state every time we see a new symbol. The main difference is that our <code>encode_step</code> doesn't just do <code>x*10 + s</code>; we do something slightly more intricate here:</p>
<pre><code class="language-python">
## Encoding
def rans_base_encode_step(x,s):
   # add details here
   return x

def encode(symbols):
    x = 0 # initial state
    for s in symbols:
        x = rans_base_encode_step(x,s)
    
    return to_binary(x)    
</code></pre>
<p>Any guesses on what the <code>rans_base_encode_step(x,s)</code> should be?</p>
<p><strong>Mahesh:</strong> I think based on our discussion earlier, for <em>optimal</em> compression we want the state <code>x</code> to roughly increase as <code>x_next &lt;- x/prob[s]</code>. Thus, in the notation you defined, I would say there has to be a term something similar to:</p>
<pre><code class="language-python">x_next ~ (x//freq[s])*M
</code></pre>
<p><strong>Brahma:</strong> Very good Mahesh! In fact the true <code>rans_base_encode_step</code> is quite close to what you suggested, and is as follows:</p>
<pre><code class="language-python">
## Encoding
def rans_base_encode_step(x,s):
   x_next = (x//freq[s])*M + cumul[s] + x%freq[s]
   return x_next
</code></pre>
<p>Lets take a concrete example: If we consider our example distribution:</p>
<pre><code class="language-python">symbols = [0,1,2]
freq -&gt; [3,3,2], M = 8
cumul -&gt; [0,3,6]
</code></pre>
<p>and the sample input as <code>[1,0,2,1]</code>, then the encoding proceeds as follows:</p>
<pre><code class="language-python">step 0: x = 0
step 1: s = 1 -&gt; x = (0//3)*8 + 3 + (0%3) = 3
step 2: s = 0 -&gt; x = (3//3)*8 + 0 + (3%3) = 8
step 3: s = 2 -&gt; x = (8//2)*8 + 6 + (8%2) = 38
step 4: s = 1 -&gt; x = (38//3)*8 + 3 + (38%3) = 101
</code></pre>
<p>Any observations?</p>
<p><strong>Vishnu:</strong> One thing I noticed is that, if we choose the uniform probability scenario where <code>prob[s] = 1/10, for s in [0,9]</code> i.e. <code>freq[s] = 1, M = 10</code>, then we reduce to the symmetric numeral system formula!</p>
<pre><code class="language-python"># rANS state update
x_next = (x//freq[s])*M + cumul[s] +  x%freq[s]

# substitute freq[s] = 1, M = 10
x_next = x*10 + s
</code></pre>
<p><strong>Brahma:</strong> Indeed! We reduce to the symmetric numeral system, if all the probabilities are equal; which is good news!</p>
<p>Okay, now that we have understood the encoding, lets try to understand the decoding. The decoding also proceeds in a similar fashion: we start with the final state <code>x</code> and in each step try to decode the symbol and the previous state.</p>
<pre><code class="language-python">def rans_base_decode_step(x):
   # perform decoding
   
   return (s,x_prev)

def decode(bits, num_symbols):
   x = to_uint(bits) # convert bits to final state

   # main decoding loop
   symbols = []
   for _ in range(num_symbols):
       s, x = rans_base_decode_step(x)
       symbols.append(s)
   return reverse(symbols) # need to reverse to get original sequence
</code></pre>
<p>Any thoughts on how the decoding might proceed?</p>
<p><strong>Mahesh:</strong> Let me give it a shot: Essentially we just want to find the inverse of the <code>rans_base_encode_step</code> right?
So let me start by writing that down:</p>
<pre><code class="language-python">x = (x_prev//freq[s])*M + cumul[s] + (x_prev % freq[s])
</code></pre>
<p>Given <code>x</code> we want to retrieve the symbol <code>s</code> and <code>x_prev</code>... but, I am not sure how to make progress (:-|)</p>
<p><strong>Brahma:</strong> Good start Mahesh. Let me give you a hint, we can write the encoding step as follows:</p>
<pre><code class="language-python">block_id = x_prev//freq[s]
slot = cumul[s] + (x_prev % freq[s])
x = block_id*M + slot
</code></pre>
<p>One thing to notice is that <code>0 &lt;= slot &lt; M</code>, as:</p>
<pre><code class="language-python">slot = cumul[s] + (x_prev % freq[s])
     &lt; cumul[s] + freq[s]
     &lt;= M
</code></pre>
<p>Does this help?</p>
<p><strong>Mahesh:</strong> Aha! yes, I think as <code>0 &lt;= slot &lt; M</code>, one can think of it as the remainder on dividing <code>x</code> with <code>M</code>. So, I can retrieve the <code>block_id</code> and <code>slot</code> as:</p>
<pre><code class="language-python"># encode step 
x = block_id*M + slot

# decode step:
block_id = (x//M)
slot = (x%M)
</code></pre>
<p><strong>Brahma:</strong> Good job Mahesh! That is a good first step. Vishnu, can you go ahead and retrieve <code>s, x_prev</code> from <code>slot, block_id</code>?</p>
<p><strong>Vishnu:</strong> We know that <code>0 &lt;= slot &lt; M</code>. But we know a bit more about the slot:</p>
<pre><code class="language-python">slot = cumul[s] + (x_prev % freq[s])
     &lt; cumul[s] +  freq[s]
     = cumul[s+1]

i.e.
cumul[s] &lt;= slot &lt; cumul[s+1]
</code></pre>
<p>So, once we obtain the <code>slot</code>, we can just do a linear/binary search over the list <code>[cumul[0], cumul[1], ... cumul[9], M]</code> to decode <code>s</code>.
Once we have decoded the symbol <code>s</code>, we know <code>freq[s], cumul[s]</code>. So, we can decode <code>x_prev</code> as:</p>
<pre><code class="language-python">x_prev = block_id*freq[s] + slot - cumul[s]
</code></pre>
<p>Thus, the decoding can be described as follows:</p>
<pre><code class="language-python">def rans_base_decode_step(x):
   # Step I: find block_id, slot
   block_id = x//M
   slot = x%M
   
   # Step II: Find symbol s
   s = find_bin(cumul_array, slot) 
   
   # Step III: retrieve x_prev
   x_prev = block_id*freq[s] + slot - cumul[s]

   return (s,x_prev)
</code></pre>
<p><strong>Brahma:</strong> Absolutely correct Vishnu! That is very precise. So that we all are on the same page, lets again go through our example:</p>
<pre><code class="language-python">symbols = [0,1,2]
freq -&gt; [3,3,2], M = 8
cumul -&gt; [0,3,6]
</code></pre>
<p>and the sample input as <code>[1,0,2,1]</code> -&gt; final state <code>x = 101</code></p>
<p>Lets see how the decoding goes:</p>
<pre><code class="language-python">x = 101

# Step I
block_id = 101//8 = 12
slot = 101%8 = 5

# Step II
cumul_array = [0, 3, 6, 8]
as 3 &lt;= slot &lt; 6, the decoded symbol s=1

# Step III
x_prev = 12*3 + 5 - 3
x_prev = 38
</code></pre>
<p>Voila! we have thus decoded <code>s=1, x_prev=38</code>. We can now continue to decode other symbols. To summarize, here are our full encode decode functions:</p>
<pre><code class="language-python">
####### Encoding ###########

def rans_base_encode_step(x,s):
   x_next = (x//freq[s])*M + cumul[s] + x%freq[s]
   return x_next

def encode(symbols):
    x = 0 # initial state
    for s in symbols:
        x = rans_base_encode_step(x,s)
    
    return to_binary(x) 

####### Decoding ###########
def rans_base_decode_step(x):
   # Step I: find block_id, slot
   block_id = x//M
   slot = x%M
   
   # Step II: Find symbol s
   s = find_bin(cumul_array, slot) 
   
   # Step III: retrieve x_prev
   x_prev = block_id*freq[s] + slot - cumul[s]

   return (s,x_prev)

def decode(bits, num_symbols):
   x = to_uint(bits) # convert bits to final state

   # main decoding loop
   symbols = []
   for _ in range(num_symbols):
       s, x = rans_base_decode_step(x)
       symbols.append(s)
   return reverse(symbols) # need to reverse to get original sequence

</code></pre>
<p><strong>Mahesh:</strong> Great! I think, I now understand the <code>rANS</code> base encoding and decoding step quite well. Essentially, we are increasing the state by approximately <code>x_next ~ x/prob[s]</code> as needed to be <em>optimal</em>, but we are doing it in a smart way, so that we can follow our steps backwards and retrieve the encoded symbols and state.</p>
<p>One think I noticed is that, as the state is growing by a factor of <code>~ 1/prob[s]</code> at every step.. it is growing exponentially, and practically would surpass the 64-bit (or 32-bit) limit on computers. So, I think that is the reason you are calling this <em>theoretical rANS</em>. I think there is still some work left to make this algorithm practical.</p>
<p><strong>Brahma:</strong> Yes, that is correct Mahesh; the state <code>x</code> does increase exponentially. For example, in our example it increases as <code>0, 3, 8, 38, 101, ...</code>. Thus we need to somehow limit the state from growing beyond a certain limit. That is however a story for some other day. Let me leave you both here to re-think the base <code>rANS</code> steps!</p>
<p><strong>Mahesh, Vishnu:</strong> Thank you Brahma! See you next time!</p>
<hr />
<h2 id="streaming-rans"><a class="header" href="#streaming-rans">Streaming rANS</a></h2>
<p><strong>Brahma:</strong> Okay, hope you guys had time to revise on the base <code>rANS</code> step. Lets now talk about how to make it practical. As you recall, every time we call the the <code>x = rans_base_encode_step(x,s)</code>, we are typically increasing the state <code>x</code> exponentially. Thus, we need some way to limit this growth.</p>
<p>The methodology employed by Jarek Duda in his paper is ingenious, but at the same time quite simple. The goal is to limit the state <code>x</code> to always lie in a pre-defined range <code>[L,H]</code>.</p>
<p><strong>Mahesh</strong>: Even before we go into the details, I have a fundamental question: as a final step of <code>rANS</code>, we represent state <code>x</code> using <code>ceil(log_2(x))</code> bits. However, if <code>x</code> is always going to lie in the range <code>[L,H]</code>, we can at maximum transmit <code>ceil(log_2(H))</code> bits of information, which doesn't seem much.</p>
<p><strong>Brahma:</strong> Sharp observation Mahesh. In the theoretical <code>rANS</code> discussion, the only bits we write out are at the end, and are equal to <code>ceil(log_2(x))</code>. It is clear that this is not going to work if <code>x</code> is not growing. So, in the <code>streaming rANS</code> case, along with writing out the final state at the end, we stream out a few bits after we encode every symbol. Thus, the new encoding structure looks like this:</p>
<pre><code class="language-python">def encode_step(x,s):
    # update state + output a few bits
    return x_next, out_bits

def encode(symbols):
    x = L # initial state
    encoded_bitarray = BitArray()
    for s in symbols:
        x, out_bits = encode_step(x,s)

        # note that after the encode step, x lies in the interval [L,H]
        assert x in Interval[L,H]
        
        # add out_bits to output
        encoded_bitarray.prepend(out_bits)
    
    # add the final state at the beginning
    num_state_bits = ceil(log2(H))
    encoded_bitarray.prepend(to_binary(x, num_state_bits))
    return encoded_bitarray
</code></pre>
<p>As you can see, the new <code>encode_step</code> doesn't just update the state <code>x</code>, but it also outputs a few bits.</p>
<p><strong>Vishnu:</strong> Cool! That makes sense. I have a couple of observations:</p>
<ol>
<li>
<p>I see that the initial state in <code>streaming rANS</code> is <code>x = L</code> and not <code>x = 0</code>. I also see you have an <code> assert x in Interval[L,H]</code> after the <code>encode_step</code>. So, it seems that the input state to <code>encode_step</code> and its output always lie in <code>Interval[L,H]</code>.</p>
</li>
<li>
<p>Another interesting I noticed is that you prepend <code>out_bits</code> to the final <code>encoded_bitarray</code>. I am guessing this is because the <code>rANS</code> decoding decodes symbols in the reverse order right?</p>
</li>
</ol>
<p><strong>Brahma:</strong> That is right Vishnu! similar to <code>rANS</code> decoding, the <code>streaming rANS</code> decoder also proceeds in the reverse direction, and hence needs to access the bits in the reversed order. (NOTE: prepending could be a bit tricky and slow, due to repeated memory allocations.. but lets worry about the speed later!)</p>
<p>Lets now take a look at the decoder structure in case of <code>streaming rANS</code>:</p>
<pre><code class="language-python">def decode_step(x, enc_bitarray):
    # decode s, retrieve prev state
    # also return the number of bits read from enc
    return s, x_prev, num_bits_step

def decode(encoded_bitarray, num_symbols):
    # initialize counter of bits read from the encoded_bitarray
    num_bits_read = 0

    # read the final state 
    num_state_bits = ceil(log2(H))
    x = to_uint(encoded_bitarray[:num_state_bits])
    num_bits_read += num_state_bits

    # main decoding loop
    symbols = []
    for _ in range(num_symbols):
        # decoding step
        s, x, num_bits_step = decode_step(x, encoded_bitarray[num_bits_read:])
        symbols.append(s)

        # update num_bits_read counter
        num_bits_read += num_bits_step
    return reverse(symbols) # need to reverse to get original sequence
</code></pre>
<p>The decoder starts by retrieving the final state by reading <code>num_state_bits</code> bits from the <code>encoded_bitarray</code>. Once it has retrieved the final state <code>x</code>, we call the <code>decode_step</code> function in a loop until we have decoded all the symbols, as in case of <code>rANS</code>. The difference now is that the <code>decode_step</code> function also consumes a few bits (<code>num_bits_step</code>). As you can imagine, these <code>num_bits_step</code> correspond to the length of the <code>out_bits</code> from the <code>encode_step</code>.</p>
<p><strong>Mahesh:</strong> I see! This makes sense. So, now the missing component in our algorithms are the <code>encode_step</code> and <code>decode_step</code> functions, which need to be perfect inverses of each other, but at the same time need to ensure that the state <code>x</code> lies in the <code>Interval[L,H]</code>.</p>
<p><strong>Brahma:</strong> Thats right Mahesh! Lets think about the <code>encode_step</code> first. Let me start by writing its structure, so that we understand how we are modifying the basic <code>rans_base_encode_step</code>:</p>
<pre><code class="language-python">
def shrink_state(x,s):
    # TODO
    # reduce the state x -&gt; x_shrunk
   return x_shrunk, out_bits

def encode_step(x,s):
   # shrink state x before calling base encode
   x_shrunk, out_bits = shrink_state(x, s)

   # perform the base encoding step
   x_next = rans_base_encode_step(x_shrunk,s)
   
   return x_next, out_bits
</code></pre>
<p>Any thoughts Vishnu?</p>
<p><strong>Vishnu:</strong> Hmm, lets look at the <code>encode_step</code> function. We still have the <code>rans_base_encode_step(x_shrunk,s)</code> call as before; but before this base encode step, we seem to modify the state <code>x -&gt; x_shrunk</code> using the <code>shrink_state(x,s)</code> function call, which as the name suggests is probably making the state smaller.</p>
<p>I think I understand intuitively what is happening: We know that the input state <code>x</code> to <code>encode_step</code> lies in the range <code>[L,H]</code>. We also know that typically <code>rans_base_encode_step</code> increases the state; Thus, to ensure <code>x_next</code> lies in <code>[L,H]</code>, we shrink <code>x -&gt; x_shrunk</code> beforehand.</p>
<p><strong>Brahma:</strong> Thats right! The main function of <code>shrink_state</code> is indeed to reduce the state <code>x -&gt; x_shrunk</code>, so that <em>after</em> encoding symbol <code>s</code> using the <code>rans_base_encode_step</code>, the output state, <code>x_next</code> lies in <code>[L,H]</code>.</p>
<p>One more thing to note is that, as we are performing lossless compression, the "information" content should be retained at every step. If we look at the <code>encode_step</code> function, and the two sub-steps:</p>
<ul>
<li>We know that <code>rans_base_encode_step(x_shrunk,s)</code> encodes the information of the state <code>s</code> and state <code>x_shrunk</code> into a single state <code>x_next</code>, and that no information is lost.</li>
<li>Thus, the first step <code>shrink_state</code> also needs to also ensure that state <code>x</code> is retrievable from <code>x_shrunk</code> and <code>out_bits</code>.</li>
</ul>
<p>As we will see, the <code>shrink_state</code> function basically tries to do this, in the simplest possible way:</p>
<pre><code class="language-python">def shrink_state(x,s):
   # initialize the output bitarray
   out_bits = BitArray()

   # shrink state until we are sure the encoded state will lie in the correct interval
   while rans_base_encode_step(x,s) not in Interval[L,H]:
       out_bits.prepend(x%2)
       x = x//2
   x_shrunk = x
   return x_shrunk, out_bits
</code></pre>
<p>The <code>shrink_state</code> function shrinks the state by streaming out the lower bits of the state <code>x</code>. For example, if state <code>x = 22 = 10110b</code>, then then <code>shrink_state</code> could output:</p>
<pre><code class="language-python"># input state 
x = 21 = 10110b

# output options: 
x_shrunk = 10110b = 22, out_bits = ""
x_shrunk = 1011b = 11, out_bits = "0"
x_shrunk = 101b = 5, out_bits = "10"
x_shrunk = 10b = 2, out_bits = "110"
...
</code></pre>
<p>The question is now: <em><strong>how many bits to stream out from <code>x</code>?</strong></em></p>
<p>the answer is quite simple; as we want to ensure that
<code>rans_base_encode_step(x,s) lies in Interval[L,H]</code>, we keep streaming out bits until this condition is satisfied! It is quite simple in the hindsight.</p>
<p>Any thoughts Vishnu?</p>
<p><strong>Vishnu:</strong> Yes, I think it is quite simple and intuitive. I however think that we have to choose the Interval <code>[L,H]</code> carefully, as otherwise it might be possible that there is no state <code>x_shrunk</code> for which <code>rans_base_encode_step(x_shrunk,s)</code> is in that interval.</p>
<p><strong>Brahma:</strong> That is right Vishnu, the range will have to be chosen carefully and precisely. We will also see that we can simplify the comparison step <code>while rans_base_encode_step(x,s) not in Interval[L,H]</code>, so that we don't need to call the <code>base_encode_step</code> multiple times. But, before that, lets see if we can understand how the <code>decode_step</code> might work. Any thoughts on the same Mahesh?</p>
<p><strong>Mahesh:</strong> Let me start by thinking about the overall <code>decode_step</code> structure first. I think as the <code>decode_step</code> has to be an inverse of <code>encode_step</code>, we need to have <code>expand_state</code> step which is an inverse of the <code>shrink_state</code> step. Also, as the decoding proceeds in a reverse order: so, we first perform the <code>rans_base_decode_step</code> and then call the <code>expand_state</code> function.</p>
<pre><code class="language-python">def expand_state(x_shrunk, enc_bitarray):
    # TODO
    # read in bits to expand x_shrunk -&gt; x
   return x, num_bits_step

def decode_step(x, enc_bitarray):
    # decode s, retrieve prev state
    s, x_shrunk = rans_base_decode_step(x)

    # expand back x_shrunk to lie in Interval[L,H]
    x_prev, num_bits_step = expand_state(x_shrunk, enc_bitarray)
    return s, x_prev, num_bits_step
</code></pre>
<p><strong>Brahma:</strong> Thats good Mahesh! Another way to intuitively think about <code>expand_state</code> is that: typically  <code>rans_base_decode_step</code> reduces the state value, so we need to expand it back using <code>expand_state</code> function so that it lies back in the range <code>[L,H]</code>. Any thoughts on the specifics of <code>expand_state</code> function?</p>
<p><strong>Mahesh:</strong> Sure, let me think! We basically want to do the inverse of what we do in the <code>shrink_state</code> function.</p>
<p>We know that the input state <code>x</code> to the <code>shrink_state(x,s)</code> lies in <code>x in Interval[L,H]</code>, so in the <code>expand_state(x_shrunk, enc_bitarray)</code>, we can read in the bits into the state <code>x_shrunk</code>, until it lies in <code>Interval[L,H]</code>. Thus, the <code>expand_state</code> function might look like:</p>
<pre><code class="language-python">def expand_state(x_shrunk, enc_bitarray):
    # init
    num_bits_step = 0

    # read in bits to expand x_shrunk -&gt; x
    x = x_shrunk
    while x not in Interval[L,H]:
        x = x*2 + enc_bitarray[num_bits_step]
        num_bits_step += 1
    return x, num_bits_step
</code></pre>
<p><strong>Brahma:</strong> Good job Mahesh, your <code>expand_state</code> function is very precise! However, as Vishnu said in case of <code>encode_step</code>, one thing to note even in case of the decoding is that, we need to choose the <code>L</code> and <code>H</code> values carefully.</p>
<p>For example: we need to make sure that while we expand the state as: <code>x = x*2 + enc_bitarray[num_bits_step]</code>, there is a unique <code>x</code> which lies in the <code>Interval[L,H]</code>, as otherwise there is no guarantee that the the state <code>x</code> we obtain from <code>expand_state</code> function is the same as the one which the <code>shrink_state</code> function used as input.</p>
<hr />
<h3 id="choosing-the-intervallh"><a class="header" href="#choosing-the-intervallh">Choosing the <code>Interval[L,H]</code></a></h3>
<p><strong>Brahma:</strong> Okay, lets try to answer the final piece of the streaming rANS puzzle: the choice of the values <code>L, H</code>. Instead of trying to derive all possible values for <code>L,H</code>, I am going to straightaway present the solution given by Duda in his paper, and we will try to understand why that works:</p>
<p>The acceptable values for <code>L, H</code> for streaming rANS are:</p>
<pre><code class="language-python">L = M*t
H = 2*M*t - 1
</code></pre>
<p>where <code>M = freq[0] + freq[1] + ... + freq[9]</code> (sum of frequencies), and <code>t</code> is an arbitrary unsigned integer. Any thoughts?</p>
<p><strong>Vishnu:</strong> Okay, lets see: Based on the previous discussions, the two conditions we need the <code>Interval[L,H]</code> to satisfy are:</p>
<p><em>1: encode-step-constraint</em>: The <code>shrink_state</code> function can be understood in terms of the shrinking operation:</p>
<pre><code class="language-python">start -&gt; x_0 in Interval[L,H]
shrink_op -&gt; x_i = x_{i-1}//2
</code></pre>
<p>Then, <code>Interval[L,H]</code> should be large enough so that for all <code>x_0 in Interval[L,H]</code>, there is at least one <code>i</code> such that <code>rans_base_encode_step(x_i,s) in Interval[L,H]</code> for all symbols <code>s</code></p>
<p><em>2: decode-step-constraint</em>: During the <code>expand_state</code> we repeatedly perform the state expansion operation:</p>
<pre><code class="language-python">start -&gt; x_0
expand_op -&gt; x_i = 2*x_{i-1} + bit
</code></pre>
<p>The <code>Interval[L,H]</code> should be small enough so that there is at max a single <code>i</code> for which <code>x_i in Interval[L,H]</code>.</p>
<p>I think it is easy to see that the second constraint, i.e. <em>decode-step-constraint</em> will be satisfied if we choose <code>L, H </code>as: <code>L, H &lt;= 2L - 1</code>. The idea is simple.. Lets assume that <code>x_{i-1} in interval[L,H</code>, i.e <code>x_{i-1} &gt;= L</code>. Then <code>x_i = 2*x_{i-1} + bit &gt;= 2*L &gt; H</code>. Thus, if <code>x_{i-1}</code> is in the <code>Interval[L,H]</code>, then <code>x_{i}</code> cannot. Thus if <code>H &lt;= 2L - 1</code>, then the <em>decode-step-constraint</em> is satisfied.</p>
<p>I am not sure how to ensure the first constraint is satisfied.</p>
<p><strong>Brahma:</strong> Good job Vishnu with the second constraint. As for the first constraint, the <em>encode-step-constraint</em>: lets start by writing down <code>rans_base_encode_step</code>:</p>
<pre><code class="language-python">def rans_base_encode_step(x,s):
   x_next = (x//freq[s])*M + cumul[s] + x%freq[s]
   return x_next
</code></pre>
<p>Now, for any given symbol <code>s</code>, it is clear that <code>rans_base_encode_step(x, s)</code> is monotonically increasing in <code>x</code>. If not, I will leave it as an exercise for you to verify!</p>
<p>Now, lets see what range of values of <code>x_shrunk</code> map to in the <code>Interval[L,H]</code>, where <code>L = M*t, H = 2*M*t - 1</code>:</p>
<pre><code class="language-python">M*t = rans_base_encode_step(freq[s]*t,s)
2*M*t = rans_base_encode_step(2*freq[s]*t,s)
</code></pre>
<p>Thus, as <code>rans_base_encode_step(x_shrunk, s)</code> is monotically increasing, if <code>freq[s]*t &lt;= x_shrunk &lt;= (2*freq[s] - 1)</code>, only then
<code>rans_base_encode_step(x_shrunk, s) in Interval[L,H]</code>.</p>
<p>So, to satisfy the  <em>encode-step-constraint</em>, we need to argue that, as we remove bits from <code>x_shrunk</code>, it will always end up in the <code>Interval[freq[s]*t, 2*freq[s]*t - 1]</code>. This is again true for a similar argument as to what we discussed for <em>decode-step-constraint</em>. (I will again keep it an exercise for you to prove this!).</p>
<p>So, we can conclude that:</p>
<blockquote>
<p>For streaming rANS: L=M<em>t, H=2</em>M*t - 1</p>
</blockquote>
<p><strong>Vishnu:</strong> Wow, that was a bit involved, but I now understand how the <code>L, H</code> values were chosen by Duda! Essentially everything revolves around the interval of type <code>[l, 2*l-1]</code>. These intervals play well with the shrinking op (i.e <code>x -&gt; x//2</code>) or the expand op (<code>x -&gt; x*2 + bit</code>), and guarantee uniqueness.</p>
<p><strong>Mahesh:</strong> Yeah, that was quite innovative of Duda! Another thing which I noticed from Brahma's arguments is that:</p>
<p>we can basically replace the check <code>while rans_base_encode_step(x,s) not in Interval[L,H]</code> with <code>while x not in Interval[freq[s]*t, 2*freq[s]*t - 1]</code>. That way, we don't need to call the <code>rans_base_encode_step</code> again and again, and this should increase our encoding efficiency.</p>
<p><strong>Brahma:</strong> Good observation Mahesh! And, in fact that was the last thing we needed to complete our streaming rANS. To summarise here are our full encoder and decoder:</p>
<pre><code class="language-python">########################### Global variables ############################
freq[s]  # frequencies of symbols 
cumul[s] # cumulative frequencies of symbols
M = freq[0] + freq[1] + ... + freq[9] # sum of all frequencies

## streaming params
t = 1 # can be any uint
L = M*t
H = 2*M*t - 1


########################### Streaming rANS Encoder ###########################
def rans_base_encode_step(x,s):
   x_next = (x//freq[s])*M + cumul[s] + x%freq[s]
   return x_next

def shrink_state(x,s):
   # initialize the output bitarray
   out_bits = BitArray()

   # shrink state until we are sure the encoded state will lie in the correct interval
   while rans_base_encode_step(x,s) not in Interval[freq[s]*t,2*freq[s]*t - 1]:
       out_bits.prepend(x%2)
       x = x//2
   x_shrunk = x
   return x_shrunk, out_bits

def encode_step(x,s):
   # shrink state x before calling base encode
   x_shrunk, out_bits = shrink_state(x, s)

   # perform the base encoding step
   x_next = rans_base_encode_step(x_shrunk,s)
   
   return x_next, out_bits

def encode(symbols):
    x = L # initial state
    encoded_bitarray = BitArray()
    for s in symbols:
        x, out_bits = encode_step(x,s)

        # note that after the encode step, x lies in the interval [L,H]
        assert x in Interval[L,H]
        
        # add out_bits to output
        encoded_bitarray.prepend(out_bits)
    
    # add the final state at the beginning
    num_state_bits = ceil(log2(H))
    encoded_bitarray.prepend(to_binary(x, num_state_bits))
    return encoded_bitarray

########################### Streaming rANS Decoder ###########################
def rans_base_decode_step(x):
   # Step I: find block_id, slot
   block_id = x//M
   slot = x%M
   
   # Step II: Find symbol s
   s = find_bin(cumul_array, slot) 
   
   # Step III: retrieve x_prev
   x_prev = block_id*freq[s] + slot - cumul[s]

   return (s,x_prev)

def expand_state(x_shrunk, enc_bitarray):
    # init
    num_bits_step = 0

    # read in bits to expand x_shrunk -&gt; x
    x = x_shrunk
    while x not in Interval[L,H]:
        x = x*2 + enc_bitarray[num_bits_step]
        num_bits_step += 1
    return x, num_bits_step

def decode_step(x, enc_bitarray):
    # decode s, retrieve prev state
    s, x_shrunk = rans_base_decode_step(x)

    # expand back x_shrunk to lie in Interval[L,H]
    x_prev, num_bits_step = expand_state(x_shrunk, enc_bitarray)
    return s, x_prev, num_bits_step

def decode(encoded_bitarray, num_symbols):
    # initialize counter of bits read from the encoded_bitarray
    num_bits_read = 0

    # read the final state 
    num_state_bits = ceil(log2(H))
    x = to_uint(encoded_bitarray[:num_state_bits])
    num_bits_read += num_state_bits

    # main decoding loop
    symbols = []
    for _ in range(num_symbols):
        # decoding step
        s, x, num_bits_step = decode_step(x, encoded_bitarray[:num_bits_read])
        symbols.append(s)

        # update num_bits_read counter
        num_bits_read += num_bits_step
    return reverse(symbols) # need to reverse to get original sequence
</code></pre>
<p>Whoosh! That concludes our discussion of making rANS practical. You should now be able to take this pseudocode and implement a working rANS compressor. There are some optimizations + parameter choices we can perform on streaming rANS, to make it even better. However, I will leave that discussion for some other time. Now I have to go back to meditating! The Asymmetric Numeral Systems story is not over yet...!</p>
<p><strong>Vishnu, Mahesh:</strong> Thanks Brahma! That was a great discussion. Lets take a few days to ruminate over this.. we will be back to finish the story!</p>
<hr />
<h2 id="rans-in-practice"><a class="header" href="#rans-in-practice">rANS in practice</a></h2>
<p><strong>Brahma:</strong> Allright! Now that we have understood the <code>Streaming rANS</code> algorithm, lets try to understand the speed of the algorithm, and see what all modifications we can do to make the encoding/decoding faster.</p>
<p>Lets start with the parameter choices. One of the most fundamental parameters is <code>M = sum of freqs</code>. As a reminder, given the input probability distribution, <code>prob[s]</code>, we chose the values <code>freq[s]</code> and <code>M</code> so that:</p>
<pre><code>prob[s] ~ freq[s]/M
</code></pre>
<p>Any thoughts on how the choice of <code>M</code> impacts the <code>rANS</code> algorithm?</p>
<p><strong>Vishnu:</strong> I think as for the compression performance is concerned, we probably want <code>M</code> as high as possible so that the approximation <code>prob[s] ~ freq[s]/M</code> is good. As for speed: lets' take a re-look at <code>rans_base_encode_step</code> and <code>rans_base_decode_step</code>:</p>
<pre><code class="language-python">def rans_base_encode_step(x,s):
   x_next = (x//freq[s])*M + cumul[s] + x%freq[s]
   return x_next

def rans_base_decode_step(x):
   # Step I: find block_id, slot
   block_id = x//M
   slot = x%M
   
   # Step II: Find symbol s
   s = find_bin(cumul_array, slot) 
   
   # Step III: retrieve x_prev
   x_prev = block_id*freq[s] + slot - cumul[s]

   return (s,x_prev)

</code></pre>
<p>If we look at the <code>rans_base_encode_step</code>, we have a multiplication with <code>M</code> (<code> x_next = (x//freq[s])*M + ...</code>). Choosing <code>M</code> to be a power of <code>2</code> will make this multiplication a bit-shift and slightly speed up the encoding.</p>
<p>Also, if we look at the <code>rans_base_decode_step</code>, we have operations such as</p>
<pre><code class="language-python"># Step I: find block_id, slot
block_id = x//M
slot = x%M
</code></pre>
<p>I think both of these operations should be faster if <code>M = 2^r</code> for some <code>r</code>.</p>
<p><strong>Brahma:</strong> Thats a good observation Vishnu. Indeed in practice, <code>M</code> is chosen to be of the form <code>M=2^r</code>, typically <code>M = 2^{16}</code> or higher for sufficient precision.</p>
<blockquote>
<p><strong>M = 2^r</strong> i.e. M is typically chosen to be a power of 2</p>
</blockquote>
<p><strong>Vishnu:</strong> I however noticed that even after the <code>M=2^r</code> change, the <code>rans_base_encode_step</code> is still probably slow because of the division by <code>freq[s]</code> (<code>x_next = (x//freq[s])*M + ...</code>).</p>
<p>Also, <code>rans_base_decode_step</code> seems to be dominated by the binary search over the <code>cumul[s]</code> array:</p>
<pre><code class="language-python"># Step II: Find symbol s
s = find_bin(cumul_array, slot) 
</code></pre>
<p>Any idea if we can speed this up?</p>
<p><strong>Brahma:</strong> Good observations Vishnu! Indeed we need to address these bottlenecks. As far as the <code>rans_base_encode_step</code> is concerned the division by <code>freq[s]</code> can be sped up a bit by pre-processing reciprocals <code>1/freq[s]</code> and multiplying by them. Check out the <a href="https://fgiesen.wordpress.com/2014/02/18/rans-with-static-probability-distributions/">nice blog</a> by Fabian Giesen on the same:  on the same.</p>
<p>As far as the <code>find_bin</code> step is concerned, unfortunately there is no simple way to make it fast, without changing the <code>rANS</code> algorithm itself! This is again mentioned in the <a href="https://fgiesen.wordpress.com/2014/02/18/rans-with-static-probability-distributions/">blog</a> link by Fabian, in a modification which he refers to as the <em>alias method</em>.</p>
<p>I will leave it as an exercise for you to look through the details of the <em>alias method</em>. The end result is however that instead of taking <code>log(alphabet size)</code> comparisons for the binary search, we now only need a single comparison during the base decoding. This however leads to making the base encoding a bit slower, essentially shifting the compute from encoding to decoding!</p>
<p><strong>Vishnu:</strong> Wow! definitely sounds interesting.</p>
<p><strong>Mahesh:</strong> Thanks Brahma! It is nice to see tricks to speed up the <code>rans_base_encoding/decoding steps</code>. I however wonder if we have been giving too much emphasis on speeding the base steps. The encoding for example also includes the <code>shrink_state</code> state, which we haven't thought about. Maybe we should look into how to speed those up?</p>
<p><strong>Brahma:</strong> That is right Mahesh, we can do a few things to speed up <code>shrink_state</code> and <code>expand_state</code> functions. Let me start by writing them down again:</p>
<pre><code class="language-python">
# [L,H] -&gt;  range in which the state lies
L = M*t
H = 2*L - 1


def shrink_state(x,s):
   # initialize the output bitarray
   out_bits = BitArray()

   # shrink state until we are sure the encoded state will lie in the correct interval
   while rans_base_encode_step(x,s) not in Interval[freq[s]*t,2*freq[s]*t - 1]:
       out_bits.prepend(x%2)
       x = x//2
   x_shrunk = x
   return x_shrunk, out_bits


def expand_state(x_shrunk, enc_bitarray):
    # init
    num_bits_step = 0

    # read in bits to expand x_shrunk -&gt; x
    x = x_shrunk
    while x not in Interval[L,H]:
        x = x*2 + enc_bitarray[num_bits_step]
    return x, num_bits_step

</code></pre>
<p>If you notice, in both  <code>shrink_state</code> and <code>expand_state</code>, we are streaming out/in 1 bit at a time. Due to the memory layout on modern computer architectures where the memoery unit is of size 32-bits/64-bits, this might be slower than say accessing a byte at a time.</p>
<p>Any thoughts on how we can modify the algorithm to read/write more than one bit at a time?</p>
<p><strong>Mahesh:</strong> I think i got this! If you recall, our choice of range <code>H=2*L - 1</code>, was chosen mainly because we are reading in/out one bit at a time. If we choose, <code>H = 256*L - 1</code> for example, I think we should be able to write <code>8</code> bits at a time, and still guarantee everything works out fine.</p>
<p>The new <code>shrink_state</code> and <code>expand_state</code> would then be:</p>
<pre><code class="language-python">
# [L,H] -&gt;  range in which the state lies
b = 8 # for example
b_factor = (1 &lt;&lt; NUM_OUT_BITS) # 2^NUM_OUT_BITS
L = M*t
H = b_factor*L - 1 


def shrink_state(x,s):
   # initialize the output bitarray
   out_bits = BitArray()

   # shrink state until we are sure the encoded state will lie in the correct interval
    x_shrunk = x

   while rans_base_encode_step(x,s) not in Interval[freq[s]*t,b_factor*freq[s]*t - 1]:
       out_bits.prepend(x%b_factor)
       x = x//b_factor
   x_shrunk = x
   return x_shrunk, out_bits


def expand_state(x_shrunk, enc_bitarray):
    # init
    num_bits_step = 0

    # read in bits to expand x_shrunk -&gt; x
    x = x_shrunk
    while x not in Interval[L,H]:
        x = x*b_factor + to_uint(enc_bitarray[num_bits_step:num_bits_step+b])
        num_bits_step += b
    return x, num_bits_step

</code></pre>
<p>I think that should satisfy the constraints we had laid out for the <code>strink_state</code> and <code>expand_state</code> functions, but allow us to stream out <code>b</code> bits at a time.</p>
<p><strong>Brahma:</strong> Very good Mahesh! The code indeed gets a bit complicated, but this does lead to a significant speed-up! In practice, <code>b</code> is typically chosen as <code>b=8, 16</code> (byte, word), or even <code>b=32</code> if the architecture supports it.</p>
<p>Although we have been mainly talking in terms of psuedo-code, we can look at the encode/decode times, thanks to <a href="https://github.com/rygorous/ryg_rans">implementations</a> provided by Fabian Giesen.</p>
<pre><code class="language-python"># reading/writing one byte at a time
rANS encode:
9277880 clocks, 12.1 clocks/symbol (299.6MiB/s)
9276952 clocks, 12.1 clocks/symbol (299.7MiB/s)
...
rANS: 435113 bytes
14754012 clocks, 19.2 clocks/symbol (188.4MiB/s)
14723258 clocks, 19.2 clocks/symbol (188.8MiB/s)
...
decode ok!


# reading/writing 32 bits at a time:
rANS encode:
7726256 clocks, 10.1 clocks/symbol (359.8MiB/s)
7261792 clocks, 9.4 clocks/symbol (382.8MiB/s)
...
rANS: 435116 bytes
12159778 clocks, 15.8 clocks/symbol (228.6MiB/s)
12186790 clocks, 15.9 clocks/symbol (228.1MiB/s)
...
decode ok!

</code></pre>
<p>As you can see the speed up from going from <code>b=8</code> to <code>b=32</code> is quite significant! Note that getting such fast encoders/decoders is no mean feat. You should definitely take a look at the <a href="https://github.com/rygorous/ryg_rans">source code</a>, to see how he implements this!</p>
<p><strong>Vishnu:</strong> Wow, those numbers are quite incredible! I glanced through the github page, and noticed that these are some interlaced/SIMD rANS encoder/decoders there. What are these?</p>
<p><strong>Brahma:</strong> Ahh! One thing you might note is that even though we have been able to significantly speed up the <code>rANS</code> encoders/decoders, they are still inherently sequential. This is a pity, considering the modern computer architectures have bumped up their compute through parallelization features such as: instruction level parallelism (ILP), multiple cores, SIMD cores (single instruction multiple data) and even GPUs. In a way, inspite of having SIMD cores, GPUs etc. our encoder is only currently using a core to run the encoder/decoder.</p>
<p>Any idea how we can parallelize our encoders/decoders?</p>
<p><strong>Vishnu:</strong> Yes, I think we could cut the data into multiple chunks and encode/decode each chunk independently in parallel.</p>
<p><strong>Brahma:</strong> Thats right Vishnu. Typically there are some threading overheads to doing things in this manner. One can however be a bit more smarter about it, and avoid any significant overheads. Fabian in his <a href="https://arxiv.org/abs/1402.3392">paper</a>,  explains how to achieve good performance with this parallelization, without incurring significant overheads. I myself have not read it yet, so would be great if either of you could do that for me!</p>
<p>What however struct me even more is that we can interleave two encoders/decoders together and eveen using a single core, thanks to instruction level parallelism, one can achieve some speed up. This is explained well in the blog and <a href="https://fgiesen.wordpress.com/2015/12/21/rans-in-practice/">blog</a>. We can also run the <code>interleaved rANS</code> variant, and compare the numbers for ourselves:</p>
<pre><code class="language-python">
# rANS decoding 
8421148 clocks, 11.0 clocks/symbol (330.1MiB/s)
8673588 clocks, 11.3 clocks/symbol (320.5MiB/s)
...

# interleaved rANS decoding
5912562 clocks, 7.7 clocks/symbol (470.2MB/s)
5775046 clocks, 7.5 clocks/symbol (481.4MB/s)
....

# interleaved SIMD rANS decoding:
3898854 clocks, 5.1 clocks/symbol (713.0MB/s)
3722360 clocks, 4.8 clocks/symbol (746.8MB/s)
...

</code></pre>
<p><strong>Mahesh:</strong> Thanks Brahma for all the useful references! It is great to see the incredible speedups after system optimizations.</p>
<p>I am wondering if it is possible to take an entire different approach to speeding up the <code>rANS</code> encoding/decoding algorithms, which is <em>caching</em>. Considering our state is always limited to <code>[L,H]</code>, in a way one can think of the encoding for example as a mapping from: <code>[L,H]</code> and the <code>alphabet set</code> to <code>[L,H]</code>. Thus, we just need to fill in these tables once and then it is just a matter of looking at the table for  Would that work out?</p>
<pre><code class="language-python"># rANS encode_step function
encode_step(x, s) -&gt; x_next, bits

## cached encoding:
# pre-process and cache the output of encode_step into a lookup table
encode_lookup_table[x, s] -&gt; x_next, bits
</code></pre>
<p><strong>Brahma:</strong> Good observation Mahesh! That is yet another way to speed up <code>rANS</code>, and is in fact a good segue to the <code>tANS</code> i.e. the <code>table ANS</code> variant. I will however leave the <code>tANS</code> discussion for some other day!</p>
<hr />
<h2 id="table-ans-tans"><a class="header" href="#table-ans-tans">Table ANS (tANS)</a></h2>
<p><strong>Brahma:</strong> Even before we get started on <em>caching</em> our rANS encoder/decoders, lets take a minute to understand what is caching and what are the caveats which come with it.</p>
<p>Well, the idea of caching is quite simple: if there is some operation which you are doing repeatedly, you might as well just do it once as a part of pre-processing, (or even better during compile time) and then store this result in a lookup table. Now, when we need to perform this computation again, we can just look at our lookup table and retrieve the result. No need to repeat the computation!</p>
<p>This surely must be faster than doing the computation? Well, to answer this question, we need to ask another question:</p>
<blockquote>
<p>How much time it takes to retrieve a value stored in a lookup table?</p>
</blockquote>
<p>Unfortunately, the answer to this question is not as straightforward, due to the complex memory structure (for good reasons!) we have in modern computer architectures. But the thumb rule is that, if the lookup table is small, it is going to be fast, if it is large then it will probably not be worth it.</p>
<p>To be more specific: if the table is small (typically smaller than a few MBs) then it resides in the L1/L2 or L3 cache (also known as LLC -&gt; Last Level cache). The L1/L2/L3 get progressively slower, but take about <code> &lt; 10 cycles</code> which is quite fast. But, if your lookup table size is larger, then the lookup table resides on the <code>RAM</code> (i.e. the main memory), which is much slower with reading times <code>~100 cycles</code>.</p>
<p>Here is a screenshot on the memory layout from the excellent <a href="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/">MIT OCW lecture series</a></p>
<img width="1090" alt="image" src="https://user-images.githubusercontent.com/1708665/167315248-0c5bcb46-0c01-4498-99d6-41c7644c38a2.png">
<p>So, the thumb-rule we need to keep in mind while designing <em>caching</em> based methods is to keep the tables small. The larger the table, the more likely it is that we are going to get <em>cache-misses</em> (i.e. accessing data from the RAM), which is quite slow.</p>
<p>This thumb-rule should suffice for our discussion, but if you are interested in reading more on this, feel free to look at the following references:</p>
<ol>
<li>https://arstechnica.com/gadgets/2002/07/caching/</li>
<li>https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/</li>
</ol>
<p><strong>Brahma:</strong> Okay, now that we have a memory model as a reference, lets think about how we can go about <em>caching</em> rANS.  For simplicity lets assume the following:</p>
<pre><code class="language-python">M = 2^r
L = M
H = 2M - 1
A -&gt; alphabet size
</code></pre>
<p>i.e. we are choosing <code>t=1</code> and <code>M</code> to be a power of <code>2</code>. Lets try to think of what the decoding will look like first. Mahesh any thoughts?</p>
<p><strong>Mahesh:</strong> Hmm, let me start by writing the <code>decode_step</code> function:</p>
<pre><code class="language-python">def decode_step(x, enc_bitarray):
    # decode s, retrieve prev state
    s, x_shrunk = rans_base_decode_step(x)

    # expand back x_shrunk to lie in Interval[L,H]
    x_prev, num_bits_step = expand_state(x_shrunk, enc_bitarray)
    return s, x_prev, num_bits_step
</code></pre>
<p>I don't think there is a direct way to <em>cache</em> the entire <code>decode_step</code> function into a lookup table, as the <code>enc_bitarray</code> input is not finite in size, but we could probably cache the functions <code>rans_base_decode_step</code> and <code>expand_state</code> into individual lookup tables.</p>
<p>Let me start by thiking about <code>rans_base_decode_step(x)</code> first. This is just a mapping from <code>x in [L,H]</code> to <code>x_shrunk, s</code>. So the lookup table has <code>M</code> rows, each row consisting of the decoded symbol <code>s</code> and <code>x_shrunk</code>.</p>
<p>As for the <code>expand_state(x_shrunk, enc_bitarray)</code> function, it is a bit confusing as one of the inputs is <code>enc_bitarray</code>, so it is not obvious to me how to cache this function.</p>
<p><strong>Brahma:</strong> Good start Mahesh! You are right that the <code>expand_state(x_shrunk, enc_bitarray)</code> in the current form is not amenable to caching. But, we can modify the <code>expand_state</code> function appropriately to make it cachable!</p>
<p>It is easiest to think about this in the binary alphabet.
Lets say <code>r=5, i.e. M = 2^5</code>, then</p>
<pre><code class="language-python">L =    M = 100000b
H = 2L-1 = 111111b
</code></pre>
<p>and so the state <code>x</code> always looks like <code>1zzzzzb</code> where <code>z</code> is any bit. Does this give any hints?</p>
<p><strong>Vishnu:</strong> Ahh! I see, so after choosing <code>M=2^r</code> and looking in the binary alphabet, it is quite easy to imagine what <code>expand_state</code> is doing. For example: if <code>x_shrunk = 101</code> then, the expanded state <code>x</code> will look like <code>101zzzb</code> where <code>z</code> are the bits we read from the encoded bitstream.</p>
<p>Thus, we can simplify the <code>expand_state</code> function as:</p>
<pre><code class="language-python">
M = 2^r
L = M
H = 2L - 1

####### Original expand_state func ############
def expand_state(x_shrunk, enc_bitarray):
    # init
    num_bits_step = 0

    # read in bits to expand x_shrunk -&gt; x
    x = x_shrunk
    while x not in Interval[L,H]:
        x = x*b_factor + to_uint(enc_bitarray[num_bits_step:num_bits_step+b])
        num_bits_step += b
    return x, num_bits_step

######## New expand_state func #################
def expand_state(x_shrunk, enc_bitarray):
    # if x_shrunk = 101, num_bits_step = (5+1) - 3
    num_bits_step = (r+1) - ceil(log2(x_shrunk+1))

    # get the new state:
    x = (x_shrunk &lt;&lt; num_bits_step) + to_uint(enc_bitarray[:num_bits_step])

    return x, num_bits_step
</code></pre>
<p>Given the new <code>expand_state</code> function, we can convert it into a lookup table which outputs <code>num_bits_step</code>.</p>
<pre><code class="language-python">num_bits_step = expand_state_num_bits_table[x_shrunk]
</code></pre>
<p>As <code>x_shrunk</code> lies in <code>Interval[freq[s], 2*freq[s] - 1]</code>, the <code>expand_state_num_bits_table</code> lookup table will at most have <code>M</code> rows.</p>
<p><strong>Brahma:</strong> Great job Vishnu! That is indeed the correct way to go about things. Notice how our simplified <code>expand_state</code> function for <code>M=2^r</code> can also be used in non-cached <code>rANS</code>.</p>
<p>To summarise the here is how the new rans cached decoding will look like:</p>
<pre><code class="language-python">M = 2^r
L = M
H = 2L - 1

def rans_base_decode_step(x):
   ...
   return (s,x_prev)

def get_expand_state_num_bits(x_shrunk):
    # if x_shrunk = 101, num_bits_step = (5+1) - 3
    num_bits_step = (r+1) - ceil(log2(x_shrunk+1))

    return num_bits_step

### build the tables ###
# NOTE: this is a one time thing which can be done at the beginning of decoding
# or at compile time

# define the tables
rans_base_decode_table_s = {} # stores s
rans_base_decode_table_x_shrunk = {} # stores x_shrunk
expand_state_num_bits = {} # stores num_bits to read from bitstream

# fill the tables
for x in Interval[L,H]:
    s, x_shrunk = rans_base_decode_step(x)
    rans_base_decode_table_s[x] = s
    rans_base_decode_table_x_shrunk[x] = x_shrunk

for s in Alphabet:
    for x_shrunk in Interval[freq[s], 2*freq[s] - 1]:
        expand_state_num_bits[x] = get_expand_state_num_bits(x_shrunk)


### the cached decode_step ########
def decode_step_cached(x, enc_bitarray):
    # decode s, retrieve prev state
    s = rans_base_decode_table_s[x]
    x_shrunk = rans_base_decode_table_x_shrunk[x]

    # expand back x_shrunk to lie in Interval[L,H]
    num_bits_step = expand_state_num_bits[x_shrunk]

    # get the new state:
    x_prev = (x_shrunk &lt;&lt; num_bits_step) + to_uint(enc_bitarray[:num_bits_step])
    return s, x_prev, num_bits_step

</code></pre>
<p>That's it! this completes the <code>cached rANS</code> decoder. Let's also analyse how big our lookup tables are:</p>
<pre><code class="language-python">M = 2^r, L = M, H = 2L - 1
rans_base_decode_table_s #M rows, stores symbol s in Alphabet
rans_base_decode_table_x_shrunk #M rows, stores state x_shrunk &lt;= H
expand_state_num_bits  #M rows, stores num_bits (integer &lt;= r)
</code></pre>
<p>So, the decoding lookup tables are reasonable in terms of size! for <code>M = 2^16</code>, the size of the table is <code>~200KB</code> which is not bad!</p>
<p>Does that make sense?</p>
<p><strong>Mahesh:</strong> Nice! Yes, it does make lot of sense. I believe the construction for <code>L = M*t</code> for a general <code>t</code> is also similar. We still need to assume <code>M = 2^r</code> for our caching to work I suppose.</p>
<p><strong>Brahma:</strong> Thats right Mahesh! <code>M=2^r</code> does simplify things quite a bit. As we will see, it will also simplify the <code>encode_step</code> caching. Do you want to try <em>caching</em> the encoding?</p>
<p><strong>Mahesh:</strong> Yes, let me take a stab: Lets start by writing the <code>encode_step</code> function structure again:</p>
<pre><code class="language-python">def encode_step(x,s):
   # shrink state x before calling base encode
   x_shrunk, out_bits = shrink_state(x, s)

   # perform the base encoding step
   x_next = rans_base_encode_step(x_shrunk,s)
   
   return x_next, out_bits
</code></pre>
<p>Lets start by looking at <code>rans_base_encode_step(x_shrunk,s)</code> function first,  We know that the input state <code>x_shrunk</code> lies in <code>[freq[s], 2*freq[s] - 1]</code>. Thus, we can replace <code>rans_base_encode_step(x_shrunk,s)</code> with a lookup table:</p>
<pre><code class="language-python">base_encode_step_table = {} # contains the next state x_next
for s in Alphabet:
    for x_shrunk in Interval[freq[s], 2*freq[s] - 1]:
        base_encode_step_table[x_shrunk, s] = rans_base_encode_step(x_shrunk,s)
</code></pre>
<p>The <code>base_encode_step_table</code> indexed by <code>(x_shrunk, s)</code> should contain in total <code>sum(freq[s]) = M</code> elements.</p>
<p>Now, coming to the <code>shrink_state(x,s)</code>, I think it might again help visualizing the state <code>x</code> in the binary alphabet.</p>
<p>Lets again take the same example. Say <code>r=5, i.e. M = 2^5</code>, then</p>
<pre><code class="language-python">L =    M = 100000b
H = 2L-1 = 111111b
</code></pre>
<p>and so the state <code>x</code> always looks like <code>1zzzzzb</code> where <code>z</code> is any bit. If you recall, the <code>shrink_state</code> function basically streams out the lower bits of the state <code>x</code>, until it is in the <code>Interval[freq[s], 2*freq[s] - 1]</code>.</p>
<p>Now, lets say, <code>freq[s] &lt;= 2^y &lt; (2*freq[s] - 1)</code>. Then we know the following:</p>
<ol>
<li>For any <code>x_shrunk in [2^y, 2*2^y - 1]</code>, we need to output <code>r+1-y</code> bits.</li>
<li>For any <code>x_shrunk in [2^{y-1}, 2*2^{y-1} - 1]</code>, we need to output <code>r+2-y</code> bits.</li>
</ol>
<p>Note that there can be one and only one integer <code>y</code> satisfying the condition. Also, note that due to the size of the interval <code>[Freq[s], 2*freq[s] - 1]</code>, we either output <code>r+1-y</code> or <code>r+2-y</code> bits, no other scenario can occur. This should help us simplify the <code>shrink_state</code> function as follows:</p>
<pre><code class="language-python">M = 2^r
L = M
H = 2L - 1

####### Original shrink_state func ############
def shrink_state(x,s):
    # initialize the output bitarray
    out_bits = BitArray()

    # shrink state until we are sure the encoded state will lie in the correct interval
    x_shrunk = x
    while x_shrunk not in Interval[freq[s]*t,2*freq[s]*t - 1]:
        out_bits.prepend(x%2)
        x = x//2
    return x_shrunk, out_bits

####### New/Simplified shrink_state func ############
def shrink_state(x,s):
    # calculate the power of 2 lying in [freq[s], 2freq[s] - 1]
    y = ceil(log2(freq[s]))
    num_out_bits = r+1 - y
    thresh = freq[s] &lt;&lt; (num_out_bits+1)
    if x &gt;= thresh:
        num_out_bits += 1
    
    x_shrunk = x &gt;&gt; num_out_bits
    out_bits = to_binary(x)[-num_out_bits:]
             
    return x_shrunk, out_bits
</code></pre>
<p>The new <code>shrink_state</code> much more amenable to caching. We can just cache the <code>y, thresh</code> values for each symbol <code>s</code>. This completes the <code>encode_step</code> caching. The full pseudo-code is as follows:</p>
<pre><code class="language-python">M = 2^r
L = M
H = 2L - 1

def rans_base_encode_step(x_shrunk,s):
    ...
    return x_next

def shrink_state_num_out_bits_base(s):
    # calculate the power of 2 lying in [freq[s], 2freq[s] - 1]
    y = ceil(log2(freq[s]))
    return r + 1 - y

### build the tables ###
# NOTE: this is a one time thing which can be done at the beginning of encoding
# or at compile time

base_encode_step_table = {} #M rows, each storing x_next in [L,H]
for s in Alphabet:
    for x_shrunk in Interval[freq[s], 2*freq[s] - 1]:
        base_encode_step_table[x_shrunk, s] = rans_base_encode_step(x_shrunk,s)

shrink_state_num_out_bits_base = {} #stores the exponent y values as described above
shrink_state_thresh = {} # stores the thresh values
for s in Alphabet:
    shrink_state_num_out_bits_base[s] = shrink_state_num_out_bits_base(s)
    shrink_state_thresh[s] = freq[s] &lt;&lt; (shrink_state_num_out_bits_base[s] + 1)

### the cached encode_step ########
def encode_step_cached(x,s):
    # shrink state x before calling base encode
    num_out_bits = shrink_state_num_out_bits_base[s]
    if x &gt;= shrink_state_thresh[y]:
        num_out_bits += 1
    x_shrunk = x &gt;&gt; num_out_bits
    out_bits = to_binary(x)[-num_out_bits:]

    # perform the base encoding step
    x_next = base_encode_step_table[x_shrunk,s]
   
    return x_next, out_bits
</code></pre>
<p><strong>Brahma:</strong> Very good Mahesh! That is quite precise. Lets again take a minute to see how much memory do our encoding lookup tables take:</p>
<pre><code class="language-python">M = 2^r, L = M, H = 2L - 1
base_encode_step_table = {} #M rows, each storing x_next in [L,H]
shrink_state_num_out_bits_base = {} # A rows (alphabet size) each storing the base num_bits &lt;= r
shrink_state_thresh = {} # A rows (alphabet size) storing an integer &lt;= H
</code></pre>
<p>For an alphabet size of <code>256</code>, this takes around <code>100KB</code> which is quite reasonable. Note that the cached version of the encoding is a bit slow as compared with the decoding, as we need to do a comparison, but it is not bad at all.</p>
<p><strong>Vishnu:</strong> Thanks Brahma! Is the <code>cached rANS</code> another name for <code>tANS</code> or the <code>table ANS</code>? Intuitively the name makes sense, considering all we are now during encoding and decoding is access the lookup tables!</p>
<p><strong>Brahma:</strong> You are right Vishnu! In fact <code>cached rANS</code>, the way in we have discussed is in fact on variant of the table ANS, the <code>tANS</code>. So, in a way <code>tANS</code> is a <em>family</em> of table-based encoders/decoders which have the exact same <code>decode_step_cached</code> and <code>encode_step_cached</code> functions. The main difference between them is how the lookup tables are filled.</p>
<p>For example, if you look at the lookup tables we have, the only constraint we have for the encode table <code>base_encode_step_table</code> and the decode tables <code>rans_base_decode_table_s</code>, <code>rans_base_decode_table_x_shrunk</code> is that they need to be inverses of each other.</p>
<pre><code class="language-python">base_encode_step_table[x_shrunk, s] = x

# implies that
s = rans_base_decode_table_s[x]
x_shrunk = rans_base_decode_table_x_shrunk[x]
</code></pre>
<p>We can for example, permute the values in the tables, ensuring that they are inverses, and that will give us another <code>tANS</code> encoder/decoder.</p>
<p><strong>Vishnu:</strong> Is there a particular permutation of these tables which is the <em>optimal</em> tANS?</p>
<p><strong>Brahma:</strong> As you can imagine, all of these <code>tANS</code> encoders have subtle differences in their compression performance. A more in-depth analysis on <code>tANS</code> and which permutations should one choose is discussed in great detail by Yann and Charles:</p>
<hr />
<p><em>This brings us to the end of our discussion on the Asymmetric Numeral Systems. Hope you enjoyed the discussion! In case you are interested in learning more, here is a curated list of references</em></p>
<ol>
<li><a href="https://arxiv.org/abs/0902.0271">Original Paper</a>: Although the algorithms are great, I unfortunately found it difficult to follow the paper. Still, it might be good to skim through it, as everything we discussed is in there (in some form)</li>
<li><a href="https://fgiesen.wordpress.com/2014/02/02/rans-notes/">Fabian Giesen's Blog</a>: Great blog + implementation focused on <code>rANS</code>. the seqeunce of articles also illustrates some brilliant tricks in code speed optimizations to make <code>rANS</code> fast.</li>
<li><a href="https://cbloomrants.blogspot.com/2014/02/02-18-14-understanding-ans-conclusion.html">Charles Bloom's Blog</a>: Great sequence of articles again. We borrow quite a few things (including the examples) from here. It contains more in-depth discussion of different variants <code>tANS</code> among other things</li>
<li><a href="http://fastcompression.blogspot.com/2013/12/finite-state-entropy-new-breed-of.html">Yann Collet's Blog</a>: A sequence of articles discussing <code>tANS</code> from a totally different perspective, which connects <code>tANS</code> with Huffman coding and unifies them under a single framework. Very illuminating!</li>
</ol>
<p><strong>TODO: Add links to theoretical works on ANS</strong></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="non-iid-sources-and-entropy-rate"><a class="header" href="#non-iid-sources-and-entropy-rate">Non IID sources and entropy rate</a></h1>
<h2 id="recap"><a class="header" href="#recap">Recap</a></h2>
<p>In the previous chapters, we focused on various entropy coding techniques like Huffman codes, arithmetic codes and rANS/tANS. We can roughly summarize these as shown below.</p>
<ul>
<li>Huffman Coding
<ul>
<li>very fast</li>
<li>optimal symbol coder</li>
<li>achieves entropy when working with larger blocks - exponential complexity</li>
</ul>
</li>
<li>Arithmetic Coding
<ul>
<li>achieves entropy efficiently by treating entire input as a block</li>
<li>division operations are expensive</li>
<li>easily extends to complex and adaptive probability models</li>
</ul>
</li>
<li>ANS (Asymmetric Numeral Systems)
<ul>
<li>achieves entropy efficiently by treating entire input as a block</li>
<li>small compression overhead over arithmetic coding</li>
<li>two variants: rANS and tANS</li>
<li>faster than arithmetic coding</li>
<li>can modify base algorithm to use modern instructions like SIMD</li>
</ul>
</li>
</ul>
<p>Various implementation details of the above coders is beyond the scope here, but you can refer to the following resources to learn about these:</p>
<ul>
<li>Further reading on entropy coders, particularly ANS
<ul>
<li><a href="https://stanforddatacompressionclass.github.io/notes/lossless_iid/ans.html">Kedar's lecture notes</a></li>
<li><a href="https://arxiv.org/abs/1311.2540">Paper by inventor (Jarek Duda)</a>: "Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding"</li>
<li><a href="https://fastcompression.blogspot.com/2013/12/finite-state-entropy-new-breed-of.html">Yann Collet's blog</a> - creator of FSE (tANS), LZ4 and zstd compression libraries</li>
<li><a href="https://cbloomrants.blogspot.com/2014/02/02-18-14-understanding-ans-conclusion.html">Charles Bloom's blog</a></li>
<li>Fabian Giesen's <a href="https://fgiesen.wordpress.com/2015/12/21/rans-in-practice/">blog</a> and <a href="https://arxiv.org/abs/1311.2540">paper on rANS with SIMD</a></li>
<li>https://encode.su/forums/2-Data-Compression - compression forum</li>
</ul>
</li>
</ul>
<p>To summarize their performance metrics on some data set which is roughly representative of their performance:</p>
<div class="table-wrapper"><table><thead><tr><th>Codec</th><th>Encode speed</th><th>Decode speed</th><th>compression</th></tr></thead><tbody>
<tr><td>Huffman coding (1950s)</td><td>252 Mb/s</td><td>300 Mb/s</td><td>1.66</td></tr>
<tr><td>Arithmetic coding (1970s)</td><td>120 Mb/s</td><td>69 Mb/s</td><td>1.24</td></tr>
<tr><td>rANS (2010s)</td><td>76 Mb/s</td><td>140 Mb/s</td><td>1.24</td></tr>
<tr><td>tANS (2010s)</td><td>163 Mb/s</td><td>284 Mb/s</td><td>1.25</td></tr>
</tbody></table>
</div>
<p>Source: <a href="http://cbloomrants.blogspot.com/2014/02/02-01-14-understanding-ans-3.html">Charles Bloom's blog</a></p>
<p>As you can see, all of these entropy coders are used in practice due to their unique strengths (and sometimes for legacy reasons). We will keep revisiting these as components of various compression methods.</p>
<h2 id="non-iid-sources"><a class="header" href="#non-iid-sources">Non-IID sources</a></h2>
<p>Let us do a simple experiment on a real text file, a Sherlock Holmes novel!</p>
<pre><code>$ cat sherlock.txt
    ...
      In mere size and strength it was a terrible creature which was
      lying stretched before us. It was not a pure bloodhound and it
      was not a pure mastiff; but it appeared to be a combination of
      the twoâ€”gaunt, savage, and as large as a small lioness. Even now
      in the stillness of death, the huge jaws seemed to be dripping
      with a bluish flame and the small, deep-set, cruel eyes were
      ringed with fire. I placed my hand upon the glowing muzzle, and
      as I held them up my own fingers smouldered and gleamed in the
      darkness.

      â€œPhosphorus,â€ I said.

      â€œA cunning preparation of it,â€ said Holmes, sniffing at the dead
    ...
</code></pre>
<p>Let's try and compress this 387 KB book.</p>
<pre><code class="language-py">&gt;&gt;&gt; from core.data_block import DataBlock
&gt;&gt;&gt; 
&gt;&gt;&gt; with open("sherlock.txt") as f:
&gt;&gt;&gt;     data = f.read()
&gt;&gt;&gt; 
&gt;&gt;&gt; print(DataBlock(data).get_entropy()*len(data)/8, "bytes")

199833 bytes
</code></pre>
<pre><code>$ gzip &lt; sherlock.txt | wc -c
134718

$ bzip2 &lt; sherlock.txt | wc -c
99679
</code></pre>
<p>How can compressors like gzip and bzip2 do better than entropy? Recall that our entropy bound only applied to iid sources. In fact, our focus till now has exclusively been on iid data, i.e., we assumed that the random variables comprising our source were independent and identically distributed. Recall that for an iid source, if we have a sequence $U^n = (U_1,\dots,U_n)$, then it's probability can be decomposed like $P(U^n) = \Pi_{i=1}^n P(U_i)$. As you might have seen in your probability class, this is hardly the only class of probabilistic sources. In general the chain rule of probability gives us
$$P(U^n) = \Pi_{i=1}^n P(U_i|U^{i-1})$$
which tells us that the conditional probability of the $i$th symbol depends on the sequence of symbols seen before. In contrast, for iid sources, knowing the history tells us nothing about the distribution of the next symbol. Let's consider some of the most common data sources and see how they have strong dependence across the sequence of symbols.</p>
<ul>
<li><strong>Text</strong>: In English (or any other common language), knowing the past few letters/words/sentences helps predict the upcoming text. For example, if the last letter was "Q", there is a high probability that the next letter is a "U". In contrast, if the last letter was a "U", it is very unlikely that the next letter is also a "U".</li>
<li><strong>Images</strong>: In a typical image, nearby pixels are very likely to have similar brightness and color (except at sharp edges).</li>
<li><strong>Videos</strong>: Consecutive video frames are usually very similar with a small shift or rotation.</li>
</ul>
<p>In this and the next chapter, we will study the theory behind compression of non-IID data and look at algorithmic techniques to achieve the optimal compression. The previous techniques focusing on IID data compression play a critical role even in this setting, as will become clear shortly.</p>
<h2 id="definitions-and-examples"><a class="header" href="#definitions-and-examples">Definitions and examples</a></h2>
<p>Given alphabet $\mathcal{X}$, a <em>stochastic process</em> $(U_1,U_2,\dots)$ can have arbitrary dependence across the elements and is characterized by $P((U_1,U_2,\dots,U_n)= (u_1,u_2,\dots,u_n))$ for $n=1,2,\dots$ and $(u_1,u_2,\dots,u_n)\in \mathcal{U}^n$. This is way too general to develop useful theory or algorithms (need to separately define joint distribution for every $n$), so we mostly restrict our attention to time-invariant or stationary processes.</p>
<h3 id="stationary-process"><a class="header" href="#stationary-process">Stationary process</a></h3>
<div id="admonition-definition-stationary-process" class="admonition admonish-example" role="note" aria-labelledby="admonition-definition-stationary-process-title">
<div class="admonition-title">
<div id="admonition-definition-stationary-process-title">
<p>Definition: Stationary process</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/non_iid_sources.html#admonition-definition-stationary-process"></a>
</div>
<div>
<p>A stationary process is a stochastic process that is time-invariant, i.e., the probability distribution doesn't change with time (here time refers to the index in the sequence). More precisely, we have
$$P(U_1=u_1,U_2=u_2,\dots,U_n=u_n)=P(U_{l+1}=u_1,U_{l+2}=u_2,\dots,U_{l+n}=u_n)$$
for every $n$, every shift $l$ and all $(u_1,u_2,\dots,u_n)\in \mathcal{U}^n$.</p>
</div>
</div>
<p>In particular, statistical properties such as mean and variance (and entropy for that matter) do not change with time. Most real data sources are either close to stationary or can be invertibly transformed into a stationary source. Before we look at a few examples of stationary and non-stationary sources, let us first define an important subcategory of stationary sources.</p>
<h4 id="kth-order-markov-sources"><a class="header" href="#kth-order-markov-sources">$k$th order Markov sources</a></h4>
<p>In a stationary source, there can be arbitrarily long time dependence between symbols in the sequence (also referred to as "memory"). In practice, however, this is typically not true as the dependence is strongest between nearby elements and rapidly gets weaker. Furthermore, it is impossible to accurately describe a general stationary distribution with finite memory. Therefore, in many cases, we like to focus in further on source with finite memory $k$. Most practical stationary sources can be approximated well with a finite memory $k$th order Markov source with higher values of $k$ typically providing a better approximation (with diminishing returns).</p>
<div id="admonition-definition-kth-order-markov-source" class="admonition admonish-example" role="note" aria-labelledby="admonition-definition-kth-order-markov-source-title">
<div class="admonition-title">
<div id="admonition-definition-kth-order-markov-source-title">
<p>Definition: $k$th order Markov source</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/non_iid_sources.html#admonition-definition-kth-order-markov-source"></a>
</div>
<div>
<p>A $k$th order Markov source is defined by the condition
$$P(U_n|U_{n-1}U_{n-2}\dots) = P(U_n|U_{n-1}U_{n-2}\dots U_{n-k})$$
for every $n$. In words, the conditional probability of $U_n$ given the entire past depends only on the past $k$ symbols. Or more precisely, $U_n$ is independent of the past older than $k$ symbols given the last $k$ symbols.</p>
<p>We will restrict attention to stationary Markov sources meaning that the distribution above doesn't change with a time-shift.</p>
</div>
</div>
<p>You might have already studied first order Markov sources (often called just Markov sources) in a probability class. We will not go into the extensive properties of Markov sources here, and we refer the reader to any probability text. In closing, we ask the reader to convince themselves that a stationary $0$th order Markov source is simply an iid source!</p>
<h3 id="conditional-entropy-and-entropy-rate"><a class="header" href="#conditional-entropy-and-entropy-rate">Conditional entropy and entropy rate</a></h3>
<div id="admonition-definition-conditional-entropy" class="admonition admonish-example" role="note" aria-labelledby="admonition-definition-conditional-entropy-title">
<div class="admonition-title">
<div id="admonition-definition-conditional-entropy-title">
<p>Definition: Conditional entropy</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/non_iid_sources.html#admonition-definition-conditional-entropy"></a>
</div>
<div>
<p>The conditional entropy of $U$ given $V$ is defined as
$$H(U|V) \triangleq E\left[\log \frac{1}{P(U|V)}\right]$$
This can also be written as
$$
\begin{aligned}</p>
<p>H(U|V) &amp;= \sum_{u\in \mathcal{U},v \in \mathcal{V}} P(u,v) \log \frac{1}{P(u|v)}\
&amp;= \sum_{v \in \mathcal{V}} P(v) \sum_{u\in \mathcal{U}} P(u|v) \log \frac{1}{P(u|v)}\
&amp;= \sum_{v \in \mathcal{V}} P(v) H(U| V=v)
\end{aligned}
$$</p>
</div>
</div>
<p>Using the properties of entropy and KL-Divergence, it is easy to show the following properties of conditional entropy:</p>
<div id="admonition-quiz-properties-of-conditional-entropy" class="admonition admonish-question" role="note" aria-labelledby="admonition-quiz-properties-of-conditional-entropy-title">
<div class="admonition-title">
<div id="admonition-quiz-properties-of-conditional-entropy-title">
<p>Quiz: Properties of conditional entropy</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/non_iid_sources.html#admonition-quiz-properties-of-conditional-entropy"></a>
</div>
<div>
<p>Show the following:</p>
<ol>
<li>Conditioning reduces entropy: $$H(U|V) \leq H(U)$$ with equality iff $U$ and $V$ are independent. In words, this is saying that conditioning reduces entropy on average - which is expected because entropy is a measure of uncertainty and additional information should help reduce the uncertainty unless the information ($V$) is independent of $U$. Note that this is true in average, but that $H(U|V=v)$ may be greater than or less than or equal to $H(U)$. Citing an example from Cover and Thomas, in a court case, specific new evidence might increase uncertainty, but on the average evidence decreases uncertainty.</li>
<li>Chain rule of entropy:
$$H(U,V) = H(U) + H(V|U) = H(V) + H(U|V)$$
<em>Hint</em>: Use the chain rule of probability.</li>
<li>Joint entropy vs. sum of entropies:
$$H(U,V) \leq H(U) + H(V)$$
with equality holding iff $U$ and $V$ are independent.</li>
</ol>
</div>
</div>
<p>Thus conditional entropy $H(U|V)$ can be thought of as the remaining uncertainty in $U$ once $V$ is known, and the conditional entropy $H(U|V=v)$ can be thought of as the remaining uncertainty in $U$ if $V$ is known to be $v$. Intuitively one can expect that if $V$ is known to both the encoder and decoder, then compressing $U$ should take $H(U|V)$ bits (which is less than or equal to $H(U)$ with equality when $V$ is independent of $U$ and hence shouldn't help with compression). This is also apparent from $H(U,V) = H(V) + H(U|V)$ which says that after $V$ is stored with $H(V)$ bits only an additional $H(U|V)$ bits are needed to represent $U$.</p>
<p>The above can be generalized to conditioning $U_{n+1}$ on $(U_1,U_2,\dots,U_n)$:
$$H(U_{n+1}|U_1,U_2,\dots,U_n)$$</p>
<p>Let us now define the entropy rate for a stationary process. Before we do that, let us list some desired properties:</p>
<ul>
<li>works for arbitrarily long dependency so $H(U_{n+1}|U_1,U_2,\dots,U_n)$ for any finite $n$ won't do</li>
<li>has <em>operational</em> meaning in compression just like entropy</li>
<li>is well-defined for any stationary process</li>
</ul>
<div id="admonition-definition-entropy-rate" class="admonition admonish-example" role="note" aria-labelledby="admonition-definition-entropy-rate-title">
<div class="admonition-title">
<div id="admonition-definition-entropy-rate-title">
<p>Definition: Entropy rate</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/non_iid_sources.html#admonition-definition-entropy-rate"></a>
</div>
<div>
<p>$$H_1(\mathbf{U}) = \lim_{n\rightarrow\infty} H(U_{n+1}|U_1,U_2,\dots,U_n)$$
$$H_2(\mathbf{U}) = \lim_{n\rightarrow\infty} \frac{H(U_1,U_2,\dots,U_n)}{n}$$</p>
<p><strong>C&amp;T Thm 4.2.1</strong>
For a stationary stochastic process, the two limits above are equal. We represent the limit as $H(\mathbf{U})$ (entropy rate of the process, also denoted as $H(\mathcal{U})$).</p>
</div>
</div>
<p>We end this discussion with the operational significance of the entropy rate, where it plays the same role for stationary sources as entropy does for iid sources. This is based on the following theorem (see Cover and Thomas for proof):</p>
<div id="admonition-theorem-shannonmcmillanbreiman-theorem-asymptotic-equipartition-property-aep-for-stationary-sources" class="admonition admonish-example" role="note" aria-labelledby="admonition-theorem-shannonmcmillanbreiman-theorem-asymptotic-equipartition-property-aep-for-stationary-sources-title">
<div class="admonition-title">
<div id="admonition-theorem-shannonmcmillanbreiman-theorem-asymptotic-equipartition-property-aep-for-stationary-sources-title">
<p>Theorem: Shannonâ€“McMillanâ€“Breiman theorem (Asymptotic equipartition property (AEP) for stationary sources)</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/non_iid_sources.html#admonition-theorem-shannonmcmillanbreiman-theorem-asymptotic-equipartition-property-aep-for-stationary-sources"></a>
</div>
<div>
<p>$$-\frac{1}{n}\log_2 P(U_1,U_2,\dots,U_n) \rightarrow H(\mathbf{U}) \textrm{ almost surely}$$
under technical conditions (ergodicity).</p>
</div>
</div>
<h3 id="examples"><a class="header" href="#examples">Examples</a></h3>
<h4 id="iid-sources"><a class="header" href="#iid-sources">IID sources</a></h4>
<p>For an iid source with distribution $U$, $H(U_n|U_{n-1},\dots,U_{n-k}) = H(U_n) = H(U)$ and therefore, the entropy rate according to either definition is simply $H(U)$. Thus the entropy rate of an iid source is simply the entropy of the source.</p>
<h4 id="markov-sources"><a class="header" href="#markov-sources">Markov sources</a></h4>
<p>For a $k$th order stationary Markov source, $H(U_n|U_{n-1},\dots,U_{1}) = H(U_n | U_{n-1},\dots,U_{n-k}) = H(U_{k+1}|U_k, \dots, U_1)$ (first equality is by the Markov property and the second is by stationarity). Thus the entropy rate is simply $H(U_{k+1}|U_k, \dots, U_1)$.</p>
<p>Let us look at an example of a 1st order Markov source. Consider the following Markov chain, represented as a set of equations, as a transition matrix and as a state diagram:</p>
<p>$$U_1 \sim Unif({0,1,2})$$
$$U_{i+1} = (U_i + Z_i) \ \textrm{mod}\  3$$
$$Z_i \sim Ber\left(\frac{1}{2}\right)$$</p>
<pre><code>Transition matrix
   U_{i+1} 0   1   2
U_i 
0         0.5 0.5 0.0
1         0.0 0.5 0.5 
2         0.5 0.0 0.5
</code></pre>
<p><img src="lossless_iid/images/markov.png" alt="" /></p>
<p>In the chain, given the previous symbol, the next symbol can only take one of two values with equal probability. Thus the entropy rate is $H(U_2|U_1) = 1$ bit. This is also apparent from the transition matrix which has only two non-zero entries in each row.</p>
<p>Note that the condition that $P(U_1)$ be uniformly distributed is crucial to ensure that the process is stationary (otherwise the distribution of $U_i$ would change with time). For a time-invariant Markov chain (i.e., a chain where the transition matrix doesn't change with time), checking stationarity is as simple as checking that the distribution of $U_i$ is the same for all $i$, or even more simply that $U_1$ and $U_2$ have the same distribution.</p>
<p>Final note on the above source: note that if you define a process $Z_i = X_{i+1} - X_{i} \textrm{ mod}\  3$ then the process $Z$ is a lossless transformation of the process $X$, and in fact the process $Z$ is an iid process. Given all our knowledge about entropy coding for lossless processes, we now have an easy way to optimally compress the Markov process. While such a simple transformation is not always possible, we can often apply simple transforms to the data to make it more amenable to compression with simpler techniques.</p>
<h4 id="bus-arrival-times"><a class="header" href="#bus-arrival-times">Bus arrival times</a></h4>
<p>Consider arrival times for buses at a bus stop: $U_1, U_2, U_3, U_4, \dots$ which look something like 4:16 pm, 4:28 pm, 4:46 pm, 5:02 pm (the expected gap between buses is 15 minutes here). This is not a stationary process because the actual arrival times keep increasing. However the difference process obtained by subtracting the previous arrival time from the current arrival time can be modeled as iid. Again, we see that simple transformations can make a process into a simpler process that can be compressed easily with our entropy coders.</p>
<h4 id="entropy-rate-of-english-text"><a class="header" href="#entropy-rate-of-english-text">Entropy rate of English text</a></h4>
<p><img src="lossless_iid/images/text_entropy_rate.png" alt="w:1000" /></p>
<p>The figure above from http://reeves.ee.duke.edu/information_theory/lecture4-Entropy_Rates.pdf shows the $k$th order entropy of English text based on a large corpus. We see the conditional entropies reduce (and will eventually converge to the entropy rate). The final figure is based on a "human prediction" experiment done by Shannon where he asked humans to predict the next character based on the past and used that as a probability model to estimate the entropy rate of text! We will look much more at how text compression has evolved over the years in the next chapter.</p>
<h2 id="how-to-achieve-the-entropy-rate"><a class="header" href="#how-to-achieve-the-entropy-rate">How to achieve the entropy rate?</a></h2>
<p>Let's start small and try to achieve 1st order entropy $H(U_{k+1}|U_k)$ (note that this is indeed the entropy rate for a first-order Markov chain). Assume we have a known stationary distribution $P(U_1)$ and transition probability $P(U_2|U_1)$. Support we want to compress a block of length $n$ using
$$E\left[\log_2 \frac{1}{P(U_1,\dots,U_n)}\right] \approx nH(U_2|U_1)$$
bits.</p>
<p>The first idea might be to use a Huffman code on block of length $n$. This would work but as usual we get unreasonable complexity. So we could consider working with smaller blocks. However, for non-iid sources, working on independent symbols is just plain suboptimal even discounting the effects of non-dyadic distributions. The reason is that we miss out on the dependence across the blocks.</p>
<div id="admonition-exercise-small-block-suboptimality-for-markov-sources" class="admonition admonish-question" role="note" aria-labelledby="admonition-exercise-small-block-suboptimality-for-markov-sources-title">
<div class="admonition-title">
<div id="admonition-exercise-small-block-suboptimality-for-markov-sources-title">
<p>Exercise: Small block suboptimality for Markov sources</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/non_iid_sources.html#admonition-exercise-small-block-suboptimality-for-markov-sources"></a>
</div>
<div>
<p>Compute $H(U_1)$ and $H(U_1,U_2)$ for</p>
<p>$$U_1 \sim Unif({0,1,2})$$
$$U_{i+1} = (U_i + Z_i) \ \textrm{mod}\  3$$
$$Z_i \sim Ber\left(\frac{1}{2}\right)$$
and compare to $H(\mathbf{U})$.</p>
</div>
</div>
<p>As you might guess, a better solution is based on arithmetic coding (the basic principles are recapped below):</p>
<p><img src="lossless_iid/images/arithmetic_recap.png" alt="" /></p>
<p>The above is for an iid source, but the basic idea of reducing the interval according to the probability of the symbol can be easily applied to the Markov case! Instead of reducing the interval by a factor of $P(U_i)$, we reduce it by $P(U_i|U_{i-1})$. This is illustrated below for the given Markov source:</p>
<p><img src="lossless_iid/images/arithmetic_markov.png" alt="" /></p>
<p>The general idea is simple: at every step, split interval by $P(-|u_{i-1})$ [more generally by $P(-|\textrm{entire past})$]. To see how this performs, consider the length of interval after encoding $u_1,u_2,u_3,\dots,u_n = P(u_1)P(u_2|u_1)\dots P(u_n|u_{n-1})$.</p>
<p>Thus the bits for encoding $u_1,u_2,u_3,\dots,u_n$ is given by $\log_2 \frac{1}{\mathrm{interval\ length}}$ which is</p>
<p>$$\approx \log_2 \frac{1}{P(u_1)P(u_2|u_1)\dots P(u_n|u_{n-1})}$$</p>
<p>Thus the expected bits per symbol
$$\begin{align*}
&amp;\approx \frac{1}{n}E\left[\log_2 \frac{1}{P(U_1)P(U_2|U_1)\dots P(U_n|U_{n-1})}\right] \
&amp;= \frac{1}{n}E\left[\log_2 \frac{1}{P(U_1)}\right]+\frac{1}{n}\sum_{i=2}^{n} E\left[\log_2 \frac{1}{P(U_i|U_{i-1})}\right] \
&amp;= \frac{1}{n}H(U_1)+\frac{n-1}{n} H(U_2|U_1)\
&amp;\sim H(U_2|U_1)
\end{align*}
$$</p>
<p>Thus we can achieve the entropy rate for a first-order Markov source using arithmetic coding. This can be generalized to higher order Markov sources as well. In the next lecture, we will see how this approach can be generalized to achieve compression with arbitrary prediction models using context-based arithmetic coding as well as context-based adaptive arithmetic coding.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="context-based-arithmetic-coding"><a class="header" href="#context-based-arithmetic-coding">Context based arithmetic coding</a></h1>
<p>Recall that we studied stationary process, Markov chains, conditional entropy, and entropy rate in the last lecture.</p>
<p>Conditional entropy was defined as</p>
<p>$$H(U|V) \triangleq E\left[\log \frac{1}{P(U|V)}\right] = \sum_{v \in \mathcal{V}} P(v) \sum_{u\in \mathcal{U}} H(U|V=v)$$</p>
<p>Entropy rate was defined as
$$H(\mathbf{U}) = \lim_{n\rightarrow\infty} H(U_{n+1}|U_1,U_2,\dots,U_n) = \lim_{n\rightarrow\infty} \frac{H(U_1,U_2,\dots,U_n)}{n}$$</p>
<p>Recall also that entropy rate is the fundamental limit of lossless compression for stationary sources.</p>
<p>This lectures asks the question - how do we compress a Markov/stationary source? We will study one technique for doing so - context based arithmetic coding.</p>
<h2 id="achieving-the-entropy-rate-for-a-first-order-markov-source"><a class="header" href="#achieving-the-entropy-rate-for-a-first-order-markov-source">Achieving the entropy rate for a first order Markov source</a></h2>
<p>Let's start simple. How do we achieve the entropy rate for a first order Markov source with known probability distribution?</p>
<p>Recall entropy rate
$H(\mathbf{U}) = \lim_{n\rightarrow\infty} \frac{H(U_1,U_2,\dots,U_n)}{n} = \lim_{n\rightarrow\infty} H(U_{n+1}|U_1,U_2,\dots,U_n)$</p>
<p>For a first-order Markov source this is simply
$H(\mathbf{U}) = \lim_{n\rightarrow\infty} \frac{H(U_1,U_2,\dots,U_n)}{n} = H(U_2|U_1)$</p>
<p>This suggests two ways:</p>
<ol>
<li>Coding in bigger and bigger blocks (to achieve $\lim_{n\rightarrow\infty} \frac{H(U_1,U_2,\dots,U_n)}{n}$)</li>
<li>Coding incrementally (to achieve $H(U_2|U_1)$)</li>
</ol>
<p>We already saw an example of the first idea with Huffman coding at the end of the previous chapter. That suffers from complexity as the block size grows. Let us now focus on the second idea.</p>
<p>Recall arithmetic coding for iid sources:
<img src="lossless_iid/images/arithmetic_recap.png" alt="" /></p>
<p>The above is for an iid source, but the basic idea of reducing the interval according to the probability of the symbol can be easily applied to the Markov case! Instead of reducing the interval by a factor of $P(U_i)$, we reduce it by $P(U_i|U_{i-1})$. This is illustrated below for the given Markov source:</p>
<p><img src="lossless_iid/images/arithmetic_markov.png" alt="" /></p>
<p>The general idea is simple: at every step, split interval by $P(-|u_{i-1})$ [more generally by $P(-|\textrm{entire past})$]. To see how this performs, consider the length of interval after encoding $u_1,u_2,u_3,\dots,u_n = P(u_1)P(u_2|u_1)\dots P(u_n|u_{n-1})$.</p>
<p>Thus the bits for encoding $u_1,u_2,u_3,\dots,u_n$ is given by $\log_2 \frac{1}{\mathrm{interval\ length}}$ which is</p>
<p>$$\approx \log_2 \frac{1}{P(u_1)P(u_2|u_1)\dots P(u_n|u_{n-1})}$$</p>
<p>Thus the expected bits per symbol
$$\begin{align*}
&amp;\approx \frac{1}{n}E\left[\log_2 \frac{1}{P(U_1)P(U_2|U_1)\dots P(U_n|U_{n-1})}\right] \
&amp;= \frac{1}{n}E\left[\log_2 \frac{1}{P(U_1)}\right]+\frac{1}{n}\sum_{i=2}^{n} E\left[\log_2 \frac{1}{P(U_i|U_{i-1})}\right] \
&amp;= \frac{1}{n}H(U_1)+\frac{n-1}{n} H(U_2|U_1)\
&amp;\sim H(U_2|U_1)
\end{align*}
$$</p>
<p>Thus we can achieve the entropy rate for a first-order Markov source using arithmetic coding.</p>
<h2 id="context-based-arithmetic-coding-1"><a class="header" href="#context-based-arithmetic-coding-1">Context based arithmetic coding</a></h2>
<p>Generalizing this further, we get
<img src="lossless_iid/images/arith_general.png" alt="" /></p>
<p>So as long as we can estimate the probability distribution of the next symbol given some context, we can use arithmetic coding to encode the data. The bits used to encode $u_n$ is simply $\log_2 \frac{1}{P(u_n|past)}$. Higher the probability of the actually observed symbol, lower the bits you pay!</p>
<p>For a $k$th order model, the previous $k$ symbols are sufficient to predict the next symbol. But in general, the more past context you can use, the better the prediction. As an example let us consider using an LLM to predict the next token given some past:</p>
<pre><code>&gt;&gt;&gt; predict_next_token("than")
Token: x, Probability: 18.6%
Token: e, Probability: 8.5%
Token: , Probability: 5.2%
Token: the, Probability: 5.2%
Token: king, Probability: 4.3%
</code></pre>
<pre><code>&gt;&gt;&gt; predict_next_token("louder than")
Token: words, Probability: 30.4%
Token: love, Probability: 11.9%
Token: a, Probability: 11.2%
Token: the, Probability: 5.8%
Token: bombs, Probability: 4.7%
</code></pre>
<pre><code>&gt;&gt;&gt; predict_next_token("speak louder than")
Token: words, Probability: 47.8%
Token: money, Probability: 7.8%
Token: a, Probability: 4.7%
Token: the, Probability: 3.2%
Token: actions, Probability: 2.5%
</code></pre>
<pre><code>&gt;&gt;&gt; predict_next_token("Actions speak louder than")
Token: words, Probability: 96.5%
Token: the, Probability: 0.2%
Token: a, Probability: 0.1%
Token: any, Probability: 0.1%
Token: Words, Probability: 0.1%
</code></pre>
<p>We see that the prediction gets better as we use more context, and the probability of the actual token is higher. This is the basic idea behind context based arithmetic coding.</p>
<p>Before we look at some specific prediction models, let's look at the general framework for context-based arithmetic coding.</p>
<p><img src="lossless_iid/images/arith_block.png" alt="" /></p>
<p>Total bits for encoding:
$$\sum_{i=1}^n \log_2 \frac{1}{\hat{P}(u_i|u_1,\dots,u_{i-1})}$$</p>
<p>Let us think about how decoding would work. The idea is simple: the decoder uses same model, at step $i$ it has access to $u_1,\dots,u_{i-1}$ already decoded and so can generate the $\hat{P}$ for the arithmetic coding step!</p>
<p>Now let us generalize this one more step: what if you don't already know the model? There are two common approaches:</p>
<p><strong>Option 1:</strong> Two pass: first build ("train") model from data, then encode using it.</p>
<p><strong>Option 2:</strong> Adaptive: build ("train") model from data as we see it.</p>
<p>These approaches have their pros and cons, some of which are listed below:</p>
<h3 id="two-pass-vs-adaptive"><a class="header" href="#two-pass-vs-adaptive">Two-pass vs. adaptive</a></h3>
<h4 id="two-pass-approach"><a class="header" href="#two-pass-approach">Two-pass approach</a></h4>
<p>âœ… learn model from entire data, leading to potentially better compression</p>
<p>âœ… more suited for parallelization</p>
<p>âŒ need to store model in compressed file</p>
<p>âŒ need two passes over data, not suitable for streaming</p>
<p>âŒ might not work well with changing statistics</p>
<h4 id="adaptive-approach"><a class="header" href="#adaptive-approach">Adaptive approach</a></h4>
<p>âœ… no need to store the model</p>
<p>âœ… suitable for streaming</p>
<p>âŒ adaptively learning model leads to inefficiency for initial samples</p>
<p>âœ… works pretty well in practice!</p>
<p>For the most part, we will focus on the adaptive approach, which is shown the figure below:</p>
<p><img src="lossless_iid/images/arith_block_adaptive.png" alt="" /></p>
<p>A few notes on this approach:</p>
<p>âš ï¸ It is important for encoder and decoder to share exactly the same model state at every step (including at initialization). Otherwise the models go out of sync and arithmetic decoding fails to recover the original data. This is especially important for neural net based models where there are sources of randomness such as GPUs. Note that this point affects both the adaptive and pretrained approaches.</p>
<p>âš ï¸ Be careful about updating the model with $u_i$ only after you perform the encoding for $u_i$. As a general thumb rule, always think whether the decoder has the knowledge needed to perform the next step!</p>
<p>âš ï¸ Try not to provide $0$ probability to any symbol, otherwise arithmetic coding will not be able to produce a bounded output. The trick is to assign small but non-zero probability to all symbols, however unlikely they might be.</p>
<h3 id="compression-and-prediction"><a class="header" href="#compression-and-prediction">Compression and prediction</a></h3>
<p>All of this discussion suggests a close relation between compression and prediction. In fact the cross-entropy loss for prediction (classes $\mathcal{C}$, predicted probabilities $\hat{P}$, ground truth class: $y$):
$$\sum_{c\in\mathcal{C}} \mathbf{1}<em>{y_i=c} \log_2\frac{1}{\hat{P}(c|y_1,\dots,y</em>{i-1})}$$</p>
<p>Loss incurred when ground truth is $y_i$ is $\log_2\frac{1}{\hat{P}(y_i|y_1,\dots,y_{i-1})}$</p>
<p>Exactly matches the number of bits used for encoding with arithmetic coding!</p>
<p>Prediction implies compression:</p>
<ul>
<li>Good prediction =&gt; Good compression</li>
<li>Compression = having a good model for the data</li>
<li>Need not always explicitly model the data</li>
<li>Possible to use rANS instead of arithmetic coding in some settings</li>
</ul>
<p>This also goes the other way:</p>
<ul>
<li>Each compressor induces a predictor!</li>
<li>Recall relation between code length and induced probability model $p \sim 2^{-l}$</li>
<li>Generalizes to prediction setting</li>
<li>Explicitly obtaining the prediction probabilities easier with some compressors than others. For compressors that explicitly model the data it is easier, but for others (such as LZ77) it is much harder to describe the probability model in a computationally efficient way. One can brute force compute the model by compressing all possible choices and then calculating the probabilities according to the code lengths, but this is computationally infeasible in many scenarios.</li>
</ul>
<p>You can read more about this relationship in the recent perprint by DeepMind titled "Language Modeling Is Compression": https://aps.arxiv.org/abs/2309.10668.</p>
<h2 id="prediction-models-used-for-context-based-arithmetic-coding"><a class="header" href="#prediction-models-used-for-context-based-arithmetic-coding">Prediction models used for context based arithmetic coding</a></h2>
<h3 id="kth-order-adaptive-arithmetic-coding"><a class="header" href="#kth-order-adaptive-arithmetic-coding">$k$th order adaptive arithmetic coding</a></h3>
<ul>
<li>Start with a frequency of 1 for each symbol in the $(k+1)$th order alphabet (to avoid zero probabilities)</li>
<li>As you see symbols, update the frequency counts</li>
<li>At each step you have a probability distribution over the alphabet induced by the counts</li>
</ul>
<p>Remember to update the counts with a symbol after you encode a symbol!</p>
<p>Example: if you saw BANA in past followed by N 90% of times and by L 10% of times, then predict N with probability 0.9 and L with probability 0.1 given a context of BANA.</p>
<h3 id="example-1st-order-adaptive-arithmetic-coding"><a class="header" href="#example-1st-order-adaptive-arithmetic-coding">Example: 1st order adaptive arithmetic coding</a></h3>
<p>Consider the data sequence: 101011</p>
<p>We initialize the counts like
$$c(0,0) = 1$$
$$c(0,1) = 1$$
$$c(1,0) = 1$$
$$c(1,1) = 1$$</p>
<p>and assume the past is padded with 0s (to handle the initial condition).</p>
<p>For the first input <strong>1</strong>01011:
<code>Current symbol: 1 Previous symbol: 0 (padding)</code></p>
<p>Predicted probability: $P(1|0) = \frac{c(0,1)}{c(0,0)+c(0,1)} = \frac{1}{2}$</p>
<p>Counts:
$$c(0,0) = 1$$
$$c(0,1) = 1 \rightarrow 2$$
(this got updated)
$$c(1,0) = 1$$
$$c(1,1) = 1$$</p>
<p>For the second input 1<strong>0</strong>1011:
<code>Current symbol: 0 Previous symbol: 1</code></p>
<p>Predicted probability: $P(0|1) = \frac{c(1,0)}{c(1,0)+c(1,1)} = \frac{1}{2}$</p>
<p>Counts:
$$c(0,0) = 1$$
$$c(0,1) = 2$$
$$c(1,0) = 1 \rightarrow 2$$
(this got updated)
$$c(1,1) = 1$$</p>
<p>For the third input 10<strong>1</strong>011:
<code>Current symbol: 1 Previous symbol: 0</code></p>
<p>Predicted probability: $P(1|0) = \frac{c(0,1)}{c(0,1)+c(1,1)} = \frac{2}{3}$</p>
<p>Counts:
$$c(0,0) = 1$$
$$c(0,1) = 2 \rightarrow 3$$
$$c(1,0) = 2$$
$$c(1,1) = 1$$</p>
<p>and so on...</p>
<p>We observe a few things here:</p>
<ul>
<li>Over time this will learn the empirical distribution of the data</li>
<li>We initially start off with uniform distribution in this example, but we can change prior to enforce some prior knowledge [of course both the encoder and the decoder need to know!]</li>
<li>You can do this for $k=0$ (iid data with unknown distribution)!</li>
</ul>
<h3 id="kth-order-adaptive-arithmetic-coding-aac"><a class="header" href="#kth-order-adaptive-arithmetic-coding-aac">$k$th order adaptive arithmetic coding (AAC)</a></h3>
<p>Let's test this out!</p>
<pre><code class="language-py">    def freqs_current(self):
        """Calculate the current freqs. We use the past k symbols to pick out
        the corresponding frequencies for the (k+1)th.
        """
            freqs_given_context = np.ravel(self.freqs_kplus1_tuple[tuple(self.past_k)])
</code></pre>
<pre><code class="language-py">    def update_model(self, s):
        """function to update the probability model. This basically involves update the count
        for the most recently seen (k+1) tuple.

        Args:
            s (Symbol): the next symbol
        """
        # updates the model based on the new symbol
        # index self.freqs_kplus1_tuple using (past_k, s) [need to map s to index]
        self.freqs_kplus1_tuple[(*self.past_k, s)] += 1

        self.past_k = self.past_k[1:] + [s]
</code></pre>
<p>On <code>sherlock.txt</code>:</p>
<pre><code class="language-py">&gt;&gt;&gt; with open("sherlock.txt") as f:
&gt;&gt;&gt;     data = f.read()
&gt;&gt;&gt; 
&gt;&gt;&gt; data_block = DataBlock(data)
&gt;&gt;&gt; alphabet = list(data_block.get_alphabet())
&gt;&gt;&gt; aec_params = AECParams()
&gt;&gt;&gt; encoder = ArithmeticEncoder(aec_params, AdaptiveOrderKFreqModel(alphabet, k, aec_params.MAX_ALLOWED_TOTAL_FREQ))
&gt;&gt;&gt; encoded_bitarray = encoder.encode_block(data_block)
&gt;&gt;&gt; print(len(encoded_bitarray)//8) # convert to bytes
</code></pre>
<div class="table-wrapper"><table><thead><tr><th>Compressor</th><th>compressed bits/byte</th></tr></thead><tbody>
<tr><td>$0$th order</td><td>4.26</td></tr>
<tr><td>$1$st order</td><td>3.34</td></tr>
<tr><td>$2$nd order</td><td><strong>2.87</strong></td></tr>
<tr><td>$3$rd order</td><td>3.10</td></tr>
<tr><td>gzip</td><td>2.78</td></tr>
<tr><td>bzip2</td><td><strong>2.05</strong></td></tr>
</tbody></table>
</div>
<p>We observe that the compression improves as we increase the order of the model, but only up to a point. The reason is due to the counts becoming sparse as we increase the order. This is a common problem with higher order models. The issues with this can be summarized as below:</p>
<ul>
<li>slow, memory complexity grows exponentially in $k$</li>
<li>counts become very sparse for large $k$, leading to worse performance</li>
<li>unable to exploit similarities in prediction for <em>similar</em> contexts. Each $k$ context is considered independent of each other, even if they are similar and hence the probability conditioned on them is likely to be similar.</li>
</ul>
<p>These issues can be overcome with smarter modeling as discussed later. Also note that despite their performance limitations, context based models are still employed as the entropy coding stage after suitably preprocessing the data (LZ, BWT, etc.).</p>
<p>We also observe that $k$th order AAC performs close to gzip, but not as well as bzip2. In the next chapters we will understand how gzip and bzip2 are able to achieve better compression by effectively making use of larger contexts.</p>
<h3 id="two-pass-approach-and-minimum-description-length-mdl-principle"><a class="header" href="#two-pass-approach-and-minimum-description-length-mdl-principle">Two-pass approach and Minimum Description Length (MDL) principle</a></h3>
<p>What if we did a two-pass approach? In this case, we'll pay for the empirical conditional entropy for compression.</p>
<div class="table-wrapper"><table><thead><tr><th>order</th><th>adaptive</th><th>empirical conditional entropy</th></tr></thead><tbody>
<tr><td>$0$th order</td><td>4.26</td><td>4.26</td></tr>
<tr><td>$1$st order</td><td>3.34</td><td>3.27</td></tr>
<tr><td>$2$nd order</td><td>2.87</td><td>2.44</td></tr>
<tr><td>$3$rd order</td><td>3.10</td><td>1.86</td></tr>
</tbody></table>
</div>
<p>We see that there is an increasing gap between adaptive coding performance and empirical entropy as we increase the order. The reason is that we are not taking into account the model size. As the order increases, knowing the empirical distribution becomes closer to just storing the data itself in the model. At the extreme, you just have a single $|data_size|$ long context and the model is just the data itself! In practice, adaptive models are often preferred due to their simplicity and not requiring two passes over the data.</p>
<p>This can be thought of in terms of the Minimum Description Length (MDL) principle, which considers minimizing the sum of model size and compressed size given model. This is shown in the figure below. As the model complexity grows, we can compress the data better but the model takes more bits to describe. Thus, there is a tradeoff and an optimum model size.</p>
<p><img src="lossless_iid/images/mdl.png" alt="" /></p>
<h3 id="other-prediction-models-used-for-compression"><a class="header" href="#other-prediction-models-used-for-compression">Other prediction models used for compression</a></h3>
<p>We will not go very deep into the other prediction models used for compression, but will briefly mention them here.</p>
<ul>
<li>$k$th order adaptive (in SCL): https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/compressors/probability_models.py</li>
<li>Solving the sparse count problem: These try to solve the sparse count problem by not forcing a particular $k$ but instead using a mixture of models of different orders. If you have seen a longer context several times, you use that to predict, but if not you can use the shorter context based prediction. These can do pretty well and are often used for heavy-duty entropy coding.
<ul>
<li><a href="https://ieeexplore.ieee.org/document/382012">Context Tree Weighting (CTW)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Prediction_by_partial_matching">Prediction by Partial Matching (PPM)</a></li>
</ul>
</li>
<li>Advanced prediction models:
<ul>
<li>Neural net based (use a neural network as the predictor): <a href="https://bellard.org/nncp/">NNCP</a>, <a href="https://github.com/byronknoll/tensorflow-compress">Tensorflow-compress</a>, <a href="https://arxiv.org/abs/1911.03572">DZip</a></li>
<li>Ensemble methods: <a href="https://www.byronknoll.com/cmix.html">CMIX</a>, <a href="https://www.youtube.com/watch?v=NxzlrF5z5_Y&amp;ab_channel=StanfordResearchTalks">CMIX talk</a></li>
</ul>
</li>
<li>Resources: https://mattmahoney.net/dc/dce.html#Section_4</li>
</ul>
<p>These are some of the most powerful compressors around, but often too slow for many applications!</p>
<p>We can quickly look at the advanced prediction models here.</p>
<p><img src="lossless_iid/images/deepzip.png" alt="" /></p>
<p>Both DeepZip and NNCP use a neural net to predict the next character based on the context. They differ in how they train the neural net. DeepZip uses a two-pass approach, where it first trains the neural net on the data and then uses the trained model to compress the data. The trained model needs to be stored in the compressed file. NNCP on the other hand, starts with a random model (with the pseudorandom seed known to the decoder) and keeps updating the neural net as it sees more data.</p>
<p><img src="lossless_iid/images/cmix.png" alt="" /></p>
<p>CMIX uses several contexts (last byte, second-last byte, same bit in last byte, last word and so on) and maintains counts for all of them. A neural network and other techniques are then used to mix the prediction according to all the contexts and produce the final prediction. As time goes on the mixer is trained to focus more on models that predict the next symbol well.</p>
<h3 id="text-compression-over-the-years"><a class="header" href="#text-compression-over-the-years">Text compression over the years</a></h3>
<p>To wrap up the context based compression discussion, let us look at how text compression has evolved over the years. The figure below shows the compression ratio for text compression over the years. We see how CMIX and NNCP have now beaten Shannon's entropy estimates for text compression!</p>
<p><img src="lossless_iid/images/pulkit_text.png" alt="" /></p>
<h2 id="llm-based-compression---going-beyond-the-mdl-principle"><a class="header" href="#llm-based-compression---going-beyond-the-mdl-principle">LLM based compression - going beyond the MDL principle</a></h2>
<p>We now consider a different setting where the model size doesn't matter, we just want to get the best compression possible. So we just throw our most powerful predictor at the problem, a model that would take gigabytes to describe, and use that to compress the data. In a traditional setting, the size of the model would be prohibitive to store as part of the compressed file (also painstakingly slow to compress and decompress). But this can still be relevant in a few cases:</p>
<ul>
<li>For understanding limits of compressibility/entropy rate estimation</li>
<li>When there is a large amount of data of the same type and you can afford to deploy the model separately on each decompression node</li>
<li>To demonstrate concepts in a compression course!</li>
</ul>
<p>We will now look at using LLMs for compression. Note that LLMs are trained as predictors, and their loss function is simply the cross-entropy loss (or perplexity = $2^{\textrm{cross-entropy}}$). Thus they are in-effect being trained for compression! We will use <a href="https://bellard.org/ts_server/ts_zip.html">ts_zip: Text Compression using Large Language Models</a> to use LLMs for compression, specifically the rwkv_169M and rwkv_430M models.</p>
<p>Let's look at some results for a 2023 novel (848 KB English text).</p>
<p><img src="lossless_iid/images/LLM_plot.png" alt="" /></p>
<p>We see how LLM beats the other compressors by a wide margin, and how larger context helps a lot for LLMs. The bigger model does marginally better for both context lengths given that it is a more powerful predictor.</p>
<p>Now let us look at the compression ratio for an ancient Pali text (transcribed in Roman script):</p>
<div class="table-wrapper"><table><thead><tr><th>Compressor</th><th>compressed bits/byte</th></tr></thead><tbody>
<tr><td>$2$nd order AAC</td><td>2.66</td></tr>
<tr><td>gzip</td><td>2.11</td></tr>
<tr><td>bzip2</td><td>1.71</td></tr>
<tr><td>small LLM model</td><td>2.41</td></tr>
<tr><td>medium LLM model</td><td>2.19</td></tr>
</tbody></table>
</div>
<p>Why do the LLMs no longer do so well compared to bzip2? Because this is not similar to anything it saw in the training dataset. Thus, our usual compressors that don't assume much about the data (gzip, bzip2) can do much better than LLMs in this case.</p>
<p>What if we took an even more powerful model. See below the results for Llama-13B (4 bit quantized) (using code <a href="https://gist.github.com/chachachaudhary274/707eeed868167b2e8c30000d747316d9">here</a>)</p>
<div class="table-wrapper"><table><thead><tr><th>Dataset</th><th>Context length</th><th>compressed bits/byte</th></tr></thead><tbody>
<tr><td>2023 short story</td><td>10</td><td>1.228</td></tr>
<tr><td>2023 short story</td><td>50</td><td>1.027</td></tr>
<tr><td>2023 short story</td><td>512</td><td>0.874</td></tr>
</tbody></table>
</div>
<p>Again we see how this gets amazing compression, even better than the smaller models above. Larger context length also helps, although with diminishing returns beyond a point. Now let's repeat this on a Sherlock Holmes novel:</p>
<div class="table-wrapper"><table><thead><tr><th>Dataset</th><th>Context length</th><th>compressed bits/byte</th></tr></thead><tbody>
<tr><td>Sherlock</td><td>10</td><td>1.433</td></tr>
<tr><td>Sherlock</td><td>50</td><td>0.542</td></tr>
<tr><td>Sherlock</td><td>512</td><td>0.200</td></tr>
</tbody></table>
</div>
<p>This one is way too good! What's going on?</p>
<p>The reason is that the LLM is able to memorize a lot of its training dataset and thus achieve amazing compression. This is a common problem in machine learning, and therefore the evaluation needs to be done on a held-out dataset.</p>
<p>To summarize the LLM based compression results:</p>
<ul>
<li>Remarkable results, far beyond the commonly used compressors and even better than the state-of-the-art compressors like CMIX and NNCP (see the results on the <a href="lossless_iid/ts_https://bellard.org/ts_server/ts_zip.htmlzip">ts_zip</a> page)</li>
<li>Be careful about model-data mismatch (e.g., Pali text) and overfitting to training data (e.g., Sherlock)</li>
<li>Very slow and compute intensive today, but they might become practical with hardware acceleration in future (at least for some applications)</li>
<li>Resources for futher reading:
<ul>
<li>ts_zip: <a href="https://bellard.org/ts_server/ts_zip.html">https://bellard.org/ts_server/ts_zip.html</a></li>
<li>DeepMind paper: <a href="https://aps.arxiv.org/abs/2309.10668">https://aps.arxiv.org/abs/2309.10668</a></li>
</ul>
</li>
</ul>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<p>We studied context based arithmetic coding, and looked at various models ranging from simple $k$th order adaptive models to advanced neural net based models. We also looked at LLM based compression, and saw how it can achieve amazing compression shining a light on the lower bounds for compression. In the next lecture we will look at a different class of compressors: universal compressors and dive into the realm of practical compressors that are ubiquitous today.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="universal-compression-with-lz77"><a class="header" href="#universal-compression-with-lz77">Universal Compression with LZ77</a></h1>
<p>In the previous chapters we talked about entropy rate, which is the fundamental limit of lossless compression for stationary sources. Recall that entropy rate was defined as<br />
$$H(\mathbf{U}) = \lim_{n\rightarrow\infty} H(U_{n+1}|U_1,U_2,\dots,U_n) = \lim_{n\rightarrow\infty} \frac{H(U_1,U_2,\dots,U_n)}{n}$$
and we discussed special cases of entropy rate for iid and Markov sources. For iid sources, the entropy rate is just the entropy of the source, and for a ($k$th order) Markov sources, the entropy rate is the conditional entropy of the next symbol given the past $k$ symbols.</p>
<p>We also discussed context based arithmetic coding, based on the fundamental principle that a good predictor induces a good compressor. In particular, we saw that if we have a good predictor for the next symbol given the past $k$ symbols, we can use arithmetic coding to achieve the entropy rate for a $k$th order Markov source. We also looked at using LLMs as a predictor achieving incredible compression performance. These schemes can be adapted to achieve the entropy rate for a stationary source by slow growing the context size, however they face issues with efficiency.</p>
<p>Suppose you know your data is 2nd order Markov but not the transition probabilities. You can build a context-based adaptive arithmetic coder with context size 2 and achieve the entropy rate. If you knew the distribution, you don't even need the adaptive part and can just use the known transition probabilities! Now suppose you are not told anything about the source distribution. You could assume a high-order Markov model or maybe try to use LLM based compression, and might get reasonable results in practice. But wouldn't you like a compressor that gives you guaranteed optimality even with such lack of knowledge about the source. That's where universal compressors come in! For a long time, it wasn't known if such a technique even existed - and the world changed after a couple of seminal papers by Jacob Ziv and Abraham Lempel in the late 1970s.</p>
<p>In simple words, a <em>universal</em> compressor is a scheme that does <em>well</em> on <strong>any</strong> stationary input without prior knowledge of the source distribution. As part of this class of compressors, we will explore one of the most common schemes used in practical compressors.</p>
<h2 id="universal-compressor"><a class="header" href="#universal-compressor">Universal compressor</a></h2>
<p>Consider a compressor $C$ that works on arbitrary length inputs and has length function $l(x^n)$.</p>
<div id="admonition-definition-universal-compressor" class="admonition admonish-note" role="note" aria-labelledby="admonition-definition-universal-compressor-title">
<div class="admonition-title">
<div id="admonition-definition-universal-compressor-title">
<p>Definition: Universal Compressor</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/lz77.html#admonition-definition-universal-compressor"></a>
</div>
<div>
<p>Consider a compressor $C$ that works on arbitrary length inputs and has length function $l(x^n)$.
$C$ is universal if
$$\lim_{n \rightarrow \infty} \frac{1}{n}E[l(X^n)] = H(\mathbf{X})$$
for any stationary ergodic (ergodicity is a technical condition we won't talk about) source.</p>
</div>
</div>
<p>So a single compressor $C$ is asymptotically optimal for every stationary distribution without prior knowledge of the source distribution! No requirements on the Markov order or anything else.</p>
<p>Thinking in terms of universal predictors, recall from last lecture that a compressor induces a distribution via its length function: $\hat{p}(x^n) = 2^{-l(x^n)}$. Thus, a universal compressor's $\hat{p}$ approximates any stationary distribution arbitrarily closely as $n$ grows (illustrated below). In particular a universal compressor is a universal predictor!</p>
<img src="lossless_iid/images/universal_prediction.png" alt="universal_prediction" width="500"/>
<p>Obviously all this needs to be rigorously formulated, e.g., see the reference below or check out the notes from EE 376C provided in the <a href="lossless_iid/../resources.html">resources section</a>:</p>
<blockquote>
<p>M. Feder, N. Merhav and M. Gutman, "Universal prediction of individual sequences," in IEEE Transactions on Information Theory, vol. 38, no. 4, pp. 1258-1270, July 1992, doi: 10.1109/18.144706.</p>
</blockquote>
<h2 id="lempel-ziv-universal-algorithms"><a class="header" href="#lempel-ziv-universal-algorithms">Lempel-Ziv universal algorithms</a></h2>
<p>As mentioned above, for a long time, the existence of universal algorithms was unknown. Lempel and Ziv, in their seminal papers, showed that there exist universal algorithms for lossless compression. The compressors and their variants used in a variety of commonly used compressors are listed below:</p>
<ul>
<li>LZ77: in gzip, zstd, png, zip, lz4, snappy</li>
<li>LZ78: strong theoretical guarantees</li>
<li>LZW (Lempel-Ziv-Welch) (LZ78 variant): in linux compress utility, GIF</li>
<li>LZMA (Lempelâ€“Zivâ€“Markov chain algorithm) (LZ77 variant): 7-Zip, xz</li>
</ul>
<p>As you can see, LZ based techniques are used in a variety of general-purpose compressors such as zip as well as in image compressors like png and GIF. For this lecture, we'll mostly focus on LZ77 which is the most commonly used compressor in practice. You can read more about the other techniques in the references below.</p>
<h3 id="references"><a class="header" href="#references">References:</a></h3>
<ol>
<li><strong>LZ77:</strong> Ziv, Jacob, and Abraham Lempel. "A universal algorithm for sequential data compression." IEEE Transactions on information theory 23.3 (1977): 337-343.</li>
<li><strong>LZ78:</strong> Ziv, Jacob, and Abraham Lempel. "Compression of individual sequences via variable-rate coding." IEEE transactions on Information Theory 24.5 (1978): 530-536.</li>
<li><strong>LZW:</strong> Welch, Terry A. "A technique for high-performance data compression." Computer 17.06 (1984): 8-19.</li>
</ol>
<h2 id="lz77-algorithm"><a class="header" href="#lz77-algorithm">LZ77 algorithm</a></h2>
<p>The idea is simple: "history repeats itself" - if you see something in your input you've probably seen that before. So just replace repeated segments in data with pointers and lengths! Whenever you see a sequence you have seen before, you encode it in terms of when you saw it last and it's length. Consider the popular song lyrics below which are very conducive to this form of compression!</p>
<img src="lossless_iid/images/around_the_world.png" alt="around_the_world" width="300"/>
<h3 id="lz77-parsing"><a class="header" href="#lz77-parsing">LZ77 parsing</a></h3>
<p>LZ77 compression involves parsing the input text into a sequence of matches to the past. This is done by generating three streams: unmatched literals, match length and match offset. Let's try to understand the parsing through an example. Consider the input sequence:
<b>ABBABBABBCAB</b></p>
<p>We will fill in the table below, to store any unmatched portions followed by the match length (how long the match is) and the match offset (how far back the match is).</p>
<div class="table-wrapper"><table><thead><tr><th><span style="color:maroon">Unmatched literals</span></th><th>Match length</th><th>Match offset</th></tr></thead><tbody>
<tr><td>-</td><td>-</td><td>-</td></tr>
<tr><td>-</td><td>-</td><td>-</td></tr>
<tr><td>-</td><td>-</td><td>-</td></tr>
</tbody></table>
</div>
<p>We start from the left and note that the first two characters don't have a match in the past. So we store them as unmatched literals. The third character (<code>B</code>) does match the character right before. If we try for a longer match, that doesn't work because the third and fourth characters <code>BA</code> haven't been seen in the past. So we get</p>
<p><b><span style="color:maroon">A[B]</span><span style="color:blue">B</span>ABBABBCAB</b></p>
<div class="table-wrapper"><table><thead><tr><th><span style="color:maroon">Unmatched literals</span></th><th><span style="color:blue">Match length</span></th><th><span style="color:blue">Match offset</span></th></tr></thead><tbody>
<tr><td>AB</td><td>1</td><td>1</td></tr>
<tr><td>-</td><td>-</td><td>-</td></tr>
<tr><td>-</td><td>-</td><td>-</td></tr>
</tbody></table>
</div>
<p>Next we observe the fourth character <code>A</code> has a match in the past so we try to extend to a longer and longer match. We see that <code>ABB</code> matches the <code>ABB</code> seen three positions ago. Looking ahead, we see that the <code>ABB</code> actually matches the <code>ABB</code> in the current match so effectively you have a match of length 6 (<code>ABBABB</code>). This might look suspicious at first glance since our match overlaps itself. But we'll see that this is fine and we will be able to decode. So we get (note there were no unmatched literals this time around):</p>
<p><b>[ABB<span style="color:blue">ABB]ABB</span>CAB</b></p>
<div class="table-wrapper"><table><thead><tr><th><span style="color:maroon">Unmatched literals</span></th><th><span style="color:blue">Match length</span></th><th><span style="color:blue">Match offset</span></th></tr></thead><tbody>
<tr><td>AB</td><td>1</td><td>1</td></tr>
<tr><td>-</td><td>6</td><td>3</td></tr>
<tr><td>-</td><td>-</td><td>-</td></tr>
</tbody></table>
</div>
<p>And moving on in the same way, we get the full LZ77 parsing for this input as:</p>
<p><b>ABBABB[AB]B<span style="color:maroon">C</span><span style="color:blue">AB</span></b></p>
<div class="table-wrapper"><table><thead><tr><th><span style="color:maroon">Unmatched literals</span></th><th><span style="color:blue">Match length</span></th><th><span style="color:blue">Match offset</span></th></tr></thead><tbody>
<tr><td>AB</td><td>1</td><td>1</td></tr>
<tr><td>-</td><td>6</td><td>3</td></tr>
<tr><td>C</td><td>2</td><td>4</td></tr>
</tbody></table>
</div>
<p>This can be decoded in the same exact order (left as an exercise, do check we can unparse the overlapping match!). To make this a little formal, let's write out a pseudocode for the parsing and unparsing algorithms.</p>
<div id="admonition-lz77-parsing-pseudocode" class="admonition admonish-note" role="note" aria-labelledby="admonition-lz77-parsing-pseudocode-title">
<div class="admonition-title">
<div id="admonition-lz77-parsing-pseudocode-title">
<p>LZ77 parsing pseudocode</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/lz77.html#admonition-lz77-parsing-pseudocode"></a>
</div>
<div>
<pre><code>For input sequence x[0], x[1], ...

Suppose we have parsed till x[i-1].

- Try to find largest k such that for some j &lt; i
  x[j:j+k] = x[i:i+k]

- Then the match length is k and the match offset is i-j 

  [note that the ranges j:j+k and i:i+k are allowed to overlap]

- If no match found, store as literal.

</code></pre>
</div>
</div>
<div id="admonition-lz77-unparsing-pseudocode" class="admonition admonish-note" role="note" aria-labelledby="admonition-lz77-unparsing-pseudocode-title">
<div class="admonition-title">
<div id="admonition-lz77-unparsing-pseudocode-title">
<p>LZ77 unparsing pseudocode</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/lz77.html#admonition-lz77-unparsing-pseudocode"></a>
</div>
<div>
<pre><code>At each step:

- First read any literals and copy to output y.

- To decode a match with length l and offset o.
  - If l &lt; o:
    - append y[-o:-o+l] to y
  - Else:
    // Need to be more careful with overlapping matches!
    - For _ in 0:l:
      - append y[-o] to y

</code></pre>
<pre><code>The two cases are as illustrated below:

             o                           
â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º           
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 
â—„â”€â”€â”€â”€â”€â”€â”€â”€â–º                    â—„â”€â”€â”€â”€â”€â”€â”€â”€â–º 
    l                              l     
            Non-overlapping match l &lt; o  
                                         
                                         
                      o                   
                  â—„â”€â”€â”€â”€â”€â”€â–º               
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º       
                          â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
                                 l       
            Overlapping match l &gt;= o
</code></pre>
</div>
</div>
<p>Some observations:</p>
<ul>
<li>Decompression/unparsing is very fast since it just involves copying! This is a major advantage of LZ77 based techniques over context based coding methods where decompression is quite symmetric to compression due to the same predictor being used.</li>
<li>The parsing can be done in various ways while still being compatible with the unparsing algorithm. This is unlike context based coding methods where the encoding and decoding algorithms are tightly coupled. Thus you can spend more time or less time finding matches and the same decoder (unparser) will work! This makes the same format compatible with a variety of different encoder implementations. An extreme example of a parser is one that doesn't try to match anything and simply writes out everything as a literal. The other extreme is a parser that attempts to use dynamic programming to find the optimal parsing with the fewest literals (e.g., see this <a href="https://cbloomrants.blogspot.com/2008/10/10-10-08-7_10.html">blog post</a>)!</li>
<li>As we observed above, allowing overlapping matches is a parser choice that enables better compression for highly repetitive sequences. However, this requires support in the parser and unparser and some implementations might not support this. In particular other LZ variants like LZ78 do not allow overlapping matches, leading to worse performance for highly repetitive sequences since you cannot exploit such long matches.</li>
</ul>
<hr />
<div id="admonition-quiz-1-lz77-parsing-and-unparsing" class="admonition admonish-question" role="note" aria-labelledby="admonition-quiz-1-lz77-parsing-and-unparsing-title">
<div class="admonition-title">
<div id="admonition-quiz-1-lz77-parsing-and-unparsing-title">
<p>Quiz-1: LZ77 parsing and unparsing</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/lz77.html#admonition-quiz-1-lz77-parsing-and-unparsing"></a>
</div>
<div>
<p>Apply the above parsing and unparsing algorithms for the following:</p>
<ol>
<li>Parse <strong>AABBBBBBBAABBBCDCDCD</strong>.</li>
<li>Unparse the below table (note that this parsing was generated using a different parser than the one described above!):</li>
</ol>
<div class="table-wrapper"><table><thead><tr><th><span style="color:maroon">Unmatched literals</span></th><th><span style="color:blue">Match length</span></th><th><span style="color:blue">Match offset</span></th></tr></thead><tbody>
<tr><td>AABBB</td><td>4</td><td>1</td></tr>
<tr><td>-</td><td>5</td><td>9</td></tr>
<tr><td>CDCD</td><td>2</td><td>2</td></tr>
</tbody></table>
</div></div>
</div>
<h3 id="encoding-step"><a class="header" href="#encoding-step">Encoding step</a></h3>
<pre><code>                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚             â”‚      â”‚   Literals   â”‚      â”‚ Entropy   â”‚     â”‚Compressedâ”‚
â”‚Input dataâ”œâ”€â”€â”€â”€â”€â”€â”€â–ºâ”‚LZ77 parsing â”œâ”€â”€â”€â”€â”€â–ºâ”‚              â”œâ”€â”€â”€â”€â”€â–ºâ”‚  coding   â”œâ”€â”€â”€â”€â–ºâ”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚             â”‚      â”‚    Matches   â”‚      â”‚           â”‚     â”‚  File    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p>Now let's look at the end-to-end flow for LZ77 compression. As discussed above the first step is parsing which produces the literals, match lengths and match offsets. However, we still need to encode these streams into bits to get the final compressed file. Different implementations (gzip, zstd, etc.) differ in the approach, and we typically use Huffman coding/ANS with some modifications to optimize for real-life data. We will go into the details in the sections below after covering the universality proof.</p>
<h2 id="lz77-universality-proof-sketch"><a class="header" href="#lz77-universality-proof-sketch">LZ77 universality proof sketch</a></h2>
<p>Let's try to intuitively understand why LZ77 is universal. First let's understand how far apart do we expect matches to be.</p>
<div id="admonition-quiz-2-consecutive-occurences-of-symbols" class="admonition admonish-question" role="note" aria-labelledby="admonition-quiz-2-consecutive-occurences-of-symbols-title">
<div class="admonition-title">
<div id="admonition-quiz-2-consecutive-occurences-of-symbols-title">
<p>Quiz-2: Consecutive occurences of symbols</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/lz77.html#admonition-quiz-2-consecutive-occurences-of-symbols"></a>
</div>
<div>
<p>Consider an i.i.d. sequence $\dots,X_{-2},X_{-1},X_0,X_1,X_2,\dots$</p>
<p>If symbol $a$ has probability $P(a)$, what's the expected gap between consecutive occurences of $a$?</p>
<p><strong>Hint:</strong> In a block of size $n$, how many times do you expect to see $a$? What's the average spacing between the occurences?</p>
</div>
</div>
<p><strong>Answer:</strong> (using law of large numbers and the iid-ness): You expect to see $a$ around $nP(a)$ times, and the average spacing is $\frac{n}{nP(a)} = \frac{1}{P(a)}$. Recall that this is also the mean of a geometric random variable with parameter $P(a)$.</p>
<p>Surprisingly this simple formula applies more generally to $n$-tuples in a stationary ergodic process.</p>
<div id="admonition-kacs-lemma" class="admonition admonish-example" role="note" aria-labelledby="admonition-kacs-lemma-title">
<div class="admonition-title">
<div id="admonition-kacs-lemma-title">
<p>Kac's Lemma</p>
</div>
<a class="admonition-anchor-link" href="lossless_iid/lz77.html#admonition-kacs-lemma"></a>
</div>
<div>
<p>Let $\dots,X_{-2},X_{-1},X_0,X_1,X_2,\dots$ be a stationary ergodic process and let $R_n(X_0,\dots,X_{n-1})$ be the recurrence time (last time $X_0,\dots,X_{n-1}$ occurred before index $0$). Given that $(X_0,\dots,X_{n-1})=x_0^{n-1}$, we have
$$E[R_n(X_0,\dots,X_{n-1})] = \frac{1}{p(x_0^{n-1})}$$</p>
</div>
</div>
<p>In simple words, if $(X_0,\dots,X_{n-1})=x_0^{n-1}$, the same sequence $x_0^{n-1}$ also occurred roughly $\frac{1}{p(x_0^{n-1})}$ positions ago. Thus, the match offset in LZ77 is $\frac{1}{p(x_0^{n-1})}$ on average.</p>
<p>Now with this knowledge, let's try to understand how well LZ77 compresses by looking at the various components:</p>
<ol>
<li>We can encode the match offset $\frac{1}{p(x_0^{n-1})}$ using close to $\log_2 \frac{1}{p(x_0^{n-1})}$ bits using an appropriate integer coder (e.g., check out the <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/compressors/elias_delta_uint_coder.py">Elias Delta code in SCL</a>).</li>
<li>Match length and literal contribution is negligible! The lengths can be encoded in logarithmically many bits as the total sequence length, while the literals are expected to die out as we go further in the sequence (since matches get longer and longer).</li>
<li>Expending $\log_2 \frac{1}{p(x_0^{n-1})}$ bits means we are following the thumb rule, and we use on average $E[l(X^n)] \approx E [\log_2 \frac{1}{p(x_0^{n-1})}] = H(X^n)$</li>
<li>Taking this to the limit, we can see that LZ77 achieves the entropy rate!</li>
</ol>
<p>As you might have noticed, this is extremely hand-wavy and glosses over a lot of details. For a more detailed and rigorous proof, check out Cover and Thomas chapter 13, the EE 376C notes provided in the <a href="lossless_iid/../resources.html">resources section</a>, or the original paper below:</p>
<blockquote>
<p>A. D. Wyner and J. Ziv, "The sliding-window Lempel-Ziv algorithm is asymptotically optimal," in Proceedings of the IEEE, vol. 82, no. 6, pp. 872-877, June 1994, doi: 10.1109/5.286191.</p>
</blockquote>
<p>The LZ77 proof is not as satifying from a theoretical standpoint as compared to LZ78 (covered in the EE376C notes), but in practice LZ77 based techniques dominate due to their compression and computational efficiency on real-life datasets. In fact, the asymptotic theory doesn't fully explain the excellent performance in practice. If you think of a $k$th order Markov process, you expect LZ77 to do well in the limit, but not amazing for reasonable sized data due to the poor convergence rate. However, the idea of finding matches is just very well-matched to real-life data and the data is not always modeled easily as a $k$th order Markov process. For example, we see much more frequent and longer repeats in real data (e.g., English text) compared to what you'd expect from a Markov process. This often leads to LZ77 based techniques outperforming other techniques in practice for a variety of datasets.</p>
<h2 id="lz77-parsing-on-real-data---examples"><a class="header" href="#lz77-parsing-on-real-data---examples">LZ77 parsing on real data - examples</a></h2>
<p>Before we jump to more details about practical LZ77 implementations, let's gain some intuition of how matches look in practice and how the match lengths and offsets are typically distributed. We will use the <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/compressors/lz77_sliding_window.py">LZ77 implementation in SCL</a> for this purpose.</p>
<p>Consider the text of "Alice's Adventures in Wonderland" by Lewis Carroll where we see some long matches. You can imagine LZ77 getting amazing compression here by replacing this entire repeated segment with just the offset and the length which can be encoded in very few bits.</p>
<img src="lossless_iid/images/alice_long_match.png" alt="alice_long_match" width="300"/>
<p>This is not an isolated example - consider the CSS file for the bootstrap website where we again see long matches.</p>
<p><img src="lossless_iid/images/bootstrap_long_match.png" alt="w:2000" /></p>
<p>It is easy to fall under the misapprehension that only long matches help with compression, or that we need the offset to be very small to get gains. But that's not the case. Even a match of 5-10 characters with a large offset can be encoded more efficiently than storing it as a literal. This is because the space needed to store the offset and the length is logarithmic in the integer values, meaning that even offset of a million can be encoded in about 20 bits. Whereas even if you encoded a character with just 3 bits (using, e.g., Huffman coding), you would need 30 bits to encode the literal! For example, as shown below, the word <code>pleasure</code> occurs only twice in the book "Alice's Adventures in Wonderland" 150 KB apart and you would still expect it to be cheaper to encode it as a match of length 8 and offset 150 KB rather than as literals. You can also imagine there is a break-even point for this, and matches with very large offsets and/or small lengths might not help with compression.</p>
<p>First page:</p>
<pre><code>So she was considering in her own mind (as well as she could,
for the hot day made her feel very sleepy and stupid), whether
the **pleasure** of making a daisy-chain would be worth the trouble
of getting up and picking the daisies, when suddenly a White
Rabbit with pink eyes ran close by her.
</code></pre>
<p>Last page:</p>
<pre><code>, and make THEIR eyes bright and eager
with many a strange tale, perhaps even with the dream of
Wonderland of long ago:  and how she would feel with all their
simple sorrows, and find a **pleasure** in all their simple joys,
remembering her own child-life, and the happy summer days.

                            THE END
</code></pre>
<p>Let's look at the full match length and offset distributions for these two files. Note that the distribution heavily depends on the particular parsing strategy used, but we can still get some insights from these plots. As we see below, the distributions vary from file to file, and suggests that the different parsing strategies and entropy coding techniques can be adapted to the specific dataset for best performance.</p>
<p><img src="lossless_iid/images/alice_lz77.png" alt="w:2000" /></p>
<p>Looking at the distributions for the Alice in Wonderland text, we see that the match offsets largely fall in the small-ish range, but there are some large offsets as well (even as large as the file size of 150 KB). Similarly, the match lengths are mostly less than 50, though we do see some matches around length 200 as well.</p>
<p><img src="lossless_iid/images/bootstrap_css_lz77.png" alt="w:2000" /></p>
<p>For the bootstrap CSS file, we see a similar trend with most offsets being small, but some large offsets as well. The match lengths are generally larger than the Alice in Wonderland text, which makes sense since CSS files have a lot of repeated patterns (function names, variable names, etc.). Some matches are as long as 500 characters.</p>
<h2 id="practical-considerations-for-lz77-implementations"><a class="header" href="#practical-considerations-for-lz77-implementations">Practical considerations for LZ77 implementations</a></h2>
<p>The pseudocode we saw above was a very simplistic version of LZ77. In practice you would run into several issues. How much past do you need to remember? How do you actually find the matches efficiently? What entropy coders should you use for the literals, match lengths and offsets? Let's look into these and understand at least the basic considerations. Practical implementations of LZ77 like gzip and zstd include several optimizations to make them fast and efficient, which we will not cover in any detail.</p>
<h3 id="sliding-window"><a class="header" href="#sliding-window">Sliding window</a></h3>
<p>The algorithm as presented above assumes infinite past memory for both the parsing and the unparsing, since the match offsets are allowed to be arbitrarily large. This is obviously not practical for large files. In practice, we use a sliding window - only find matches in past 10s of KBs (gzip) to multiple MBs (zstd) window (with the progress of time, as memory available on machines has increased, compressors have started using larger and larger windows). The window is typically implemented using a circular buffer to efficiently handle windows without reallocation. Bigger window gives better compression but needs more memory for both compression and decompression. For a well-documented and minimal implementation, we encourage the reader to look at the <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/compressors/lz77_sliding_window.py">implementation in SCL</a>, in particular the <code>LZ77Window</code>, <code>LZ77SlidingWindowEncoder</code> and <code>LZ77SlidingWindowDecoder</code> classes.</p>
<p>The figure below shows the effect of window size for the Alice in Wonderland text. We see that increasing the window size helps with compression (leveling off above the file size as expected). Depending on the dataset, the effect can be more or less pronounced.</p>
<img src="lossless_iid/images/alice_window_size.png" alt="alice_window_size" width="600"/>
<h3 id="match-finding"><a class="header" href="#match-finding">Match finding</a></h3>
<p>The algorithm pseudocode above glossed over the details of how to find matches efficiently, just saying that "find the longest match". A brute-force approach would be to check all past positions for a match, but that would be very slow (quadratic time). The nice thing about LZ77 is that the parsing and unparsing are decoupled, so we can use any match finding strategy we want withut affecting the decoder performance or implementation.</p>
<p>The basic idea is to index past occurences of sequences (e.g., 4-length substrings) in a data structure like a hash table or a binary tree. For example, you could store that <code>ABCD</code> occurred at positions <code>[1, 10, 15]</code> while <code>ABAB</code> occurred at positions <code>[2, 6]</code>. Then for the given position where we are trying to find matches, we determine the current 4-length substring. Then we do a lookup to find previous occurences. There can be multiple places where the substring could have occurred, and we then extend each of the candidate matches to find the longest match. Using the 4-length substring allows us to efficiently index and reduce the search space, however it forces the minimum match length we would get to be at least 4. A smaller substring could give you more false positives and increase the time complexity for searching the longest match, but can give better compression in some cases.</p>
<p>In the above approach, we simply look for the longest match meaning this is a greedy strategy. But we could consider a different strategy to get longer matches. For example, we could incur a literal to find a longer match at the next position (e.g., lazy strategies don't immediately take a match, instead look ahead to find if there's a longer one). Let's consider an example for this. Suppose the next section to compress involves <code>ABCDEF</code> and you get a match in the past <code>[AB]EF</code> of length 2. A greedy strategy would take this match of length 2. But a lazy strategy could say, wait a second, if I skip this match and encode <code>A</code> as a literal, can I get a longer match starting a <code>B</code> (e.g., if [BCDEF] occurred in the past). Existing LZ77 strategies range from fast and greedy to slow and optimal (e.g., using dynamic programming to find the "optimum" parsing under some definition of optimality). The choice of match-finding strategy affects both speed and compression performance, but usually doesn't affect the decompression.</p>
<p>The <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/compressors/lz77_sliding_window.py">SCL implementation</a> allows the user to pass the match finder as a parameter to the <code>LZ77SlidingWindowEncoder</code> class. The match finder is a subclass of <code>MatchFinderBase</code> and implements the <code>find_best_match</code> and <code>extend_match</code> methods as shown below. The <code>HashBasedMatchFinder</code> class implements a hash based match finder with several parameters to control the parsing strategy as shown below, but you can implement your own match finder as well!</p>
<pre><code>class LZ77SlidingWindowEncoder:
    - match_finder: Match finder used for encoding (not required for decoding)
    - window_size: size of sliding window (maximum lookback)
</code></pre>
<pre><code>class MatchFinderBase:
      def extend_match(
        self, match_start_in_lookahead_buffer, match_pos, lookahead_buffer, left_extension=True)

      def find_best_match(self, lookahead_buffer)
</code></pre>
<pre><code>class HashBasedMatchFinder(MatchFinderBase):
    - hash_length (int): The length of byte sequences to hash.
    - hash_table_size (int): Size of the hash table.
    - max_chain_length (int): Maximum length of the chains in the hash table.
    - lazy (bool): Whether to use lazy matching where LZ77 considers one step ahead and skips a literal if it finds a longer match.
    - minimum_match_length (int): Minimum length of a match to be considered.
</code></pre>
<p>Let's look at some real data on the effect of minimum match length and lazy parsing strategies on compression performance.</p>
<img src="lossless_iid/images/minimum_match_length_2023.png" alt="minimum_match_length_2023" width="600"/>
<p>The figure above shows the effect of minimum match length on the contribution of matches and literals as well as the overall compression performance. As the match length increases, we that the literals take up more space since we are ignoring matches of smaller lengths. Whereas the matches take up less space. Overall we see there is a break-even point since having the minimum match length too small leads to poor compression (due to overhead of storing many small matches) while having it too large also leads to poor compression (due to ignoring many matches). As you might imagine the time complexity will also vary with the minimum match length since smaller lengths lead to more candidate matches to check.</p>
<img src="lossless_iid/images/lazy_greedy.png" alt="lazy_greedy" width="600"/>
<p>The figure above compares the lazy and greedy strategy at various values of minimum match length. We see the lazy strategy consistently outperforms the greedy strategy. At smaller minimum match lengths, the gap is even larger since the greedy strategy can fall in the trap of taking many small matches while the lazy strategy can skip them to find longer matches.</p>
<h3 id="entropy-coding"><a class="header" href="#entropy-coding">Entropy coding</a></h3>
<p>For entropy coding, the different streams (literals, match lengths, match offsets), a standard strategy (used in zstd but not gzip) is to concatenate all literals into a single stream and separately keep the literal counts. See below for an example.</p>
<div class="table-wrapper"><table><thead><tr><th><span style="color:maroon">Unmatched literals</span></th><th><span style="color:blue">Match length</span></th><th><span style="color:blue">Match offset</span></th></tr></thead><tbody>
<tr><td>AABBB</td><td>4</td><td>1</td></tr>
<tr><td>-</td><td>5</td><td>9</td></tr>
<tr><td>CDCD</td><td>2</td><td>2</td></tr>
</tbody></table>
</div>
<p>encoded as</p>
<pre><code>literals = AABBBCDCD
</code></pre>
<p>and</p>
<div class="table-wrapper"><table><thead><tr><th><span style="color:maroon">Literal counts</span></th><th><span style="color:blue">Match length</span></th><th><span style="color:blue">Match offset</span></th></tr></thead><tbody>
<tr><td>5</td><td>4</td><td>1</td></tr>
<tr><td>0</td><td>5</td><td>9</td></tr>
<tr><td>4</td><td>2</td><td>2</td></tr>
</tbody></table>
</div>
<p>Now each of streams can be encoded using various entropy coding approaches. In fact, you can imagine LZ77 just converting the complex stationary source into three simpler iid-like sources which can be encoded using standard entropy coding techniques discussed below. One general principle, based on experimental data, is that for literals it is often sufficient to use something like Huffman coding (which is also super-fast), while the lengths and offsets can benefit from more sophisticated coders like tANS or arithmetic coding.</p>
<ul>
<li>Huffman coding - this can be dynamic (codebook is constructed during encoding time) or static (codebook is fixed a priori based on some predefined data distribution).
<ul>
<li>gzip exclusively relies on this</li>
<li>zstd - uses Huffman only for literals</li>
</ul>
</li>
<li>tANS: zstd uses tANS for literal lengths, match lengths and match offsets</li>
<li>Context-based arithmetic coding (slower to encode and decode but better compression): LZMA, xz, 7-zip</li>
<li>For very high speeds, skip entropy coding and use fixed length codes! (LZ4, Snappy)</li>
</ul>
<h4 id="entropy-coding-of-integers"><a class="header" href="#entropy-coding-of-integers">Entropy coding of integers</a></h4>
<p>While literals are from a small alphabet (e.g., all 256 possible bytes), lengths and offsets have a large range (offsets be even be in millions). Naively applying Huffman coding or ANS on such a large alphabet is inefficient due to lack of counts (e.g., a specific match offset like 14546 might not occur many times). The typical approach is to divide the integers into bins: (i) entropy code the bin index and (ii) encode the position in bin using plain old fixed length code. The binning is often logarithmic in nature, so you could have bins like 1, 2-3, 4-7, 8-15, and so on. If you are in bin 8-15 then you use 3 bits to encode within bin position. The bin index represents the logarithm, while the position in the bin represents the lower order bits. The binning is again a design choice, although logarithmic binning tends to work decently well. One way to imagine this is as a prefix tree where you first have the bins as leaves and then attach a complete binary tree to each of these (as illustrated in the figure below).</p>
<img src="lossless_iid/images/length_offset_entropy_coding_tree.png" alt="length_offset_entropy_coding_tree" width="500"/>
<p>You can check the binning table for zstd <a href="https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md#the-codes-for-literals-lengths-match-lengths-and-offsets">here</a>. As you can see the smaller bins are divided a bit more finely (e.g., 32-39, 40-47, etc.) while the larger bins are coarser and actually logarithmic (e.g., 2048-4095, 4096-8191, etc.). This is because smaller integers occur more frequently and need to be represented more finely for better compression (since the index within bin is stored with simply fixed bitwidth all the fancy entropy coding is confined to the bin index). You can also check out <code>LogScaleBinnedIntegerEncoder</code> in the <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/5e9a699db81d7452cdf4f34b5b7023bac39f5dd5/scl/compressors/lz77.py#L213">SCL LZ77 implementation</a> for a simple reference (binning table and number of bits for offset within bin shown in table below).</p>
<div class="table-wrapper"><table><thead><tr><th>Bin range</th><th>Number of values in bin</th><th>Number of bits for offset within bin</th></tr></thead><tbody>
<tr><td>1</td><td>1</td><td>0</td></tr>
<tr><td>2-3</td><td>2</td><td>1</td></tr>
<tr><td>4-7</td><td>4</td><td>2</td></tr>
<tr><td>8-15</td><td>8</td><td>3</td></tr>
<tr><td>16-31</td><td>16</td><td>4</td></tr>
<tr><td>32-63</td><td>32</td><td>5</td></tr>
<tr><td>64-127</td><td>64</td><td>6</td></tr>
<tr><td>128-255</td><td>128</td><td>7</td></tr>
</tbody></table>
</div>
<h3 id="gzip-and-zstd"><a class="header" href="#gzip-and-zstd">gzip and zstd</a></h3>
<p>For a very long time (~mid-1990s to mid-2010s), gzip was the most commonly used general-purpose compressor. It is based on LZ77 with Huffman coding for entropy coding. However, with the advent of zstd in mid-2010s, which uses several more <a href="https://engineering.fb.com/2016/08/31/core-infra/smaller-and-faster-data-compression-with-zstandard/">modern techniques</a> (e.g., tANS entropy coding, efficient implementation using SIMD vectorization, larger windows, better match-finding strategies), zstd has started to replace gzip in many applications due to its better compression and speed. Today, except for highly resource constrained environments or legacy settings, zstd is often the default choice for general-purpose compression. Although it does consume slightly more memory than gzip, this is rarely a concern on modern machines.</p>
<p><img src="lossless_iid/images/zstd_speed.png" alt="w:1000" /></p>
<p>The figure above (<a href="https://facebook.github.io/zstd/">source</a>) compares the compression ratio, compression speed and decompression speed of zstd and gzip (labeled zlib in the figure). In the compression speed vs. ratio plot, the different "levels" (mainly determining the match-finding strategy and window size) are shown for both compressors. As seen in the figure, zstd achieves a better pareto frontier than gzip, being faster than gzip at similar compression ratios. In addition zstd achieves higher compression ratio/speed at the higher/lowest level, meaning it can be used in wider applications.</p>
<p>Looking at the decompression speed, we see that while gzip is decently fast, zstd achieves extremely high decompression speeds. This is because all aspects of zstd's design, including the format itself, are optimized for fast decompression. For example the format is constructed in a way that a decoder can avoid branching as much as possible, leading to better pipelining and vectorization opportunities in modern processors. Similarly efforts are made in the implementation to be cache friendly and use SIMD instructions to speed up operations. Overall zstd is a great example of how careful design and implementation can lead to significant performance improvements in practice. Looking at LZMA (which use context-based arithmetic coding) in the decompression speed plot, we see that it is significantly slower than both gzip and zstd, which is why it is not commonly used for general-purpose compression despite its good compression performance. Notice that the decompression speed doesn't include the levels, this is because the decompression speed is largely unaffected by the compression level since the same decoder is used for all levels.</p>
<p>The figure below (<a href="https://facebook.github.io/zstd/">source</a>) shows a wider variety of LZ77 implementations at different levels, we'll look in the next chapter on how to make informed choices between these.</p>
<img src="lossless_iid/images/lzbench.png" alt="lzbench" width="500"/>
<h2 id="summary-2"><a class="header" href="#summary-2">Summary</a></h2>
<p>In this chapter we covered the LZ77 algorithm, it's universality proof sketch and practical considerations for implementation. We saw that LZ77 based techniques are widely used in practice due to their good compression and speed performance on real-life datasets. We also saw that several design choices (window size, match-finding strategy, entropy coding approach) affect the performance of LZ77 implementations.</p>
<p>Before we wrap up, let's understand that universal doesn't mean perfect. LZ77 is universal in an asymptotic sense, but need not be the best choice for a given dataset. Even if it is ideal, there are things to consider beyond the compression rate - speed, memory usage and so on. And as discussed above choices made in LZ77 implementations play a big role in its performance.</p>
<p>That's basically it for LZ77! In the next chapter, we will wrap up lossless compression and suggest some practical tips when it comes to selecting a lossless compressor your data. A few parting notes on further reading:</p>
<ul>
<li>We didn't talk about LZ78 and LZW - similar core ideas but slightly different tree-based parsing method
<ul>
<li>For LZ78, possible to prove very powerful universality results, including non-asymptotic ones!</li>
<li>In particular can show that LZ78 gets compression rate within $O(\frac{k}{\log n}+\frac{\log\log n}{\log n})$ of the optimal $k$th order model for any sequence.</li>
<li>For a more theoretical treatment, check out the EE376C notes in the <a href="lossless_iid/../resources.html">resources section</a>.</li>
</ul>
</li>
<li>There is much more on LZ77 we didn't cover (e.g., repcodes, optimal parsing). Information about these can be found in certain blogs in the <a href="lossless_iid/../resources.html">resources section</a> or in the actual implementation/documentation of compressors like zstd.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="practical-tips-on-lossless-compression"><a class="header" href="#practical-tips-on-lossless-compression">Practical tips on lossless compression</a></h1>
<p>Over the last several chapters, we have covered the theory and algorithms behind lossless compression. We understood the fundamental limits (entropy and entropy rate), looked at various prefix-free codes and entropy coders like Huffman coding, Arithmetic coding and ANS. We also saw how to deal with non-IID sources using context-based coding and universal compression algorithms like LZ77. Each of these ideas was optimal in some sense. In this chapter, we will look into how you could use this knowledge in practice when you are faced with a real world compression problem.</p>
<p>The first question you want to ask of yourself is:</p>
<blockquote>
<p>Is compression the most pressing problem you are facing?</p>
</blockquote>
<p>Try to think about the following:</p>
<ul>
<li>Do you have a lot of data that needs to be stored/transmitted?</li>
<li>Is the cost of storage/transmission significant enough to warrant investing time and resources into compression?</li>
<li>Will compression just save on storage or will it also improve performance (e.g., faster data transfer, reduced latency)?</li>
</ul>
<p>You want to make sure you are solving the right problem! Many times compression is a cost-saving measure, but maybe your business is not at the point where this is a priority.</p>
<p>Suppose you convinced yourself that you have too much data or it is indeed becoming a bottleneck in some way. The next question you want to ask is:</p>
<blockquote>
<p>Is it possible you don't need all this data? Can you eliminate the entire data or parts of it?</p>
</blockquote>
<ul>
<li>Identify parts of data that are costing the most:
<ul>
<li>Can you get rid of it or change the representation? Do you need the text-based logs at full fidelity? Can you sample the data?
<ul>
<li>Make sure you use your compression knowledge to understand what's really contributing to the size of the data. A long string will hardly take up any space it is repeated many times (e.g., the server host name in a log that repeats for every request). Whereas a short string of random characters (e.g., unique identifiers, hashes) will be incompressible and take up space after compression.</li>
</ul>
</li>
<li>Does the identifier need to be so random? Is there a real security reason behind all the random incompressible strings in your data?</li>
</ul>
</li>
<li>Do you need to store the data for so long? Can you filter it down to fewer fields or aggregate it in some way?
<ul>
<li>Sometimes legal requirements and unavoidable to store.</li>
</ul>
</li>
<li>What is your data access pattern?
<ul>
<li>Backup/archive (cold storage) cheaper than hot storage.</li>
</ul>
</li>
</ul>
<p>Also: lossy compression is crucial for multimedia data - starting in the next chapter!</p>
<p>Remember that compression is not only about storage. It can have multiple benefits like:</p>
<ul>
<li>memory reduction</li>
<li>bandwidth reduction</li>
<li>faster querying</li>
</ul>
<p>So we reached this point where we want to compress some data and need to compress it losslessly.</p>
<blockquote>
<p>You definitely need to store this data losslessly! Now what?</p>
</blockquote>
<p>There can be so many approaches, and sadly many of the suboptimal ones are pretty commonly undertaken. Being a compression expert, it is very important to avoid the following <strong>Things not to do</strong>:</p>
<ul>
<li>Use your recently acquired compression knowledge and straightaway start spending a bunch of time developing your own compressor. Implement LZ77 and all the entropy coders on your own.</li>
<li>Search online and purchase the license to a fancy looking compression solution.</li>
<li>Use CMIX since your company deserves the best possible compression irrespective of the compute costs.</li>
<li>Use gzip because LZ77 is an universal algorithm and so it has to be the best compressor for every occasion, and use it with the default settings since that's the popular thing.</li>
<li>Make a statistically accurate model of your data, design a predictor, and then use a context-based arithmetic coder.</li>
</ul>
<p>We are not saying these approaches can never make sense, but they are not the first step!</p>
<p>A much better approach is to:</p>
<blockquote>
<p>Understand the application requirements, try existing compressors like zstd and then evaluate whether there are benefits to create a domain specific compressor based on an approximate model for the data.</p>
</blockquote>
<h3 id="understand-your-application"><a class="header" href="#understand-your-application">Understand your application</a></h3>
<ul>
<li>Resource requirements - compression/decompression speed/memory - this is the single most important factor in choosing a compressor in practice. Sometimes for write-once-read-many applications, decompression speed can be more important than compression speed. Whereas in some real-time applications, both compression and decompression speed are crucial.</li>
<li>Where will compression and decompression happen?
<ul>
<li>Closed system owned by you? Deployed by customers? Open source library? Production use vs. research?</li>
</ul>
</li>
<li>How will compression and decompression happen?
<ul>
<li>CLI? Library in some programming language?</li>
</ul>
</li>
<li>Is the data homogenous? Is there a lot of the same kind of data? For example, you could have a lot of log files of the same format, or genomic data. On the other hand, you could be running a cloud storage service where users can upload arbitrary files.</li>
</ul>
<p>All these would affect your choice of compressor (or your decision to build your own), as well as how to deploy it.</p>
<h3 id="use-compression-benchmarks"><a class="header" href="#use-compression-benchmarks">Use compression benchmarks</a></h3>
<p>Compression benchmarks are a great way to compare different compressors on a common ground. They provide standardized datasets, evaluation metrics and comparison plots. Let's look a popular benchmark available at <a href="http://quixdb.github.io/squash-benchmark/unstable/">http://quixdb.github.io/squash-benchmark/unstable/</a>. In general, you want to measure compression ratio as well as compression and decompression speed on your specific dataset or on a general corpus, and then find the most prefered operating point depending on your application requirements.</p>
<p><img src="lossless_iid/images/compression_speed_ratio_squash.png" alt="" /></p>
<p>In the plot above, you see the compression ratio vs. compression speed for some corpus (for different levels of different compressors). You can see LZ4 handily wins on compression speed, while ZPAQ provides excellent compression ratio at the cost of speed. zstd seems to provide a good trade-off between the two. If you want to get slightly better compression ratio than zstd but not be extremely slow, you can try bsc or bzip2. Some compressors like gzip are not on the pareto frontier so you should ideally never use it (unless you only have KBs of memory). As you can see such benchmarks are a great way to quickly narrow down your choices.</p>
<p><img src="lossless_iid/images/decompression_speed_ratio_squash.png" alt="" /></p>
<p>Similarly, the plot above shows compression ratio vs. decompression speed. Again, LZ4 is the fastest, while zstd provides a good trade-off. Most LZ77 based compressors (gzip, zstd, brotli) are pretty fast at decompression (since it's just entropy decoding followed by memcpy), while BWT based compressors (bzip2, bsc) are slower.</p>
<h3 id="general-rule-of-thumb"><a class="header" href="#general-rule-of-thumb">General rule of thumb</a></h3>
<p>With the above understanding, here are some rules of thumb to guide your choice of compressor:</p>
<ul>
<li>zstd has very fast decompression, and a variety of compression levels - should be the first thing to try. Don't use gzip unless you have a very good reason (e.g., your readers can't consume zstd).</li>
<li>To go even faster, try LZ4. LZ4 uses a very fast match-finder and doesn't use entropy coding making both compression and decompression fast.</li>
<li>For slower but better compression (e.g., you want to store the data for archiving, and storage cost is your primary objective), you can try out:
<ul>
<li>LZMA based compressors (LZ77+arithmetic coding) (7-zip, xz)</li>
<li>BWT based compressors (bzip2, bsc) - faster compression (than LZMA), slower decompression. We didn't cover BWT in these notes, but it is a reversible transformation that makes the data compressible by creating repetitive patterns.</li>
</ul>
</li>
<li>For even more resource intensive - try context based arithmetic coding with $k$th order, PPMd, etc. (these don't always beat LZ/BWT - check for your data!)</li>
<li>CMIX/NNCP/LLM - if you want to understand the limits, rarely useful in practice due to extreme resource requirements.</li>
</ul>
<h4 id="using-zstd-and-using-it-well"><a class="header" href="#using-zstd-and-using-it-well">Using zstd and using it well</a></h4>
<ul>
<li>Use the latest version</li>
<li>Choose the right level (1-19, also negative and ultra levels)
<ul>
<li>sometimes makes sense to use custom advanced parameters (e.g., window size, hash length). Levels are just presets for these advanced parameters, and you can set them manually if you want more control (recall it doesn't affect the decompression).</li>
</ul>
</li>
<li>Use CLI/library in language of your choice - zstd also supports multithreaded compression</li>
<li>Reuse zstd context objects if processing multiple files. There is some overhead in creating these objects and reusing them can significantly speed up compression/decompression if you are processing many files.</li>
<li>For small files, consider using dictionary compression (check out the Small data section in the <a href="https://engineering.fb.com/2016/08/31/core-infra/smaller-and-faster-data-compression-with-zstandard/">zstd blog post</a>). This is helpful when you have many small files with similar format or content. While you can't expect to find matches within a small file, you can find matches across files using a shared dictionary. zstd provides tools to build such dictionaries from sample data, and then you can use those dictionaries to compress small files as long as the same dictionary is used for decompression.</li>
</ul>
<h3 id="domain-specific-compressors"><a class="header" href="#domain-specific-compressors">Domain specific compressors</a></h3>
<p>So we've seen that general-purpose compressors, and specifically zstd, are a great starting point for lossless compression. The question arises:</p>
<blockquote>
<p>When does a domain specific compressor make sense?</p>
</blockquote>
<ul>
<li>You see noticeable gap between existing compressors and estimated entropy. Theory super useful here to meaningful estimates!</li>
<li>You can do well while being faster than existing compressors (using the structure of the data to "help out" zstd make smarter choices). An example could be that in a table you expect a column to only have matches within the column, whereas zstd would search for matches across the entire data.</li>
<li>Easier if closed ecosystem since it's easier to deploy, maintain and make sure all consumers can decompress.</li>
<li>There is lots of data of the same type (homogenous data) making investment worth it - check out <a href="https://www.youtube.com/watch?v=G5n37deW3uw">the talk by Yann Collet</a> to understand all these aspects that come after you build your compressor. Genomics is a great example of this where the vast amount of homogenous data and the huge storage costs make it worthwhile to build domain specific compressors. Other cases include a large company having there own proprietary data format that is used across the company, or a startup whose entire product is based on storing some specific kind of data (e.g., time series data).
<ul>
<li>design</li>
<li>implement</li>
<li>test</li>
<li>format and versioning (with compatibility guarantees)</li>
<li>monitoring and maintenance</li>
<li>portability and language support</li>
</ul>
</li>
</ul>
<p>The general approach to building a domain specific compressor is to parse (columnarize) your data, apply various transforms to the fields and make them compressible by general-purpose compressors like zstd. The idea is that we can make the job of zstd easier by removing redundancies and making patterns more explicit. <a href="https://arxiv.org/abs/2510.03203">OpenZL: A Graph-Based Model for Compression</a> is a great paper that describes such a general approach to building domain-specific compressors.</p>
<p>That's all on lossless compression, and we hope you found these notes useful! Next up, we will start our journey into lossy compression, starting with the basics of quantization.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lossy-data-compression"><a class="header" href="#lossy-data-compression">Lossy data compression</a></h1>
<p>The second set of lecture notes pertains to lossy data compression</p>
<ol>
<li><a href="lossy/./quant.html">Vector Quantization</a></li>
<li><a href="lossy/./rd.html">Rate-Distortion Theory</a></li>
<li><a href="lossy/./transform_coding_theory.html">Transform Coding Theory</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lossy-compression-basics-and-quantization"><a class="header" href="#lossy-compression-basics-and-quantization">Lossy Compression Basics and Quantization</a></h1>
<h2 id="recap-1"><a class="header" href="#recap-1">Recap</a></h2>
<p>In the first half of this course, we learned about lossless compression techniques and the fundamental limits imposed by entropy. We also learnt about the tradeoffs for various <em>entropy coders</em>. Here is a summary:</p>
<ul>
<li>Learnt about fundamental limits on lossless compression: entropy, $H(p)$</li>
<li>Thumb rule: $L(x) \propto \lceil log_2(1/p(x)) \rceil$</li>
<li>Learn about various lossless compressors aka <em>entropy coders</em> and their implementations
<ul>
<li>Block codes: Shannon coding and Huffman coding</li>
<li>Streaming codes: Arithmetic coding and Asymmetric Numeral Systems</li>
<li>Universal (pattern) matching codes: LZ77</li>
</ul>
</li>
<li>Learnt about how to deal with non-IID sources
<ul>
<li>Context-based coding</li>
<li>Adaptive coding</li>
</ul>
</li>
</ul>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p>However, today we want to discuss a different setting. Many real-world data sources such as audio, images, and video are continuous in nature rather than discrete. To represent these sources digitally, we need to approximate and quantize them, which inherently introduces some loss of information. Previously, we assumed a discrete information source $X$ that could be losslessly compressed by entropy coding techniques like Huffman or arithmetic coding. The discrete entropy $H(X)$ represented a hard limit on the best possible lossless compression. Let's think about a continuous source $X$ now. How much information does it contain?</p>
<p><span style="color:purple;"> Quiz-1: How much information does a continuous source <code>X</code> contain? </span></p>
<p>A continuous source contains an infinite amount of information, so it cannot be represented digitally in a lossless manner. This is in-fact related to a fundamental property about real numbers: they are uncountable, i.e., there are infinite real numbers between any two real numbers. Instead, we need to approximate it by quantizing to a discrete representation. This quantization step will inevitably induce some loss or distortion.</p>
<p>The key aspects of lossy compression are:</p>
<ul>
<li>It allows some loss of information or fidelity in order to achieve higher compression. The original source cannot be perfectly reconstructed.</li>
<li>Lossless compression is a special case of lossy compression with zero distortion.</li>
<li>Quantization is used to convert the continuous source into a discrete representation. This is a fundamental part of lossy compression.</li>
<li>Entropy coding is still widely applicable and typically used as the final step after quantization.</li>
</ul>
<h2 id="lossy-compression-basics-rate-distortion-tradeoff"><a class="header" href="#lossy-compression-basics-rate-distortion-tradeoff">Lossy Compression Basics: Rate-Distortion Tradeoff</a></h2>
<p>A continuous source contains infinite information, so we cannot represent it exactly. We need to approximate it, which implies some unavoidable loss of information.</p>
<p><strong>Distortion (D)</strong> is a quantitative measure of this loss of information introduced by the approximation/quantization process. Common distortion metrics include:</p>
<ul>
<li>Mean squared error (MSE): $D = \mathbb{E}[(X - \hat{X})^2]$</li>
<li>Mean absolute error (MAE): $D = \mathbb{E}[|X - \hat{X}|]$</li>
</ul>
<p>In lossy compression, we have a choice regarding the tradeoff between rate and distortion:
<strong>Rate (R)</strong> refers to the number of bits used per sample to represent the lossy approximation of the source.
Higher rate implies we can represent the source more accurately with lower distortion <code>D</code>.
Lower rate means we have to tolerate more distortion to achieve higher compression.</p>
<p>This inherent tradeoff between rate R and distortion D is fundamental to lossy compression and quantization. Figure below shows a cartoon of the rate-distortion tradeoff. Fundamental to this discussion is the fact that we always strive to achieve the best possible rate-distortion tradeoff, i.e.</p>
<ul>
<li>given a distortion level ($D^{*}$), we want to achieve the lowest possible rate ($min~\text{R}$)</li>
<li>given a rate ($R^{*}$), we want to achieve the lowest possible distortion ($min~\text{D}$)</li>
</ul>
<p>In next set of notes, we will learn more about the <em>rate-distortion</em> theory which provides a theoretical framework for this tradeoff.</p>
<p><img src="lossy/./images/rd1.png" alt="" />
<img src="lossy/./images/rd2_1.png" alt="" />
<img src="lossy/./images/rd2_2.png" alt="" /></p>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<p>Let's work through an example together.</p>
<p><code>Example 1</code></p>
<ul>
<li>Let's say you are measuring temperature (<code>T</code>) in a room, say in Celsius, at an hourly interval.
<ul>
<li>Remember, physical <code>T</code> is a continuous source.</li>
</ul>
</li>
<li>Say your sensor is very sensitive and it records  <code>T = [38.110001, 36.150901, 37.122020, 37.110862, 35.827111]</code></li>
</ul>
<p><span style="color:purple;"> Quiz-2: How many bits do we want to represent <code>T</code>? </span></p>
<p>It depends on the application! If we are using it to control the AC, we might need more bits than if we are using it to decide whether to wear hoodie or T-shirt. In either case,</p>
<ul>
<li>we need to decide on the <em>distortion</em> we are OK with</li>
<li>we can agree these many decimals are waste of bits</li>
</ul>
<p><span style="color:purple;"> Quiz-3: What are some reasonable values to encode? </span></p>
<p>One reasonable way to encode is to round <code>T</code> to the nearest integer, i.e., <code>T_lossy = [38, 36, 37, 37, 35]</code>. This is similar to converting <code>T</code> to <code>int</code> from <code>float</code>.</p>
<h2 id="quantization"><a class="header" href="#quantization">Quantization</a></h2>
<p>What we did in the previous example is called <em>quantization</em> (or <em>binning</em>).</p>
<ul>
<li>Quantization is the process of mapping a continuous source to a discrete source.</li>
<li>Quantization is a lossy process, i.e., it introduces distortion.</li>
<li>Quantization is a fundamental operation in lossy compression!</li>
<li>Quantized values are also sometimes called <em>symbols</em> or <em>codewords</em>, and the set of quantized values is called <em>codebook</em> or <em>dictionary</em>.
<ul>
<li>In previous example, <em>codebook</em> is <code>{35, 36, 37, 38}</code> and <em>codewords</em> for each symbol are <code>{35, 36, 37, 37, 35}</code>.</li>
</ul>
</li>
</ul>
<p><span style="color:purple;"> Quiz-4: For a codebook of size <code>N</code>, what is the rate <code>R</code>? </span></p>
<p>The rate $R = \log_2(N)$ because we have $N$ quantized symbols and we need $\log_2(N)$ bits to represent each symbol. Alternatively, we can say the quantized value for each symbol can take $2^R$ unique values.</p>
<h3 id="quantization-example---gaussian"><a class="header" href="#quantization-example---gaussian">Quantization Example - Gaussian</a></h3>
<p><code>Example 2</code>
Now say, <code>X</code> is a Gaussian source with mean 0 and variance 1, i.e., $X \sim \mathcal{N}(0, 1)$. Say we want to represent X using just 1 bit per symbol.</p>
<p><span style="color:purple;"> Quiz-5: What are some reasonable values to encode? </span></p>
<p>We can decide to convey just the sign of X, i.e., $\hat{X} = \text{sign}(X)$ as the distribution is symmetric around 0. Say we get a positive value for $\hat{X}$, what should be the <em>quantized value</em> of the recovered symbol? For this, we need to decide on the <em>distortion</em> we are OK with. For today's discussion let's say we are concerned about MSE distortion.</p>
<p><span style="color:purple;"> Quiz-6: What should be the codebook for this example? </span></p>
<p>If you have taken signal processing, you know that the conditional expectation of <code>X</code> given the observation is the best linear estimator for MSE distortion. This is called Minimal Mean Square Estimator (MMSE). Mathematically, if we want to find</p>
<p>If you have not seen this before, here are some resources:</p>
<ul>
<li><a href="https://lall.stanford.edu/engr207b/lectures/mmse_estimation_2011_02_02_01.pdf">Notes from Prof. Lall, Stanford</a></li>
<li><a href="https://probability4datascience.com/slides/Slide_8_04.pdf">Notes from Prof. Chan, Purdue</a></li>
<li><a href="https://en.wikipedia.org/wiki/Minimum_mean_square_error#:~:text=In%20statistics%20and%20signal%20processing,values%20of%20a%20dependent%20variable.">Wikipedia page</a></li>
</ul>
<p>Therefore, in our case, the codebook should be $\mathscr{C} = \left{\mathbb{E}[(X | \hat{X} &gt; 0)], \mathbb{E}[(X | \hat{X} &lt; 0)]\right}$.</p>
<p>For gaussian, this is $\mathscr{C} = \left{\sqrt{\frac{2}{\pi}}, -\sqrt{\frac{2}{\pi}}\right}$. We can work this out as follows:
$$
\begin{aligned}
\mathbb{E}[(X | \hat{X} &gt; 0)] &amp;= \int_{0}^{\infty} x \frac{2}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx \
&amp;= \sqrt{\frac{2}{\pi}} \int_{0}^{\infty} x e^{-\frac{x^2}{2}} dx \
&amp;= \sqrt{\frac{2}{\pi}} \left[-e^{-\frac{x^2}{2}}\right]_{0}^{\infty} \
&amp;= \sqrt{\frac{2}{\pi}}
\end{aligned}
$$
where the first step follows from definition of conditional expectation and symmetry of gaussian distribution around 0. Similarly, we can show that $\mathbb{E}[(X | \hat{X} &lt; 0)] = -\sqrt{\frac{2}{\pi}}$.</p>
<p><img src="lossy/./images/gaussian1.png" alt="h:400" /></p>
<h3 id="scalar-quantization"><a class="header" href="#scalar-quantization">Scalar Quantization</a></h3>
<p><img src="lossy/./images/SQ1.png" alt="bg right:50% 100%" /></p>
<p>This is an example of <em>scalar quantization</em>. In scalar quantization, we quantize each symbol independently. The figure above explains the process of scalar quantization. Quantization can be thought of as a function which maps continuous symbols $X$ to the reconstructed symbols $\hat{X}$. The quantization function is called <em>quantizer</em> and is denoted by $Q(\cdot)$. The quantizer is defined by the <em>codebook</em> $\mathscr{C}$, which is the set of quantized values. The quantizer maps each symbol $X$ to the nearest quantized value in the codebook, i.e., $\hat{X} = Q(X) = \text{argmin}_{y \in \mathscr{C}} d(X,y)$ where $d(\cdot, \cdot)$ is the distortion (or some other distance) metric. The process defines decision thresholds which are the regions where all values of $X$ are mapped to the same quantized value $\hat{X}$ in the codebook $\mathscr{C}$.</p>
<p>More formally, we can also think of a quantizer as a partition of the input space $\mathbb{R}$ into $N$ disjoint regions $S_i$ such that $\bigcup_{i=1}^N S_i = \mathbb{R}$ and $S_i \cap S_j = \emptyset$ for $i \neq j$ and a mapping from each region $S_i$ to a quantized value $y_i \in \mathscr{C}$.</p>
<h2 id="vector-quantization"><a class="header" href="#vector-quantization">Vector Quantization</a></h2>
<p>So far, we are quantizing each symbol independently. But can we do better? Maybe we can work with two (or more) symbols at a time?
Say we have $X = [X_1, X_2]$, where $X_1, X_2 \sim \mathcal{N}(0, 1)$</p>
<ul>
<li>you can also think of it as you generated <code>2*N</code> samples from $\mathcal{N}(0, 1)$ and then split them into two groups of size <code>N</code> (similar to <code>block codes</code> in lossless compression)</li>
<li>or you can think of it as you have two sensors measuring the same source</li>
<li>or you can think of it as having two sensors measuring two different sources</li>
</ul>
<p><span style="color:purple;"> Quiz-7: We want to compare it with 1 bit/symbol scalar quantization. What's the size of codebook allowed? </span></p>
<p>The size of the codebook will be $2^{{1~\text{bit}/\text{symbol}}<del>\times</del>{2~\text{symbol}/\text{code-vector}}} = 4$. Generalizing, we can have codebook of size $N=2^{R*k}$ for vectors (blocks) of size $k$ and $R$ bits/symbol. In other words, $R = (\log_2 N) / k$ bits/symbol since we are using $N$ quantized values to represent $k$ symbols.</p>
<p>More formally, generalizing what we have seen so far,</p>
<ul>
<li>A quantizer is a mapping $Q: \mathbb{R}^{k} \rightarrow \mathscr{C}$ where $\mathscr{C}=\left{\underline{y}<em>{i}\right}</em>{i=1}^{N}$ is the "codebook" or "dictionary" comprising of $N$ $k$-dimensional vectors.</li>
<li>The mapping is defined by: $Q(\underline{x})=\underline{y}<em>{i} \text { if } \underline{x} \in S</em>{i}$ where $\left{S_{i}\right}<em>{i=1}^{N}$ is a partition of $\mathbb{R}^{k}$ <br> $\bigcup</em>{i=1}^{N} S_{i}=\mathbb{R}^{k}; \quad  S_{l} \cap S_{m}=\phi, l \neq m.$</li>
<li>The rate is $R=\frac{\log N}{k} \frac{\text { bits }}{\text { sample }} ; N=2^{k R}$</li>
</ul>
<p>Vector quantization provides following benefits over scalar quantization:</p>
<ul>
<li>We can exploit dependence between vector components</li>
<li>We can have more general decision regions (than could be obtained via Scalar Quantization)</li>
</ul>
<p>The image below shows a basic example where vector quantization provides advantage over scalar quantization. Assume you have two dimensional probability density $f(X_1, X_2)$ as shown and you want to quantize it. This represents a case where both $(X_1, X_2)$ are uniformly distributed with same sign.</p>
<p>You can either quantize each dimension independently (scalar quantization) or you can quantize both dimensions together (vector quantization). The figure below shows the decision regions for both cases. We take a specific example for vector quantization as shown in the figure. For scalar quantization we have $6\times6$ codewords, i.e. $N=36$ and therefore the rate is $log_2(6^2)/2 = log_2(6)$, whereas for the vector quantizer we only need a strict subset of the scalar quantize codewords requiring half of the codewords, i.e. $N=18$ and therefore the rate is $log_2((6^2/2)/2 = log_2(6) - 1/2$. It is obvious to see here that the distortion for both these quantizers will be same but the rate for vector quantizer is lower.</p>
<p>This is also obvious intuitively! Vector quantization allows us to exploit the correlation between the two dimensions and therefore we can achieve the same distortion with lower rate. It takes advantage of the fact that both dimensions are uniformly distributed with same sign.
Therefore, vector quantization provides more flexibility in choosing the decision regions.</p>
<p><img src="lossy/./images/VQ1.png" alt="w:600" /></p>
<h3 id="some-additional-comments-on-vq"><a class="header" href="#some-additional-comments-on-vq">Some additional comments on VQ</a></h3>
<p>Optimal regions are generally not uniform (as in scalar quantization) even in simple uniform IID case! In the 2D case of uniform IID, a hexagonal lattice provides most optimal regions with respect to MSE distortion. This is called <em>lattice quantization</em> or <em>Voronoi diagram</em> and can accommodate more than 2 dimensions. In the case of uniform IID variables in 2D, $\frac{MSE_{lattice}}{MSE_{SQ}} \approx 0.962$. This conveys two points to us:</p>
<ul>
<li>VQ performs better than SQ even in simple uniform IID case. As seen previously it performs even better in correlated cases.</li>
<li>This can be a lot of effort for little gain given the source distribution, e.g. in this case we only gain 3.8% in MSE distortion.</li>
</ul>
<p>Here is a proof for the above result:</p>
<p>Scalar Quantization of each component separately would yield square decision regions of the form as shown in following figure, where $\Delta$ is the quantization step size.</p>
<p><img src="lossy/./images/VQ_square_decision_regions.png" alt="Square decision regions" title="Square decision regions" /></p>
<p>The area of the decision region is $A_1 = \Delta^2$. Therefore, since we have uniform</p>
<p>$$
\begin{aligned}
\operatorname{MSE}<em>1=\int</em>{-\frac{\Delta}{2}}^{\frac{\Delta}{2}} \int_{-\frac{\Delta}{2}}^{\frac{\Delta}{2}}\left(x^2+y^2\right) d x d y &amp;=\int_{-\frac{\Delta}{2}}^{\frac{\Delta}{2}}\left[\frac{x^3}{3}+\left.y^2 x\right|<em>{-\frac{\Delta}{2}} ^{\frac{\Delta}{2}}\right] d y
&amp;=\int</em>{-\frac{\Delta}{2}}^{\frac{\Delta}{2}}\left[\frac{\Delta^3}{12}+y^2 \Delta\right] d y
&amp;=\left[\frac{\Delta^3}{12}y+\frac{y^3}{3}\Delta\right]^{\frac{\Delta}{2}}_{\frac{-\Delta}{2}} \
&amp; =\frac{\Delta^4}{12}+\frac{\Delta^4}{12}=\frac{\Delta^4}{6}
\end{aligned}
$$</p>
<p>Alternatively, we could have chosen a vector quantizer with hexagonal decision regions as shown below:</p>
<p><img src="lossy/./images/VQ_hexagonal_decision_regions.png" alt="Hexagonal decision regions" title="Hexagonal decision regions" /></p>
<p>For this case, using symmetry, the area of the decision region is six times a triangle shown below. Therefore, $A_2 = 6 \cdot (\frac{1}{2}) \cdot (\frac{\sqrt{3}}{2} \delta) \cdot \delta  = \frac{3 \sqrt{3}}{2} \delta^2$.</p>
<p><img src="lossy/./images/VQ_hexagonal_triangle.png" alt="Hexagonal decision region dimensions" width="200"/></p>
<p>Finding the MSE is left as an exercise to the reader, but it can be shown that
$$
\operatorname{MSE}_2 = \frac{5\sqrt{3}}{8}\delta^4
$$</p>
<p>For comparing these two schemes, we will compare them for $\Delta$ and $\delta$ such that $A_1=A_2$, so that we have same rate (same number of quantization points covering the same area). This gives us the desired result
$$\frac{\operatorname{MSE}_2}{\operatorname{MSE}_1}=0.962$$</p>
<p><strong>Conslusion</strong>: Even for a simple uniform IID source, there is benefit in vector quantization over scalar quantization.</p>
<p><img src="lossy/./images/ZivScalarPaper.png" alt="w:600" /></p>
<p>The above image is a paper abstract from <a href="https://ieeexplore.ieee.org/document/1057034">On universal quantization, Ziv</a>. As you can see under certain conditions, scalar quantization can be competitive with vector quantization. However, in general, vector quantization is better than scalar quantization.</p>
<h2 id="vector-quantization-algorithm"><a class="header" href="#vector-quantization-algorithm">Vector Quantization Algorithm</a></h2>
<p>In general, optimal regions are not easy to compute, and we need to resort to iterative algorithms.
<span style="color:purple;"> Quiz-8: Have you seen this problem before in some other context? </span></p>
<p>It's same as K-means clustering algorithm in ML! Also called as <em>Lloyd-Max</em> algorithm or <em>Generalized Lloyd</em> algorithm. We want to cluster data points into <code>N</code> clusters corresponding to codebook (<code>k</code> in k-means) such that the average distortion is minimized.</p>
<div id="admonition-historical-note" class="admonition admonish-note" role="note" aria-labelledby="admonition-historical-note-title">
<div class="admonition-title">
<div id="admonition-historical-note-title">
<p>Historical Note</p>
</div>
<a class="admonition-anchor-link" href="lossy/quant.html#admonition-historical-note"></a>
</div>
<div>
<ul>
<li>First proposed by Stuart Lloyd in 1957 (motivated by audio compression) at Bell Labs</li>
<li>Was widely circulated but formally published only in 1982</li>
<li>Independently developed and published by Joel Max in 1960, therefore sometimes referred to as the Lloyd-Max algorithm</li>
<li>Generalized Lloyd specialized to squared error is the Kmeans clustering algorithm widely used in Machine Learning</li>
</ul>
</div>
</div>
<h3 id="k-means-algorithm"><a class="header" href="#k-means-algorithm">K-means Algorithm</a></h3>
<p>Given some data points, we can compute the optimal codebook and the corresponding partition of the data points. The main idea is to do each-step iteratively:</p>
<ul>
<li>Given a codebook, compute the best partition of the data points</li>
<li>Given a partition of the data points, compute the optimal codebook</li>
<li>Repeat until convergence</li>
</ul>
<p>Here is a pseudocode to illustrate the algorithm:</p>
<pre><code class="language-py">def k_means(data, k, max_iterations=100):
    centroids = initialize_centroids(data, k) # some random initialization for centroids (codebook)
    for iteration in range(max_iterations): # some convergence criteria
        # Assign data points to the nearest centroid -- this is the partition step
        clusters = assign_data_to_centroids(data, centroids)
        # Calculate new centroids -- this is the codebook update step
        new_centroids = calculate_new_centroids(data, clusters)
        # Check for convergence
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids # update centroids
    return clusters, centroids

def initialize_centroids(data, k):
    # Randomly select k data points as initial centroids
    return data[np.random.choice(len(data), k, replace=False)]

def assign_data_to_centroids(data, centroids):
    # Assign each data point to the nearest centroid
    distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)
    clusters = np.argmin(distances, axis=1)
    return clusters

def calculate_new_centroids(data, clusters):
    # Calculate new centroids as the mean of data points in each cluster
    new_centroids = np.array([data[clusters == i].mean(axis=0) for i in range(len(np.unique(clusters)))])
    return new_centroids
</code></pre>
<h2 id="more-resources"><a class="header" href="#more-resources">More resources</a></h2>
<p>Following google colab notebook contains code and additional examples for vector quantization: <a href="https://colab.research.google.com/drive/16dYjBEc499HgHoZRxcyeg0YmNAb5AwAW?usp=sharing">Notebook</a></p>
<p>We only scratched the surface of quantization. There are many more advanced topics:</p>
<ul>
<li>Constrained vector quantization</li>
<li>Predictive vector quantization</li>
<li>Trellis coded quantization</li>
<li>Generalized Lloyd algorithm</li>
</ul>
<p>For more details, see following resources:</p>
<ul>
<li><a href="https://ee.stanford.edu/~gray/shortcourse.pdf">Fundamentals of Quantization: Gray</a></li>
<li><a href="https://link.springer.com/book/10.1007/978-1-4615-3626-0">Vector Quantization and Signal Compression: Gersho, Gray</a></li>
</ul>
<h2 id="next-time"><a class="header" href="#next-time">Next time</a></h2>
<p>We will look into the question of what is the fundamental limit on lossy compression, i.e.
<strong>what is the best possible rate-distortion tradeoff?</strong></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rate-distortion-theory"><a class="header" href="#rate-distortion-theory">Rate Distortion Theory</a></h1>
<h2 id="entropy-conditional-entropy-recap"><a class="header" href="#entropy-conditional-entropy-recap">Entropy, Conditional entropy Recap:</a></h2>
<p>In case of lossless compression we saw information theoretic quantities such as entropy: $H(X)$, conditional entropy: $H(X|Y)$ etc.</p>
<p>To recap:</p>
<ol>
<li>
<p><strong>Entropy</strong>: Let $X$ be a random variable, with alphabet ${1, 2, \ldots, k}$ and discrete probability distribution $P = { p_1, p_2, \ldots, p_k}$. i.e. one can imagine the samples generate by the random variable $X$ to be independent and identically distributed as per distribution $P$.</p>
<p>Then the entropy $H(X)$ of the random variable is defined as:
$$ H(X) = \sum_{i=1}^k p_i \log_2 \frac{1}{p_i} $$</p>
</li>
<li>
<p><strong>Joint Entropy</strong>: The joint entropy of discrete random variables $X,Y$ with distribution $P(x,y)$ is simply the entropy of the joint random variable $X,Y$.</p>
</li>
</ol>
<p>$$ H(X,Y) = \sum_{x,y} p(x,y) \log_2 \frac{1}{p(x,y} $$</p>
<ol start="3">
<li><strong>Conditional Entropy/Relative Entropy</strong> The conditional entropy between two discrete random variables $X,Y$ is defined as:</li>
</ol>
<p>$$ H(Y|X) = \sum_{x,y} p(x,y) \log_2 \frac{1}{p(y|x)}$$</p>
<p>Note that $H(Y|X)$ has alternative definitions:</p>
<p>$$H(Y|X) = \sum_x P(x)H(Y|X=x)$$</p>
<p>i.e. it is the average of the entropies $H(Y|X=x)$</p>
<h2 id="mutual-information"><a class="header" href="#mutual-information">Mutual Information</a></h2>
<p>For today's discussion another important information theoretic quantity which would be interesting to us is the mutual information $I(X;Y)$</p>
<div id="admonition-mutual-information" class="admonition admonish-note" role="note" aria-labelledby="admonition-mutual-information-title">
<div class="admonition-title">
<div id="admonition-mutual-information-title">
<p>Mutual Information</p>
</div>
<a class="admonition-anchor-link" href="lossy/rd.html#admonition-mutual-information"></a>
</div>
<div>
<p>Let $X,Y$ be two random variables with joint distribution $p(x,y)$. Then we define the mutual information between $X,Y$ as:</p>
<p>$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$</p>
</div>
</div>
<p>One intuitive explanation for mututal information is that it is the difference between sum of individial entropies and the joint entropy between two random variables $X,Y$, and so in a way capture how much information is common between $X,Y$.</p>
<p>Mutual information has some nice properties, which we will use:</p>
<ol>
<li><strong>property-1: $I(X;Y) = I(Y;X)$</strong>: It is clear from the symmetry that mutual information between $X,Y$ is equal to mutual information between $Y,X$</li>
</ol>
<p>$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$</p>
<ol start="2">
<li>
<p><strong>property-2: $I(X;Y) = H(X) - H(X|Y)$</strong>:
This can be shown using the property of joint entropy: $H(U,V) = H(U) + H(V|U)$.
$$
\begin{align*}
I(X;Y) &amp;= H(X) + H(Y) - H(X,Y) \
&amp;= H(X) + H(Y) - H(Y) - H(X|Y) \
&amp;= H(X) - H(X|Y)
\end{align*}
$$</p>
</li>
<li>
<p><strong>property-3: $I(X;Y) = D_{KL}(p(x,y)||p(x)p(y))$</strong>:</p>
<p>$$
\begin{align*}
I(X;Y) &amp;= H(X) + H(Y) - H(X,Y) \
&amp;= \sum_{x} p(x) \log_2 \frac{1}{p(x)} +  \sum_{y} p(y) \log_2 \frac{1}{p(y)} + \sum_{x,y} p(x,y) \log_2 \frac{1}{p(x,y)} \
&amp;= \sum_{x,y} p(x,y) \log_2 \frac{p(x)p(y)}{p(x,y)} \
&amp;= D_{KL}(p(x,y)||p(x)p(y))
\end{align*}
$$</p>
</li>
<li>
<p><strong>property-4: $I(X;Y) \geq 0$</strong>: This follows from the non-negativity of the $D_{KL}$ and the <em>property-3</em>.</p>
</li>
</ol>
<p>Mutual Information has a very important connection to lossy compression, but even beyond that in Information theory in general. For more information look at the <a href="https://web.stanford.edu/class/ee276/files/lec/Ee276-lec9.pdf">EE276 course notes on communication capacity</a></p>
<h2 id="lossy-compression-setup"><a class="header" href="#lossy-compression-setup">Lossy Compression setup</a></h2>
<p>Let us recap the lossy compression setup, and now that we are going to discuss this in detail, let us define it more concretely.</p>
<p>Lets say we are given a sequence of random variables $X_1, X_2, \ldots, X_k$, Our goal is to encode this sequence $X_1^k$ to $n=\log_2(N)$ bits, using a <em>lossy encoder</em>. We also decode back the $N$ bits to reconstruction $Y_1, Y_2, \ldots, Y_k$, using a <em>lossy decoder</em>.</p>
<pre><code>## Encoding
X_1,X_2, ..., X_k ====&gt; [LOSSY ENCODER] ==&gt;  0100011...1 (n = log2(N) bits) 

## Decoding
0100011...1 (N bits) ======&gt; [LOSSY DECODER] ==&gt;
Y_1, Y_2, \ldots, Y_k
</code></pre>
<p>At this point, we need a few more terms to understand the performance of our <em>lossy compressor</em>:</p>
<ol>
<li><strong>Rate $R$</strong>: the compression rate $R$ is defined as, the average number of bits used per source symbols:</li>
</ol>
<p>$$ R = \frac{\log_2(N)}{k} = n/k$$</p>
<ol start="2">
<li><strong>distortion $D$</strong>: As the compression is not lossless, we also need to know how far the reconstruction $Y_1^k$ is from the input $X_1^k$.
$$ \text{distortion} \rightarrow d(X_1^k, Y_1^k)$$</li>
</ol>
<p>For simplicity lets stick to per-symbol distortion like mean square error, or hamming distortion.
Thus,</p>
<p>$$ d(X_1^k, Y_1^k) = \sum_k d(X_i, Y_i)$$</p>
<p>For example:</p>
<p>$$ \begin{align*}
\text{hamming distortion} &amp;\rightarrow d(x,y) = \mathbb{1}(x\neq y) \
\text{mse distortion} &amp;\rightarrow d(x,y) = (x-y)^2
\end{align*}
$$</p>
<ol start="3">
<li><strong>Average Distortion</strong> We mainly care about the <em>average distortion</em>:
$$ \text{expected distortion} \rightarrow \bar{D} = \mathbb{E}[d(X_1^k, Y_1^k)] $$
over the random variables $X_1^k, Y_1^k$.</li>
</ol>
<h2 id="rate-distortion-function"><a class="header" href="#rate-distortion-function">Rate-distortion function</a></h2>
<p>One interesting question to answer is:
<em>"What is the best rate R we can achieve for distortion at max D</em>"? i.e. If the target distortion of our lossy compressor is $D$, what is the best we can compress the data $X_1^k$ to? We define this optimal rate to be $R(D)$, the rate-distortion function.</p>
<p>Thus:</p>
<p>$$ R(D) \triangleq \min_{\text{all lossy compressors s.t.} \bar{D} \leq D}  R$$</p>
<p>This is the precise problem Shannon solved.</p>
<div id="admonition-shannons-lossy-compression-theorem" class="admonition admonish-note" role="note" aria-labelledby="admonition-shannons-lossy-compression-theorem-title">
<div class="admonition-title">
<div id="admonition-shannons-lossy-compression-theorem-title">
<p>Shannon's lossy compression theorem</p>
</div>
<a class="admonition-anchor-link" href="lossy/rd.html#admonition-shannons-lossy-compression-theorem"></a>
</div>
<div>
<p>Let $X_1,X_2,\ldots$ be data generated i.i.d. Then, the optimal rate $R(D)$ for a given maximum distortion $D$ is:</p>
<p>$$ R(D) = \min_{\mathbb{E}d(X,Y) \leq D} I(X;Y)$$</p>
<p>where the expectation in the minimum is over distributions $q(x,y) = p(x)q(y|x)$, where $q(y|x)$ are any arbitrary conditional distributions.</p>
</div>
</div>
<p>Here are some examples:</p>
<h3 id="example-1-rd-for-bernoulli-rv"><a class="header" href="#example-1-rd-for-bernoulli-rv">Example-1: $R(D)$ for bernoulli r.v:</a></h3>
<p>Let $X \sim Bern(0.5)$. and let $d(x,y) = \mathbb{1}(x\neq y)$ i.e the Hamming distortion. Then:</p>
<p>$$ R(D) = \begin{cases}
1 - h(D) &amp; 0 \leq D \leq 0.5\
0 &amp; D &gt; 0.5
\end{cases}
$$</p>
<p>where $h(D)$ -&gt; binary entropy function of $Bern(p) = h(p)$.</p>
<p>The $R(D)$ formula is not very intuitive. But, it is clear that:</p>
<ol>
<li>
<p>$R(D)$ decreases as $D$ increases; this should be expected as the bits required should reduce if the allowed distortion $D$ is increasing.</p>
</li>
<li>
<p>$R(D) = 0$ if $D &gt; 0.5$; This is also quite intuitve as if allowed distortion is $D &gt; 0.5$, we can always just decode all zeros $Y_1^k = 000000...00$. For all zeros, the average distortion is $0.5$.</p>
</li>
</ol>
<h3 id="example-2-rd-for-gaussian-rv"><a class="header" href="#example-2-rd-for-gaussian-rv">Example-2: $R(D)$ for gaussian r.v.:</a></h3>
<p>Let's take a look at another example: Let $X \sim \mathcal{N}(0,1)$, i.e. the data samples $X_1, X_2, \ldots$ are distributed as unit gaussians. Also, lets consider the distortion to be the mean square distortion: $d(x,y) = (x-y)^2$ i.e the mse distortion. Then:</p>
<p>$$ R(D) = \begin{cases}
\frac{1}{2} \log_2 \frac{1}{D} &amp; 0 \leq D \leq 1.0\
0 &amp; D &gt; 1
\end{cases}
$$</p>
<p>Let's try to intuitively understand why this is the case:</p>
<p><img src="https://user-images.githubusercontent.com/1708665/200032503-e0b7b247-4e4c-4ea2-844e-e7864cf35186.jpg" alt="Note Nov 3, 2022-1" /></p>
<ul>
<li>If $X_i$'s are i.i.d $\mathcal{N}(0,1)$, then it is clear that with high probability:</li>
</ul>
<p>$$ \sqrt {\sum_{i=1}^k (X_i)^2} \lesssim \sqrt{k}$$</p>
<ul>
<li>This mainly follows from the law of large numbers, and that the variance of $X_i$ is $1$. Thus, we can say that $X_1^k$ will lie in a k-dimensional sphere of radius $\sqrt{k}$, with high probability.</li>
</ul>
<p>For the remaining discussion, we can thus focus our attention on vectors $X_1^k$, only lying inside this sphere of radius $\sqrt{k}$.</p>
<ul>
<li>Now, lets say we want to cover this k-dimensional sphere of radius $\sqrt{k}$ with k-dimensional spheres of radius $\sqrt{Dk}$. How many spheres do we need?</li>
</ul>
<p>We can approximate this number based on the volumes of the spheres:</p>
<p>$$
\begin{align*}
N &amp;\geq \frac{Volume(\sqrt{k})}{Volume(\sqrt{Dk})} \
&amp;= \sqrt{1/D}^k
\end{align*}
$$</p>
<p>Although this approximation might feel very loose, as the dimension $k$ increases, it can be shown that the number of spheres of radius $\sqrt{Dk}$ required to cover the sphere of radius $\sqrt{k}$, is indeed approximately equal to:</p>
<p>$$ N \approx \left(\frac{1}{D}\right)^{k/2} $$</p>
<ul>
<li>
<p>Note that we can use these $N$ spheres of radius $\sqrt{Dk}$, as centroids for vector quantization, as we saw in the last lecture, and we would get a distortion of at max $D$, as the squared distance between any point in the $\sqrt{k}$ sized circle is at max $D$ with one of the $N$ centroids.</p>
</li>
<li>
<p>Thus our $R(D)$, the rate for distortion at maximum $D$ is:</p>
</li>
</ul>
<p>$$
\begin{align*}
R(D) \leq \frac{\log_2 N}{k} \approx \frac{1}{2} \log_2 \frac{1}{D}
\end{align*}
$$</p>
<p>Hope this hand-wavey "proof" gives an intuition for the $R(D)$ function for unit gaussian. The proof logic can however be made more precise.</p>
<p>NOTE: Similar idea proof holds for general distributions, using typical sequences balls. We won't be able to go much into the details of the Shannon's lossy compression theorem in the course unfortunately, but here are lecture notes in case you are interested: <a href="https://web.stanford.edu/class/ee376a/files/2017-18/lecture_12.pdf">EE376a Lossy compression notes</a></p>
<p>We can also experiment with this idea, here is the R-D curve for unit gaussian and the practical performance in $k=2$. We see that the R-D performance even with $k=2$ is quite reasonable.</p>
<p><img src="https://user-images.githubusercontent.com/1708665/200065773-94db5436-f2d4-4422-b0ee-4c7ab673731c.png" alt="RD" /></p>
<p>We can also see the convergence as $k$ increases:</p>
<pre><code class="language-py">## Rate: 1, optimal_mse: 0.25
k: 1, N: 2,   Rate: 1, mse_loss: 0.37
k: 2, N: 4,   Rate: 1, mse_loss: 0.36
k: 4, N: 16,  Rate: 1, mse_loss: 0.33
k: 8, N: 256, Rate: 1, mse_loss: 0.29
...
</code></pre>
<h2 id="achieving-the-rd-in-general"><a class="header" href="#achieving-the-rd-in-general">Achieving the R(D) in general</a></h2>
<p>We saw briefly how we can achieve the $R(D)$ optimal function using vector quantization for data distributed as i.i.d unit gaussians.</p>
<p>The Broad idea of using vector quantization can be actually shown to asymptotically optimal for any data distribution. i.e. as the dimension of data $k$ increases, using vector quantization, we can achieve optimal $R(D)$ performance.</p>
<p>Although the convergence w.r.t $k$ can be slow. In the next lecture we will see how we can accelerate this convergence.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lossy-compression-theory"><a class="header" href="#lossy-compression-theory">Lossy compression theory</a></h1>
<p>We continue with lossy compression and rate distortion function (Shannon's RD theory) and applications in current technologies.</p>
<p>We'll start with things that seem unrelated but we'll bring it all together towards the end. We'll only touch on some of the topics, but you can learn more in the references listed below and in EE 276.</p>
<p>We'll start with the rate distortion function and see how it carries over to sources with memory. We'll also look into Gaussian sources with memory. Finally, we'll look at the implications to transform coding which is commonly used today.</p>
<h2 id="reminder-from-linear-algebra"><a class="header" href="#reminder-from-linear-algebra">Reminder from linear algebra:</a></h2>
<p>Consider $Y = A X$ (matrix vector product)</p>
<p>Then the square of the Euclidean norm (sum of square of components), also denoting the energy in the signal is
$$||Y||^2 = Y^T Y = X^T A^T A X$$</p>
<p>In particular, if $U$ is a unitary transformation, i.e., all rows and columns orthonormal vectors $U^T U = U U^T = I$, then we have
$$Y = U X =&gt; ||Y||^2 = ||X||^2$$
This is called the Parseval's theorem which you might have seen for Fourier transform. In words, this says that the energy in transform domain matches the energy in the original.</p>
<p>If $Y_1 = U X_1$ and $Y_2 = U X_2$, then $||Y_1 - Y_2||^2 = ||X_1-X_2||^2$. That is to say, unitary transformation preserves Euclidean distances between points.</p>
<h2 id="lossy-compression-recap"><a class="header" href="#lossy-compression-recap">Lossy compression recap</a></h2>
<p><img src="lossy/images/lossy_block_diagram.png" alt="" /></p>
<p>Recall the setting of lossy compression where the information is lossily compressed into an index (equivalently a bit stream representing the index). The decoder attempts to produce a reconstruction of the original information.</p>
<p>The two metrics for lossy compression are:</p>
<ul>
<li>$rate = {{log N} \over k}$ bits/source component</li>
<li>distortion $d(X^k, \hat{X}^k) = {1 \over k} \sum_{i=1}^k d(X_i,\hat{X}_i)$ [single letter distortion - distortion between k-tuples defined in terms of distortion between components]</li>
</ul>
<h2 id="transform-coding"><a class="header" href="#transform-coding">Transform coding</a></h2>
<p><strong>Notation:</strong> we denote $X^k = (X_1,\dots,X_k)$ as $\underline{X}$ which can be thought of as a column vector.</p>
<p><img src="lossy/images/transform_block_diagram.png" alt="" /></p>
<p>Here we simply work with an arbitrary transform $T$, with the only requirement being that $T$ is invertible and we are able to efficiently compute $T$ and $T^{-1}$. In this framework, we simply apply our usual lossy encoding in the transform domain rather than in the original domain.</p>
<p>In particular, when $T (X) = U X$ for some some unitary $U$ (e.g., Fourier transform, wavelet transform). Then
$$||Y-\hat{Y}||^2 = ||X-\hat{X}||^2 $$
This corresponds to the squared-error distortion. Any lossy compression you do on $Y$, you get the same square error distortion for the original sequence $X$ as for the $Y$.</p>
<p>Why work in the transform domain? Often in the transform domain, data is simpler to model, e.g., we can construct transform in a way that the statistics of $Y$ are simpler or we get sparsity. Then we can appropriately design lossy compressor to exploit the structure</p>
<p>Nowadays people even go beyond linear transforms, e.g., learnt transforms using deep learning models.
Can even go to a vector in a smaller dimensional space, e.g., in VAE based lossy encoders. This can allow doing very simple forms of lossy compression in the transform domain.</p>
<h2 id="shannons-theorem-recap"><a class="header" href="#shannons-theorem-recap">Shannon's theorem recap</a></h2>
<p>For "memoryless sources" ($X_i$ are iid ~$X$),</p>
<p>$$R(D) = min_{E[d(X,\hat{X})] &lt;= D} I(X; \hat{X})$$</p>
<p>We sometimes write $R(X,D)$ to represent this quantity $R(D)$ when we want to be explicit about the source in question.</p>
<h3 id="beyond-memoryless-sources"><a class="header" href="#beyond-memoryless-sources">Beyond memoryless sources</a></h3>
<p>Consider source $X^n$, reconstruction $\hat{X}^n$. Then,</p>
<p>$$R(X^n, D) = min_{E[d(X^n, \hat{X}^n)] \leq D} {1\over n} I(X^n; \hat{X}^n)$$</p>
<p>Just like $R(X,D)$ was the analog of entropy of $X$, $R(X^n, D)$ is the analog of entropy of the n-tuple.</p>
<p>Now assume we are working with a process $\mathbf{X} = X_1,X_2,X_3,...$ which is stationary. Then we can define $R(\mathbf{X}, D) = \lim_{n\rightarrow \infty} R(X^n, D)$. Similar to our study of entropy rate, we can show that this limit exists.</p>
<p>Shannon's theorem for lossy compression carries over to generality. That is, the best you can do for stationary processes in the limit of encoding arbitrarily many symbols in a block is $R(\mathbf{X}, D)$.</p>
<h2 id="rate-distortion-for-gaussian-sources"><a class="header" href="#rate-distortion-for-gaussian-sources">Rate distortion for Gaussian sources</a></h2>
<p><strong>Note:</strong> For the remainder of this discussion, we'll stick to square error distortion.</p>
<p>Why work with Gaussian sources? It is a good worst case assumption if you only know the first and second order statistics about your source. This holds both for estimation and lossy compression.</p>
<p>For $X ~ N(0,\sigma^2)$, denote $R(X,D)$ by $R_G(\sigma^2, D)$.</p>
<p>Recall from last lecture that $R_G(\sigma^2, D) = {1\over 2} \log {\sigma^2 \over D}$ for $D &lt; \sigma^2$ (above it is just $0$).</p>
<p>We can compactly write this as $R_G(\sigma^2, D) = [1/2 \log {\sigma^2 \over D}]<em>+$, where $[x]</em>+ = max{0,x}$. This is shown in the figure below.</p>
<p><img src="lossy/images/gaussian_rd.png" alt="" /></p>
<p>Similarly for $X_1 \sim N(0, \sigma_1^2)$, $X_2 \sim N(0, \sigma_2^2)$ independent, denote $R(X^2, D)$ by $R_G\left(\begin{bmatrix}\sigma_1^2\ \sigma_2^2\end{bmatrix} , D\right)$.</p>
<p>It can be shown that
$$R_G\left(\begin{bmatrix}\sigma_1^2\ \sigma_2^2\end{bmatrix} , D\right) = min_{{1\over 2} (D_1+D_2)\leq D} {1\over 2} [R_G(\sigma_1^2, D_1) + R_G(\sigma_1^2, D_2)]$$
Another way to write this is
$$R_G\left(\begin{bmatrix}\sigma_1^2\ \sigma_2^2\end{bmatrix} , D\right) =min_{\frac{1}{2} (D_1+D_2)\leq D}\  {1\over 2} \left[({1\over 2} \log \frac{\sigma_1^2}{D_1})<em>+ + ({1\over 2} \log \frac{\sigma_2^2}{D_2})</em>+\right]$$</p>
<p>Intuition: the result is actually quite simple - the solution is just greedily optimizing the $X_1$ and $X_2$ case (decoupled), and finding the optimal splitting of the distortion between $X_1$ and $X_2$.</p>
<p>Using convex optimization we can show that the minimum is achieved by a reverse water filling scheme, which is expressed in equation as follows:</p>
<p>For a given parameter $\theta$, a point on the optimal rate distortion curve is achieved by setting</p>
<ul>
<li>$D_i = \min {\theta, \sigma_i^2}$ for $i = 1,2$</li>
<li>$D = {1\over 2} (D_1+D_2)$</li>
</ul>
<p>And the rate given by
$${1\over 2} \left[({1\over 2} \log \frac{\sigma_1^2}{D_1})<em>+ + ({1\over 2} \log \frac{\sigma_2^2}{D_2})</em>+\right]$$</p>
<p>This can be expressed in figures as follows (assuming without loss of generality that $\sigma_1^2 &lt; \sigma_2^2$):</p>
<p>When $D$ is smaller than both $\sigma_1^2$ and $\sigma_2^2$, we choose both $D_1$ and $D_2$ to be equal to $D$ ($\theta=D$ in this case). We assign equal distortion to the two components, and higher rate for the component with higher variance.</p>
<p><img src="lossy/images/water_filling_1.png" alt="" /></p>
<p>When $D$ exceeds $\sigma_1^2$ but is below $\frac{1}{2}(\sigma_1^2 + \sigma_2^2)$, we set $D_1$ to be $\sigma_1^2$, and choose $D_2$ such that the average distortion is $D$. The idea is that setting $D_1$ higher than $\sigma_1^2$ doesn't make sense since the rate is already $0$ for that component.</p>
<p><img src="lossy/images/water_filling_2.png" alt="" /></p>
<p>When $D$ is equal to $\frac{1}{2}(\sigma_1^2 + \sigma_2^2)$ we can just set $D_1 = \sigma_1^2$ and $D_2 = \sigma_2^2$. Here the rate is $0$ for both components!</p>
<p><img src="lossy/images/water_filling_3.png" alt="" /></p>
<p>This generalizes beyond $2$ components. For $X_1, X_2, ... ,X_n$ independent with $X_i \sim N(0,\sigma_i^2)$, we define $R_G(\underline{\sigma^2}, D)$ analogously, and can very similarly show that $$R_G(\underline{\sigma^2}, D) = min_{{1\over n} \sum D_i \leq D} {1\over n} \left[{1\over 2} log {\sigma^2 \over D_i} \right]_+$$.</p>
<p>Similar to before, the minimum is given by $D_\theta = {1 \over n} \sum_{i=1}^n \min {\theta, \sigma_i^2}$, $R_\theta = {1 \over n} \sum_{i=1}^n [{1\over 2} \log {\sigma_i^2\over D_i}]$.</p>
<h2 id="rate-distortion-for-stationary-gaussian-source"><a class="header" href="#rate-distortion-for-stationary-gaussian-source">Rate-distortion for stationary Gaussian source</a></h2>
<p>Going back to a process $X^n$ zero mean Gaussian, then for any unitary transformation $U$ if $Y^n =U X^n$ then we can show $R(X^n, D) = R(Y^n, D)$ [since the distortion is the same in both domains]. Recall that by using the transformation it's possible to go from a scheme for compressing $X^n$ to a scheme for compressing $Y^n$ (and vice versa) without any change in the distortion.</p>
<p>Therefore we can take the diagonalizing unitary matrix which converts $X^n$ to a $Y^n$ such that $Y^n$ has independent components. The variances of $Y^n$ will be the eigenvalues of the covariance matrix.</p>
<p>Thus, we have</p>
<p>$$R(X^n, D) = R_G ((\lambda_1, \dots, \lambda_n), D)$$
where the $\lambda_i$'s are the eigenvalues of the covariance matix of $X^n$.</p>
<p>When $X^n$ are the first $n$ components of a stationary Gaussian process $\mathbf{X}$ with covariance matrix $\Phi_n = {\phi_{|i-j|}}$ for $1\leq i \leq n$ and $1\leq j \leq n$, with $\phi_{k} = Cov(X_i,X_{i-k})$. Then we have
$$R(X^n, D) = R_G(\overrightarrow{\lambda}^n,D)$$
where $\overrightarrow{\lambda}^n$ is the vector of eigenvalues of $\Phi_n$.</p>
<p>Now, we use a theorem to show a profound result for Gaussian processes.</p>
<blockquote>
<p><strong>Theorem (Toeplitz distribution)</strong>
Let $S(\omega) = \sum_{k=-\infty}^{\infty} \phi_k e^{-j\omega k}$ be the spectral density of $\mathbf{X}$ and $G$ be a continuous function. Then
$$\lim_{n\rightarrow \infty} \frac{1}{n}\sum_{i=1}^n G(\lambda_i^{(n)}) = \frac{1}{2\pi}\int_{-\pi}^{\pi} G(S(\omega) d\omega$$</p>
</blockquote>
<p>Specializing this theorem to $G(\lambda)= \min{\theta,\lambda}$ and to $G(\lambda)= \left[\frac{1}{2}\log\frac{\lambda}{\theta}\right]_+$, we get</p>
<blockquote>
<p>The rate distortion function of a stationary Gaussian process with spectral density $S(\omega)$ is given parametrically by
$$D_{\theta} = \frac{1}{2\pi}\int_{-\pi}^{\pi} \min{\theta,S(\omega)} d\omega$$
$$R_{\theta} = \frac{1}{4\pi}\int_{-\pi}^{\pi} \left[\log\frac{S(\omega)}{\theta}\right]_+ d\omega$$</p>
</blockquote>
<p>This is shown in the figure below, suggesting that the reverse water-filling idea extends to Gaussian processes once we transform it to the continuous spectral domain! This gives us motivation for using working in the Fourier transform domain!</p>
<p><img src="lossy/images/gaussian_process_water_filling.png" alt="" /></p>
<p>Finally, for $D \leq \min_{\omega} S(\omega)$, we can show that $$R(D) = \left[\frac{1}{2} \log \frac{\sigma^2}{D}\right]$$
where $\sigma^2$ is the variance of the innovations of $\mathbf{X}$. This can be used to justify predictive coding ideas.</p>
<h2 id="reference"><a class="header" href="#reference">Reference</a></h2>
<p>For more details on this, you can read the survey paper "Lossy source coding" by Berger and Gibson available at <a href="https://ieeexplore.ieee.org/document/720552">https://ieeexplore.ieee.org/document/720552</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="resources"><a class="header" href="#resources">Resources</a></h1>
<p>Interested in data compression? Great! We list a few resources (apart from the lecture notes) which might be useful to take a look.</p>
<p>NOTE: If you find a resource which you found useful and is not listed here, please file an github issue at https://github.com/stanfordDataCompressionClass/notes.</p>
<h2 id="1-software"><a class="header" href="#1-software">1. Software</a></h2>
<p>A great way to learn about data compression is to play around with the compression algorithms themselves.</p>
<ol>
<li><a href="https://github.com/kedartatwawadi/stanford_compression_library">Stanford compression Library</a>: A library of compression algorithms implemented in native python for accessibility and educational purpose. Take a look at the tutorials (in progress)  to get started! We will also be using the library as a basis for the course assignments.</li>
<li><a href="https://ccrma.stanford.edu/events/python-programs-and-book-building-audio-coder-and-deep-learning-audio">Audio Compression code/book</a> A python based implementation of audio compression, in form of tutorials.</li>
<li><a href="https://github.com/leandromoreira/digital_video_introduction">Introduction to Digital Video compression</a>: Great hands-on tutorial on digital video coding.</li>
<li><a href="https://github.com/balbekov/PyH264">H264 in Python</a>: Experimental implementation of H264 in pure Python</li>
</ol>
<h2 id="2-video-resources"><a class="header" href="#2-video-resources">2. Video Resources</a></h2>
<p>Interested in data compression? Here are some nice youtube videos to watch to get an introduction. Most of them are beginner-friendly and are useful to get a sense of the data compression.</p>
<p>Here are a set of videos to watch at leisure to get an intuition for compression:</p>
<ol>
<li><a href="https://www.youtube.com/watch?v=Eb7rzMxHyOk&amp;list=PLOU2XLYxmsIJGErt5rrCqaSGTMyyqNt2H&amp;ab_channel=GoogleDevelopers">Compressor Head playlist</a>: This is a series of short talks by a google engineer. Very good introductory videos</li>
<li><a href="https://youtu.be/JsTptu56GM8">Huffman Coding (Tom Scott)</a>: I love Tom Scott and how well he explains things. Here is a nice video on Huffman coding</li>
<li><a href="https://youtu.be/r6Rp-uo6HmI">Why Snow looks terrible on video</a>, <a href="https://youtu.be/h9j89L8eQQk">Why Night scenes look bad on video</a>: Very nice two videos by Tom Scott again which gives a brief peek into video compression algorithms</li>
<li><a href="https://youtu.be/EFUYNoFRHQI">PNG Image Compression</a>: video on PNG lossless compression</li>
<li><a href="https://youtu.be/0me3guauqOU">JPEG compression</a>: A bit more detailed video on JPEG compression</li>
<li><a href="https://youtube.com/playlist?list=PLE125425EC837021F">Arithmetic coding series</a>: Great sequence of lectures Information theory in general and particularly on Arithmetic coding. One of the best I found on this subject.</li>
<li><a href="https://www.youtube.com/watch?v=G5n37deW3uw">Designing Data Compression Solutions</a>: Talk by Yann Collet (author of zstd and lz4) what comes next once you have your great compression idea.</li>
</ol>
<h2 id="3-textbooks-research-papers"><a class="header" href="#3-textbooks-research-papers">3. Textbooks, research papers</a></h2>
<p>Although there are no textbooks which exactly correspond to the course material, but here are few books which might be relevant.</p>
<ol>
<li><a href="http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf">Elements of Information Theory</a>: Classic textbook on Information Theory</li>
<li><a href="https://www.cs.cmu.edu/~guyb/realworld/compression.pdf">Introduction to Data Compression, Guy E. Blelloch</a>: Set of lecture notes/book on Data Compression by Guy E. Blelloch, CMU 2013</li>
<li><a href="http://mattmahoney.net/dc/dce.html">Data Compression Explained, Matt Mahoney</a>: A nice set of lecture notes on Data Compression by Matt Mahoney.</li>
<li><a href="https://web.stanford.edu/class/ee398a/handouts/papers/Wallace%20-%20JPEG%20-%201992.pdf">JPEG paper</a>: Summary of JPEG by Gregory K. Wallace</li>
<li><a href="https://arxiv.org/abs/2510.03203">OpenZL: A Graph-Based Model for Compression</a>: General approach to building domain-specific compressors</li>
</ol>
<h2 id="4-blogswebsites"><a class="header" href="#4-blogswebsites">4. Blogs/websites</a></h2>
<p>The web is filled with great engineers and researchers writing blogs related to Data Compression. We list a few below:</p>
<ol>
<li><a href="https://colah.github.io/posts/2015-09-Visual-Information/">Christopher Olah's Blog</a>: A fun visual introduction to Information Theory.</li>
<li><a href="http://fastcompression.blogspot.com/">Yann Collet's Blog</a>: Great blog by Yann Collet on implemeting fast and efficient data compression algorithms</li>
<li><a href="https://fgiesen.wordpress.com/category/compression/">Fabian Giesen's blog</a>: Blog by Fabian Giesen. (The parts related to rANS implementation are particularly great!)</li>
<li><a href="https://www.euccas.me/zlib/">On understanding zlib</a>: Introduction to zlib by Euccas Chen.</li>
<li><a href="https://glinscott.github.io/lz/index.html#toc4">Modern LZ Compression</a>: Walk-through of modern LZ compressors by Gary Linscott.</li>
<li><a href="http://weitz.de/dct/">DCT</a>: A nice blog post on DCT by Prof. Dr. Edmund Weitz</li>
<li><a href="https://www.cse.iitd.ac.in/~pkalra/col783-2017/DCT-History.pdf">DCT History</a>: A small red on the history of DCT by Nasir Ahmed.</li>
<li><a href="./coinflipext.pdf">Tossing a Biased Coin</a>: An accessible exploration of how to generate fair randomness from a biased coin by Michael Mitzenmacher.</li>
</ol>
<h2 id="5-course-notes-from-ee-376c-stanford"><a class="header" href="#5-course-notes-from-ee-376c-stanford">5. Course notes from EE 376C (Stanford)</a></h2>
<p>The notes from EE376C are provided below, covering universal techniques in lossless and lossy compression, as well as applications in prediction and denoising. Students can refer to these notes for a deeper understanding of universal compression techniques including the proofs of universality.</p>
<ol>
<li><a href="ee376c_notes/Part1_LZ.pdf">Part 1: Lempel-Ziv Compression</a></li>
<li><a href="ee376c_notes/Part2_CTW.pdf">Part 2: Context-Tree Weighting</a></li>
<li><a href="ee376c_notes/Part3_Denoising.pdf">Part 3: Universal Denoising</a></li>
<li><a href="ee376c_notes/Part4_Lossy_compression.pdf">Part 4: Universal Lossy Compression</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="homeworks"><a class="header" href="#homeworks">Homeworks</a></h1>
<p>Please find here the homework problems for the latest iteration of <a href="https://stanforddatacompressionclass.github.io/">the course</a>!</p>
<ol>
<li><a href="homeworks/./HW1.html">HW1</a>: Due Oct 14, midnight (11:59 PM) <a href="homeworks/./HW1_sol.html">[HW1 Solution]</a></li>
<li><a href="homeworks/./HW2.html">HW2</a>: Due Oct 28, midnight (11:59 PM) <a href="homeworks/./HW2_sol.html">[HW2 Solution]</a></li>
<li><a href="homeworks/./HW3.html">HW3</a>: Due Nov 11, midnight (11:59 PM) <a href="homeworks/./HW3_sol.html">[HW3 Solution]</a></li>
<li><a href="homeworks/./HW4.html">HW4</a>: Due Dec 2, midnight (11:59 PM)</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ee274-fall-25-homework-1"><a class="header" href="#ee274-fall-25-homework-1">EE274 (Fall 25): Homework-1</a></h1>
<ul>
<li><strong>Focus area:</strong> Prefix-free codes, Basic Information theory</li>
<li><strong>Due Date:</strong> Oct 14, midnight (11:59 PM)</li>
<li><strong>Weightage:</strong> 15%</li>
<li><strong>Total Points:</strong> 135 (Written 100 + Code 35)</li>
<li><strong>Submission Instructions:</strong>
<ul>
<li>Provided at the end of HW (ensure you read these!)</li>
<li>We will be using both manual and auto-grading for the assignment submissions. Please follow the submission instructions carefully to ensure you get full credit for your submissions.</li>
</ul>
</li>
<li><strong>Submission link:</strong>
<ul>
<li>For written part: <a href="https://www.gradescope.com/courses/1140353/assignments/6877003">HW1-Written</a> (100 points)</li>
<li>For programming part: <a href="https://www.gradescope.com/courses/1140353/assignments/6862937">HW1-Code</a> (35 points)</li>
</ul>
</li>
<li>Please post any questions on Ed, or feel free to contact the instructors during the office hours. We will be supporting Linux and Mac OS.
<ul>
<li>We strongly recommend Windows users to use <strong>Windows Subsystem Linux (WSL)</strong>. Refer to <a href="https://learn.microsoft.com/en-us/windows/wsl/setup/environment">this instruction</a> for setup. Follow the steps up to <em>Update and upgrade packages</em>. If you are using VS Code, continue with <em>Set up your favorite code editor - Use Visual Studio Code</em>.</li>
</ul>
</li>
</ul>
<p><strong>NOTE:</strong> The homework looks longer that it actually is! We try to be verbose with the questions for clarity.</p>
<h3 id="0-python-scl-setup-10-points"><a class="header" href="#0-python-scl-setup-10-points">0. Python, SCL setup (<em>10 points</em>)</a></h3>
<p>We will be using python as the programming language during the course for the assignments and for code demonstrations during the course. If you are not familiar with python, please take a look at the <a href="https://cs231n.github.io/python-numpy-tutorial/">python tutorial</a> contributed by our friends from the CS231n course.</p>
<p>Usually it is more convenient and safer to install python packages in a <em>virtual environment</em>. We will be using conda to do this</p>
<ol>
<li>
<p>Install miniconda (https://docs.conda.io/en/latest/miniconda.html). Please follow the instructions to install miniconda on your machine</p>
</li>
<li>
<p>Create a virtual conda env using the python version <code>3.9</code> (higher versions should be also ok)</p>
<pre><code class="language-sh"># create a new conda environment
conda create --name ee274_env python=3.9

# activate the new conda environment
conda activate ee274_env
</code></pre>
<p>Once you complete this step, you should see a prompt such as:</p>
<pre><code class="language-sh">(ee274_env) âžœ   
</code></pre>
<p>This indicates that you are now in a virtual environment we created, which uses python 3.9. The virtualenv is useful to install packages etc. in an isolated environment without corrupting the rest of the machine. More details: https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/</p>
<p>Please run the following command to check the python version installed. <strong>Submit the output of this command in gradescope for credit.</strong></p>
<pre><code class="language-sh">(ee274_env) âžœ python --version
</code></pre>
</li>
<li>
<p>For most of the course assignments and in-class demonstrations, we will be using the <a href="https://github.com/kedartatwawadi/stanford_compression_library">Stanford Compression Library</a> (SCL).</p>
<p>Please clone the library as follows:</p>
<pre><code class="language-sh">git clone https://github.com/kedartatwawadi/stanford_compression_library.git
cd stanford_compression_library
</code></pre>
</li>
<li>
<p>Install the library. This installs the SCL library in an editable way so you do not need to reinstall when you modify any files. This also installs any dependencies of SCL.</p>
<pre><code class="language-sh">pip install -e .
</code></pre>
<p>Once this is done let us ensure the library is installed correctly by running unit tests. <strong>Submit the output of this command in gradescope for credit.</strong> The results may include both <em>passed</em> and <em>xfailed</em> tests, but there should be no <em>failed</em> tests if you run the command on the <code>main</code> branch.</p>
<pre><code class="language-sh">find scl -name "*.py" -exec py.test -s -v {} +
</code></pre>
</li>
</ol>
<p><em>NOTE:</em> We use the <a href="https://docs.pytest.org/en/7.1.x/">pytest</a> framework to run tests in SCL. Each file of SCL also contains the tests for the block of code implemented in the file. The test functions are named as <code>def test_xxx(...)</code>. For example, the file <code>core/data_block.py</code> contains a class <code>DataBlock</code> and a test <code>test_data_block_basic_ops()</code> to test its operations. Usually these tests are a great way to understand how the block of code in the file works. You can run individual tests as:</p>
<pre><code class="language-sh">py.test -s -v scl/core/data_block.py
</code></pre>
<p>Feel free to look at different compressors in the <code>compressors/</code> folder, and the tests to get yourself familiar with them!</p>
<ol start="6">
<li>We will be using <code>EE274_Fall25/HWs</code> branch of <code>SCL</code> for releasing HWs. Checkout to the branch for getting access to HW problem templates.
<pre><code class="language-sh">git checkout EE274_Fall25/HWs
</code></pre>
</li>
</ol>
<p>You should see a <code>scl/HWs/HW1</code> folder containing template files for the rest of the assignment. Going ahead, we will refer to files relative to <code>scl</code> folder in your <code>stanford_compression_library</code> folder. For example, <code>HWs/HW1/hw1_p1.py</code> refers to the file <code>stanford_compression_library/scl/HWs/HW1/hw1_p1.py</code>.</p>
<p>For more details about SCL and to get comfortable with it, we also created a SCL tutorial. You can find the SCL tutorial linked <a href="https://stanforddatacompressionclass.github.io/notes/scl_tutorial/SCL_tutorial.html">here</a>.</p>
<h3 id="1-probability-and-coin-toss-20-points"><a class="header" href="#1-probability-and-coin-toss-20-points">1. Probability and Coin Toss (<em>20 points</em>)</a></h3>
<p>Kedar is a big <a href="https://en.wikipedia.org/wiki/Cricket">Cricket</a> fan. In cricket (like many other sports), there is a coin toss in the beginning to determine which team gets the first chance to bat. However, Kedar thinks that the coin being used is <em>biased</em> and lands as <em>Heads</em> (<code>H</code>) much more often than <em>tails</em> (<code>T</code>). Let the probability of the coin landing on <code>H</code> be $p$.</p>
<p>Shubham suggests to Kedar that this problem can be solved (i.e. we can get a <em>fair</em> coin) by tossing the coin twice! If we get a <code>H,T</code>sequence in the two tosses, then we mark it as a heads, while if we get a <code>T,H</code> sequence we can mark this as a tails. If we get a <code>H,H</code> or <code>T,T</code> then discard the result and start over!</p>
<ol>
<li>
<p>[5 points] Do you think Shubham's solution is right? Please justify.</p>
</li>
<li>
<p>[5 points] What is the expected number of coin flips needed to get one <code>H</code> or <code>T</code> outcome using Shubham's method?</p>
</li>
<li>
<p>[5 points] Kedar is still not convinced and thinks that we should really implement this idea to see if it works. Please help out Shubham by implementing the function <code>shubhams_fair_coin_generator()</code> in the file <code>HWs/HW1/hw1_p1.py</code>.
NOTE: please run all of this in the <code>(ee274_env)</code> environment which you created and that you are in the correct branch of the <code>SCL</code> repo (<code>EE274_Fall23/HWs</code>). Ensure that the tests except those in <code>HWs</code> folder pass before you start implementing the function.</p>
</li>
<li>
<p>[5 points] It is always a good practice to write tests to ensure the implementation is correct (even if you think theoretically it is alright :)). Please implement <code>test_shubhams_fair_coin_generator()</code> function in <code>HWs/HW1/hw1_p1.py</code>. You can test your code by running the following command in <code>stanford_compression_library/scl</code> folder:</p>
<pre><code class="language-sh">py.test -s -v HWs/HW1/hw1_p1.py
</code></pre>
</li>
<li>
<p>(NOT GRADED, THINK FOR FUN!): Do you think Shubham's solution is <em>optimal</em>? For example, Shubham is using at least <code>2</code> biased coin tosses to generate <code>1</code> fair coin toss. This seems wasteful in some cases (for example, if the coin is almost unbiased). Can you improve upon the <em>average</em> number of biased coin tosses needed to generate one fair coin toss?</p>
</li>
</ol>
<h3 id="q2-basic-information-theory--20-points"><a class="header" href="#q2-basic-information-theory--20-points">Q2: Basic Information theory  (<em>20 points</em>)</a></h3>
<ol>
<li>
<p>[5 points] Let $X$ be a random variable over positive integers with a distribution $$P_X(X=k) = 2^{-k}, k \geq 1$$
Compute the entropy $H(X)$ of random variable $X$. What are the optimal code-lengths for a prefix-free code designed for distribution $P_X$?</p>
</li>
<li>
<p>[5 points] Provide an optimal prefix-free code design for the distribution $P_X$.</p>
</li>
<li>
<p>[10 points] Let $Y$ be a random variable over positive integers such that $\mathbb{E}(Y) = 2$.
Then show that:$$ H(Y) \leq 2 $$
For what distribution of the random variable $Y$ does the equality hold, i.e. $H(Y) = 2$?</p>
<p><strong>HINT:</strong> KL-Divergence and it's properties will be helpful to show this.</p>
</li>
</ol>
<h3 id="q3-uniquely-decodable-codes-25-points"><a class="header" href="#q3-uniquely-decodable-codes-25-points">Q3. Uniquely decodable codes (<em>25 points</em>)</a></h3>
<p>We mainly explored prefix-free codes in class. But there is a much broader class of codes, the uniquely decodable codes. A code is uniquely decodable if no two sequences (of length 1 or more) of input symbols (say $x^n$ and $y^m$) can produce the same encoding, and hence one can uniquely determine the input sequence from the encoding. In this question, we will show that even with the uniquely decodable codes, we cannot do better than the Entropy.</p>
<ol>
<li>[5 points] Consider the code <code>C1</code> below. Is <code>C1</code> uniquely decodable? Implement a decoder for the code below. Briefly justify your decoding procedure.
<pre><code>A -&gt; 10
B -&gt; 00
C -&gt; 11
D -&gt; 110
</code></pre>
</li>
</ol>
<p>Consider an alphabet $\mathcal{U}$ and a uniquely decodable code with code lengths for $u \in \mathcal{U}$ denoted by $L(u)$. Also, we denote the codelength of the n-tuple $u^n$ as $L(u^n) = \sum_{i=1}^n L(u_i)$.</p>
<p><strong>HINT</strong>: For parts <code>Q3.2</code> and <code>Q3.3</code>, you might find it easier to gain intuition by starting with small values of $|\mathcal{U}|$ and $n$ (e.g., binary alphabet with $n=1$ or $2$) and manually expanding the summations.</p>
<ol start="2">
<li>
<p>[5 points] Show that $$\left( \sum_{u \in \mathcal{U}} 2^{-L(u)} \right)^n = \sum_{u^n \in \mathcal{U}^n} 2^{-L(u^n)} $$</p>
</li>
<li>
<p>[5 points] Let $l_{max} = \max_{u \in \mathcal{U}} L(u)$. Then we can rewrite the summation as: $$\sum_{u^n \in \mathcal{U}^n} 2^{-L(u^n)} = \sum_{j=1}^{n \cdot l_{max}} |{u^n| L(u^n) = j}| \cdot 2^{-j}$$</p>
<p>NOTE: Rewriting summation is a common proof trick, and is a useful one to watch out for!
Using <code>Q3.2</code> and the identity above, show that:</p>
<p>$$\left( \sum_{u \in \mathcal{U}} 2^{-L(u)} \right)^n \leq n\cdot l_{max}$$</p>
</li>
<li>
<p>[5 points] Using <code>Q3.3</code> show that uniquely decodable codes satisfy Kraft's inequality i.e.</p>
</li>
</ol>
<p>$$\left( \sum_{u\in\mathcal{U}} 2^{-L(u)} \right) \leq 1$$</p>
<ol start="5">
<li>[5 points] Now show that for any uniquely decodable code for $U$ with codeword lengths $L(U)$, $$\mathbb{E}[L(U)] \geq H(U) .$$
<strong>NOTE:</strong> We saw a similar proof for prefix-free codes in the class!<br>
<br>
Also argue that for any uniquely decodable code, it is possible to construct a prefix-free code with the same codeword lengths.</li>
</ol>
<p>This means that uniquely decodable codes generally do not provide any benefits over prefix-free codes and instead have a more complicated decoding procedure!</p>
<h3 id="q4-shannon-codes-25-points"><a class="header" href="#q4-shannon-codes-25-points">Q4: Shannon Code(s) (<em>25 points</em>)</a></h3>
<p>In class, we saw one version of the Shannon codes; the <a href="https://stanforddatacompressionclass.github.io/notes/lossless_iid/prefix_free_codes.html#designing-prefix-free-codes">tree-based construction</a>.</p>
<ol>
<li>
<p>[5 points]  Let's call it the <code>ShannonTreeEncoder</code> and the <code>ShannonTreeDecoder</code>. Manually calculate what the codewords should be for the following distributions:</p>
<pre><code class="language-python">ProbabilityDist({"A": 0.25, "B": 0.25, "C": 0.25, "D": 0.25})
ProbabilityDist({"A": 0.5, "B": 0.25, "C": 0.12, "D": 0.13})
ProbabilityDist({"A": 0.9, "B": 0.1})
</code></pre>
</li>
<li>
<p>[10 points] Complete the code for the <code>ShannonTreeEncoder</code> in <code>hw1_p4.py</code>. Also, complete the <code>test_shannon_tree_coding_specific_case()</code> to check the correctness of your codewords in part 1 against your implemented code. If you encounter errors, you should identify the failing test case and fix any issues. Note: we will be using automated tests to grade your code, so it is important that you do not change the function signatures. We will have more test cases for grading, so feel free to add more test cases of your own of your own when working on the problem.</p>
</li>
</ol>
<p>We have also studied the tree-parsing based decoding for Shannon codes (or prefix-free codes in general). This decoding has already been implemented in the <code>ShannonTreeDecoder</code>. The core tree-parsing logic can be found in <a href="https://stanforddatacompressionclass.github.io/notes/lossless_iid/prefix_free_codes.html">here</a>. The tree-parsing based decoder works well however includes too many if/else conditions which leads to branching -- this is pretty bad for modern hardware. (<a href="https://stackoverflow.com/questions/9820319/why-is-a-cpu-branch-instruction-slow#:~:text=A%20branch%20instruction%20is%20not,sequential%20instructions%20being%20executed%20simultaneously">Here</a> is some additional information about this inefficiency for those interested.)</p>
<p>In this next subproblem, we will implement another decoder which we call the <code>ShannonTableDecoder</code>. The <code>ShannonTableDecoder</code> works by creating a <em>decoding lookup table</em> and then using it for decoding symbols without branching.</p>
<p>As an example, lets consider the following simple code:</p>
<pre><code class="language-python">encoding_table = {"A": 0, "B": 10, "C": 110, "D": 111}
</code></pre>
<p>In this case, the <code>ShannonTableDecoder</code> keeps track of data using following tables:</p>
<pre><code class="language-python">decoding_table = {
    000: "A",
    001: "A",
    010: "A",
    011: "A",
    100: "B",
    101: "B",
    110: "C",
    111: "D",
}
codelen_table = {
    "A": 1,
    "B": 2,
    "C": 3,
    "D": 3,
}
max_codelen = 3
</code></pre>
<p>The tables can be then used for decoding as per the pseudo-code below.</p>
<pre><code class="language-python">def decode_symbol_table(encoded_bitarray):
    state = encoded_bitarray[:max_codelen] 
    decoded_symbol = decoding_table[str(state)]
    num_bits_consumed = codelen_table[decoded_symbol]
    return decoded_symbol, num_bits_consumed
</code></pre>
<ol start="3">
<li>
<p>[5 points] Based on the pseudo-code above, explain in a few sentences how the table based decoding works. Why do you need to output both the decoded symbol and the number of bits consumed?</p>
</li>
<li>
<p>[5 points] Complete the <code>ShannonTableDecoder</code> in <code>hw1_p4.py</code> by implementing the <code>create_decoding_table</code> function. You can verify if your implementation is correct by using <code>test_shannon_table_coding_end_to_end</code> function.</p>
</li>
</ol>
<h3 id="q5-huffman-coding-on-real-data-30-points"><a class="header" href="#q5-huffman-coding-on-real-data-30-points">Q5. Huffman coding on real data (<em>30 points</em>)</a></h3>
<p>In class, we have studied compression for sources where we knew the distribution of data beforehand. In most real-life scenarios, this is not the case. Often times, we use the empirical distribution of the data and design a code for this distribution. Then we encode the data with this distribution. For the decoding to work, we also need to transmit the distribution or the code in some way. This problem focuses on compression of a provided file with Huffman coding.</p>
<p>Before we start with the code, let's look at how important it is to transmit the distribution faithfully to achieve lossless compression.</p>
<ol>
<li>
<p>[10 points] Consider the probability distribution represented as a mapping from a symbol to its probability: <code>{A: 0.11, B: 0.09, C: 0.09, D: 0.71}</code>.</p>
<p>a. What are the codeword lengths for the symbols in the Huffman code?</p>
<p>After a harrowing cab experience in the US, Pulkit wants to send a codeword <code>BADCAB</code> to Shubham to describe his frustration. Shubham and Pulkit had agreed to use Huffman encoding for such communications in the past. He encodes the codeword <code>BADCAB</code> using a Huffman code for the original distribution <code>{A: 0.11, B: 0.09, C: 0.09, D: 0.71}</code>, and sends both the codeword and this distribution  to Shubham (so that Shubham is able to decode it on his end by building a Huffman tree on his own).</p>
<p>b. Unfortunately,  Shubham decodes the message to be <code>CADBAC</code> instead of <code>BADCAB</code> making him confused. What might have gone wrong during this communication between the two?</p>
<p><strong>HINT:</strong> notice the specific encoded and decoded symbols in the message.</p>
<p>Pulkit-Shubham realized this issue and fixed it.</p>
<p>c. In the spirit of frugality, Pulkit decides to transmit a rounded distribution with just one significant decimal (<code>{A: 0.1, B: 0.1, C: 0.1, D: 0.7}</code>). What are the codeword lengths for the symbols in the Huffman code for this distribution? Are there multiple possible Huffman code lengths? Why?</p>
<p>These issues suggest that it is important to transmit the distribution faithfully to achieve lossless compression. In addition, the encoder and the decoder must share the same deterministic algorithm to construct the Huffman tree from the distribution.</p>
</li>
</ol>
<p>With this context, look at the provided starter code in <code>hw1_p5.py</code> which does the following:</p>
<ol>
<li>Loop over the file in blocks of size 50 KB.</li>
<li>For each block, compute the empirical distribution from the data. This simply means we get the counts of each possible byte value ${0, 1, 2, \ldots, 255}$ in the block and normalize by the block length to get an approximate empirical probability distribution.</li>
<li>Apply Huffman encoding on the block assuming this empirical distribution.</li>
<li>Encode the empirical probability distribution in bits so that decoder knows what probability distribution to use for decoding.</li>
<li>Final encoding is the encoding from step 4 concatenated with the encoding from step 3.</li>
</ol>
<p>To understand the overall process, you should go through the provided code and specifically the <code>encode_block</code> and <code>decode_block</code> functions.</p>
<ol start="2">
<li>[10 points] Implement the encoding and decoding of probability distribution step above (Step 4) in the functions <code>encode_prob_dist</code> &amp; <code>decode_prob_dist</code> respectively. There are multiple ways to encode this, and we are not looking for the most optimized implementation here. Note that we are working with 50 KB blocks here, while the alphabet size is just 256. So as long as your implementation is not absurdly bad, the overhead will be small. You might find the following pointers useful as you implement the function:
<ul>
<li>The input <code>bitarray</code> to <code>decode_prob_dist</code> can include more than the encoding of the probability distribution itself. Thus, it should only read as much as it needs and return the number of bits read so that the Huffman decoding can start from the next bit.</li>
<li>Python dictionaries are <a href="https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-3-6">OrderedDicts</a>. If you are using dictionaries to represent probability distribution, then it is critical to maintain this ordering while creating Huffman trees during encoding/decoding. See the Pulkit-Shubham communication fiasco above for an example of what can go wrong if the order is not preserved.</li>
<li>Python's <code>float</code> type is equivalent to the <code>double</code> type in C (see <a href="https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex">this</a> for a refresher). As long as you provide the same exact distribution to the encoder and decoder, all is well!
<ul>
<li><strong>IMPORTANT</strong>: Unless you are feeling particularly adventurous, we strongly recommend using the provided helper functions <code>float_to_bitarray64</code> and <code>bitarray64_to_float</code> for conversion between <code>float</code> and <code>BitArray</code>.</li>
</ul>
</li>
<li>Also, you will hear about <a href="https://en.wikipedia.org/wiki/Canonical_Huffman_code">Canonical Huffman code</a> in class, which provides an alternative to solve some of these issues in practice. You do not need it for this question.</li>
</ul>
</li>
</ol>
<p>Verify that your implementation is correct by running
<code>    py.test -s -v HWs/HW1/hw1_p5.py    </code>
If you encounter errors, you should identify the failing test case and fix any issues. Note: we will be using automated tests to grade your code, so it is important that you do not change the function signatures and that your code passes these tests. We will have more test cases for grading, so feel free to add more test cases of your own when working on the problem. But these should be sufficient to get you started.</p>
<ol start="3">
<li>
<p>[5 points] Now download the Sherlock Holmes novel "The Hound of the Baskervilles by Arthur Conan Doyle" using the following command.</p>
<pre><code class="language-sh">curl -o HWs/HW1/sherlock.txt https://www.gutenberg.org/files/2852/2852-0.txt
</code></pre>
<p>Run the following commands to compress the file using your newly developed compressor, decompress it back, and verify that the decompression succeeds.</p>
<pre><code class="language-sh">python HWs/HW1/hw1_p5.py -i HWs/HW1/sherlock.txt -o HWs/HW1/sherlock.txt.huffz
python HWs/HW1/hw1_p5.py -d -i HWs/HW1/sherlock.txt.huffz -o HWs/HW1/sherlock.txt.decompressed
cmp HWs/HW1/sherlock.txt HWs/HW1/sherlock.txt.decompressed
</code></pre>
<p>Nothing should be printed on the terminal as a result of the last command if the files match.</p>
<ul>
<li>Report the size of your compressed file. You can run <code>wc -c HWs/HW1/sherlock.txt.huffz</code> to print the size.</li>
<li>How much is the overhead due to storing the probability distribution? <strong>Note:</strong> One easy way to check this to replace <code>encode_prob_dist</code> with a version that just returns <code>BitArray()</code>. Obviously the decoding won't work with this change!</li>
<li>Print the obtained huffman tree on sherlock.txt by using the <code>print_huffman_tree</code> function (commented in <code>encode_block</code> function of <code>HuffmanEmpiricalEncoder</code>). What do you observe? Does the printed Huffman tree make sense? Why or why not?</li>
</ul>
</li>
<li>
<p>[3 points] What happens if you repeatedly use this compressor on the file (<code>sherlock.txt -&gt; sherlock.txt.huffz -&gt; sherlock.txt.huffz.huffz -&gt; ...</code>)? Does the file size keep getting smaller? Why or why not?</p>
</li>
<li>
<p>[2 points] Now run gzip on the file (command: <code>gzip &lt; HWs/HW1/sherlock.txt &gt; HWs/HW1/sherlock.txt.gz</code>) and report the size of the gzip file (<code>sherlock.txt.gz</code>).</p>
</li>
</ol>
<p>You will observe that gzip does significantly better than the Huffman coding, even if we ignore the overhead from part 3. While gzip uses Huffman coding internally, it also relies on other ideas that we will learn about in the coming lectures.</p>
<h3 id="q6-hw1-feedback-5-points"><a class="header" href="#q6-hw1-feedback-5-points">Q6: HW1 Feedback <em>(5 points)</em></a></h3>
<p>Please answer the following questions, so that we can adjust the difficulty/nature of the problems for the next HWs.</p>
<ol>
<li>How much time did you spent on the HW in total?</li>
<li>Which question(s) did you enjoy the most?</li>
<li>Are the programming components in the HWs helping you understand the concepts better?</li>
<li>Did the HW1 questions complement the lectures?</li>
<li>Any other comments?</li>
</ol>
<h3 id="submission-instructions"><a class="header" href="#submission-instructions">Submission Instructions</a></h3>
<p>Please submit both the written part and your code on Gradescope in their respective submission links. <strong>We will be using both autograder and manual code evaluation for evaluating the coding parts of the assignments.</strong> You can see the scores of the autograded part of the submissions immediately. For code submission ensure following steps are followed for autograder to work correctly:</p>
<ul>
<li>
<p>You only need to submit the modified files as mentioned in the problem statement. For example, for problem 1, you only need to submit the modified <code>HWs/HW1/hw1_p1.py</code> file; for problem 4, you only need to submit the modified <code>HWs/HW1/hw1_p4.py</code> file; and so-on.</p>
</li>
<li>
<p>Compress the <code>HW1</code> folder into a zip file. One way to obtain this zip file is by running the following zip command in the <code>HWs</code> folder, i.e.</p>
<pre><code class="language-sh">cd HWs
zip -r HW1.zip HW1
</code></pre>
<p>Note: To avoid autograder errors, ensure that the directory structure is maintained and that you have compressed <code>HW1</code> folder containing the relevant files and not <code>HWs</code> folder, or files inside or something else. Ensure the name of the files inside the folder are exactly as provided in the starter code, i.e. <code>hw1_p4.py</code>, <code>hw1_p5.py</code> etc. In summary, your zip file should be uncompressed to following directory structure (<strong>with same names</strong>):</p>
<pre><code>HW1
â”œâ”€â”€ hw1_p1.py
â”œâ”€â”€ hw1_p4.py
â””â”€â”€ hw1_p5.py
</code></pre>
</li>
<li>
<p>Submit the zip file (<code>HW1.zip</code> obtained above) on Gradescope Programming Part Submission Link. Ensure that the autograded parts runs and give you correct scores.</p>
</li>
</ul>
<p><strong>Before submitting the programming part on Gradescope, we strongly recommend ensuring that the code runs correctly locally.</strong></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ee274-fall-25-homework-1-solution"><a class="header" href="#ee274-fall-25-homework-1-solution">EE274 (Fall 25): Homework-1 Solution</a></h1>
<ul>
<li><strong>Focus area:</strong> Prefix-free codes, Basic Information theory</li>
<li><strong>Due Date:</strong> Oct 14, midnight (11:59 PM)</li>
<li><strong>Weightage:</strong> 15%</li>
<li><strong>Total Points:</strong> 135</li>
</ul>
<h3 id="0-python-scl-setup-10-points-1"><a class="header" href="#0-python-scl-setup-10-points-1">0. Python, SCL setup (<em>10 points</em>)</a></h3>
<ol>
<li>
<p><strong>Output of Command in 1.2</strong></p>
<p>Your output to the command</p>
<pre><code class="language-sh">python --version
</code></pre>
<p>should look something like</p>
<pre><code class="language-sh"> Python 3.9.18
</code></pre>
</li>
<li>
<p><strong>Output of Command in 1.5</strong></p>
<p>Your output to the command</p>
<pre><code class="language-sh">find scl -name "*.py" -exec py.test -s -v {} +
</code></pre>
<p>should look something like</p>
<pre><code class="language-sh">================================================================= test session starts =================================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0 -- /home/jiwon/miniconda3/envs/ee274_env/bin/python3.9
cachedir: .pytest_cache
rootdir: /home/jiwon/stanford_compression_library
collected 57 items                                                                                                                                    

scl/utils/bitarray_utils.py::test_basic_bitarray_operations PASSED
scl/utils/bitarray_utils.py::test_get_bit_width PASSED
scl/utils/bitarray_utils.py::test_bitarray_to_int PASSED
scl/utils/bitarray_utils.py::test_float_to_bitarrays PASSED
scl/core/prob_dist.py::ProbabilityDistTest::test_creation_entropy PASSED
scl/core/prob_dist.py::ProbabilityDistTest::test_prob_creation_and_validation PASSED
scl/core/prob_dist.py::ProbabilityDistTest::test_sorted_prob_dist PASSED
scl/core/prob_dist.py::ProbabilityDistTest::test_validation_failure XFAIL
scl/core/data_stream.py::test_list_data_stream PASSED
scl/core/data_stream.py::test_file_data_stream PASSED
scl/core/data_stream.py::test_uint8_file_data_stream PASSED
scl/core/encoded_stream.py::test_padder PASSED
scl/core/encoded_stream.py::test_header PASSED
scl/core/encoded_stream.py::test_encoded_block_reader_writer PASSED
scl/core/data_block.py::test_data_block_basic_ops PASSED
scl/external_compressors/zlib_external.py::test_zlib_encode_decode PASSED
scl/external_compressors/zlib_external.py::test_zlib_file_encode_decode PASSED
scl/external_compressors/pickle_external.py::test_pickle_data_compressor PASSED
scl/external_compressors/zstd_external.py::test_zstd_encode_decode PASSED
scl/external_compressors/zstd_external.py::test_zstd_file_encode_decode PASSED
scl/compressors/tANS.py::test_generated_lookup_tables PASSED
scl/compressors/tANS.py::test_check_encoded_bitarray PASSED
scl/compressors/tANS.py::test_tANS_coding tANS coding: avg_log_prob=1.499, tANS codelen: 1.502
tANS coding: avg_log_prob=0.815, tANS codelen: 0.819
tANS coding: avg_log_prob=1.422, tANS codelen: 1.427
PASSED
scl/compressors/shannon_coder.py::test_shannon_coding PASSED
scl/compressors/typical_set_coder.py::test_is_typical PASSED
scl/compressors/typical_set_coder.py::test_typical_set_coder_roundtrip PASSED
scl/compressors/fixed_bitwidth_compressor.py::test_alphabet_encode_decode PASSED
scl/compressors/fixed_bitwidth_compressor.py::test_fixed_bitwidth_encode_decode PASSED
scl/compressors/fixed_bitwidth_compressor.py::test_text_fixed_bitwidth_file_encode_decode PASSED
scl/compressors/universal_uint_coder.py::test_universal_uint_encode_decode PASSED
scl/compressors/universal_uint_coder.py::test_universal_uint_encode PASSED
scl/compressors/lz77_sliding_window.py::LZ77WindowTest::test_LZ77Window PASSED
scl/compressors/lz77_sliding_window.py::LZ77WindowTest::test_LZ77Window_get_byte_out_of_range_too_big XFAIL
scl/compressors/lz77_sliding_window.py::LZ77WindowTest::test_LZ77Window_get_byte_out_of_range_too_small XFAIL
scl/compressors/lz77_sliding_window.py::test_lz77_encode_decode PASSED
scl/compressors/lz77_sliding_window.py::test_lz77_sequence_generation PASSED
scl/compressors/lz77_sliding_window.py::LZ77DecoderWindowTooSmallTest::test_LZ77DecoderWindowTooSmall XFAIL
scl/compressors/lz77_sliding_window.py::test_lz77_multiblock_file_encode_decode PASSED
scl/compressors/rANS.py::test_check_encoded_bitarray PASSED
scl/compressors/rANS.py::test_rANS_coding rANS coding: avg_log_prob=1.499, rANS codelen: 1.504
rANS coding: avg_log_prob=1.484, rANS codelen: 1.489
rANS coding: avg_log_prob=1.430, rANS codelen: 1.435
rANS coding: avg_log_prob=2.585, rANS codelen: 2.590
rANS coding: avg_log_prob=0.815, rANS codelen: 0.819
PASSED
scl/compressors/lz77.py::test_empirical_int_huffman_encoder_decoder PASSED
scl/compressors/lz77.py::test_log_scale_binned_integer_encoder_decoder PASSED
scl/compressors/lz77.py::test_lz77_encode_decode PASSED
scl/compressors/lz77.py::test_lz77_sequence_generation PASSED
scl/compressors/lz77.py::test_lz77_multiblock_file_encode_decode PASSED
scl/compressors/huffman_coder.py::test_huffman_coding_dyadic 
Avg Bits: 1.0, optimal codelen: 1.0, Entropy: 1.0
Avg Bits: 1.527, optimal codelen: 1.527, Entropy: 1.5
Avg Bits: 1.794, optimal codelen: 1.794, Entropy: 1.75
PASSED
scl/compressors/golomb_coder.py::test_golomb_encode_decode PASSED
scl/compressors/shannon_fano_elias_coder.py::test_shannon_fano_elias_coding PASSED
scl/compressors/prefix_free_compressors.py::test_build_prefix_free_tree_from_code PASSED
scl/compressors/arithmetic_coding.py::test_bitarray_for_specific_input PASSED
scl/compressors/arithmetic_coding.py::test_arithmetic_coding  avg_log_prob=1.473, avg_codelen: 1.512
avg_log_prob=1.447, avg_codelen: 1.482
avg_log_prob=1.429, avg_codelen: 1.442
avg_log_prob=2.585, avg_codelen: 2.606
PASSED
scl/compressors/arithmetic_coding.py::test_adaptive_arithmetic_coding  avg_log_prob=1.473, avg_codelen: 1.512
avg_log_prob=1.447, avg_codelen: 1.492
avg_log_prob=1.429, avg_codelen: 1.462
avg_log_prob=2.585, avg_codelen: 2.612
PASSED
scl/compressors/arithmetic_coding.py::test_adaptive_order_k_arithmetic_coding 
k: 0, expected_bitrate=1.585, avg_codelen: 1.589
k: 1, expected_bitrate=1.585, avg_codelen: 1.591
k: 2, expected_bitrate=1.000, avg_codelen: 1.016
k: 3, expected_bitrate=1.000, avg_codelen: 1.025
PASSED
scl/compressors/elias_delta_uint_coder.py::test_elias_delta_uint_encode_decode PASSED
scl/compressors/elias_delta_uint_coder.py::test_elias_delta_uint_encode PASSED
scl/compressors/range_coder.py::test_range_coding 
avg_log_prob=1.499, avg_codelen: 1.506
avg_log_prob=1.484, avg_codelen: 1.490
avg_log_prob=1.430, avg_codelen: 1.436
avg_log_prob=0.000, avg_codelen: 0.006
PASSED
scl/compressors/fano_coder.py::test_fano_coding PASSED

=========================================================== 53 passed, 4 xfailed in 51.27s ============================================================
</code></pre>
</li>
</ol>
<h3 id="1-probability-and-coin-toss-20-points-1"><a class="header" href="#1-probability-and-coin-toss-20-points-1">1. Probability and Coin Toss (<em>20 points</em>)</a></h3>
<p>Kedar is a big <a href="https://en.wikipedia.org/wiki/Cricket">Cricket</a> fan. In cricket (like many other sports), there is a coin toss in the beginning to determine which team gets the first chance to bat. However, Kedar thinks that the coin being used is <em>biased</em> and lands as <em>Heads</em> (<code>H</code>) much more often than <em>tails</em> (<code>T</code>). Let the probability of the coin landing on <code>H</code> be $p$.</p>
<p>Shubham suggests to Kedar that this problem can be solved (i.e. we can get a <em>fair</em> coin) by tossing the coin twice! If we get a <code>H,T</code>sequence in the two tosses, then we mark it as a heads, while if we get a <code>T,H</code> sequence we can mark this as a tails. If we get a <code>H,H</code> or <code>T,T</code> then discard the result and start over!</p>
<ol>
<li>
<p>[5 points] Do you think Shubham's solution is right? Please justify.</p>
<p><strong>Solution</strong></p>
<p>Shubham's solution is right. Scheme only terminates when <code>H,T</code> or <code>T,H</code> sequences are received, which because of IID (independent and identically distributed) nature of our coin tosses and symmetry have the same probability.  In fact, the iterative scheme can be thought of as a Geometric Random variable. After every two tosses, the scheme terminates with probability of $p_{success}=2p(1-p)$ (getting either a <code>H,T</code> or <code>T,H</code> sequence) and we are back to initial state with a probability of $p_{failure}=\left(p^2 + (1-p)^2\right)$ (getting <code>H,H</code> or <code>T,T</code> sequence). If the scheme terminates after n sequences of these two tosses, we declare it heads or tails with the same probability of</p>
<p>$$
\begin{aligned}
p_{shubham-head} &amp;= p_{failure}^{n-1} * p(H)*P(T) \ &amp;= p_{failure}^{n-1} * p(T)*P(H) \ &amp;= p_{shubham-tail}
\end{aligned}
$$</p>
</li>
<li>
<p>[5 points] What is the expected number of coin flips needed to get one <code>H</code> or <code>T</code> outcome using Shubham's method?</p>
<p><strong>Solution</strong></p>
<p>Using the fact that Shubham's solution is a Geometric Random variable with $p_{success}=2p(1-p)$ and involves two coin tosses per iteration, the expected number of coin flips needed to get one <code>H</code> or <code>T</code> outcome using Shubham's method is</p>
<p>$$
\mathbb{E}(tosses) = 2 * 1/p_{success} = 1/p(1-p)
$$</p>
</li>
<li>
<p>[5 points] Kedar is still not convinced and thinks that we should really implement this idea to see if it works. Please help out Shubham by implementing the function <code>shubhams_fair_coin_generator()</code> in the file <code>HWs/HW1/hw1_p1.py</code>.
NOTE: please run all of this in the <code>(ee274_env)</code> environment which you created and that you are in the correct branch of the <code>SCL</code> repo (<code>EE274_Fall23/HWs</code>). Ensure that the tests except those in <code>HWs</code> folder pass before you start implementing the function.</p>
<p><strong>Solution</strong></p>
<pre><code class="language-python">def shubhams_fair_coin_generator():
     """
     TODO:
     use the biased_coin_generator() function above to implement the fair coin generator

     Outputs:
         toss (str): "H" or "T"
         num_biased_tosses (int): number of times the biased_coin_generator() was called to generate the output
     """
     toss = None
     num_biased_tosses = 0

     ##############################
     # ADD CODE HERE
     # raise NotImplementedError
     while True:
         toss1, toss2 = biased_coin_generator(), biased_coin_generator()
         num_biased_tosses += 2
         if toss1 != toss2:
             toss = toss1
             break
     ##############################

     return toss, num_biased_tosses
</code></pre>
</li>
<li>
<p>[5 points] It is always a good practice to write tests to ensure the implementation is correct (even if you think theoretically it is alright :)). Please implement <code>test_shubhams_fair_coin_generator()</code> function in <code>HWs/HW1/hw1_p1.py</code>. You can test your code by running the following command in <code>stanford_compression_library/scl</code> folder:</p>
<pre><code class="language-sh">py.test -s -v HWs/HW1/hw1_p1.py
</code></pre>
<p><strong>Solution</strong></p>
<pre><code class="language-python">def test_shubhams_fair_coin_generator():
     """
     TODO:
     write a test to check whether the shubhams_fair_coin_generator() is really generating fair coin tosses

     Also, check if the num_biased_tosses matches the answer which you computed in part 2.
     """

     # perform the experiment
     # feel free to reduce this when you are testing
     num_trials = 10000
     tosses = []
     num_biased_tosses_list = []
     for _ in range(num_trials):
         toss, num_biased_tosses = shubhams_fair_coin_generator()

         # check if the output is indeed in H,T
         assert toss in ["H", "T"]
         tosses.append(toss)
         num_biased_tosses_list.append(num_biased_tosses)

     # NOTE: We are using the DataBlock object from SCL.
     # Take a look at `core/data_block.py` to understand more about the class
     # the get_empirical_distribution() outputs a ProbDist class. Take a look at
     # `core/prob_dist.py` to understand this object better
     tosses_db = DataBlock(tosses)
     empirical_dist_tosses = tosses_db.get_empirical_distribution()

     #############################################
     # ADD CODE HERE
     # 1. add test here to check if empirical_dist_tosses is close to being fair
     # 2. add test to check if avg value of num_biased_tosses matches what you expect
     # (both within a reasonable error range)
     # You can use np.testing.assert_almost_equal() to check if two numbers are close to some error margin 
     # defined by the `decimal` argument.
     # NOTE: With 10000 trials, you can expect the empirical probability to match within `decimal=2` (2 decimal places)
     # and the average value of `num_biased_tosses` to match within `decimal=1` (1 decimal place). You can always
     # increase the num_trials to get better accuracy. The default value of `decimal=7` is too strict for our purpose.
     np.testing.assert_almost_equal(
         empirical_dist_tosses.probability("H"), 0.5, decimal=2,  err_msg="Probability of H is not 0.5",
     )
     np.testing.assert_almost_equal(
         empirical_dist_tosses.probability("T"), 0.5, decimal=2, err_msg="Probability of T is not 0.5",
     )

     expected_num_biased_tosses = sum(num_biased_tosses_list) / len(num_biased_tosses_list)
     np.testing.assert_almost_equal(
         expected_num_biased_tosses, 1/(0.8*0.2), decimal=1,
         err_msg="Expected Number of biased tosses is not 1/(p*(1-p))",
     )
     #############################################
</code></pre>
</li>
<li>
<p>(NOT GRADED, THINK FOR FUN!): Do you think Shubham's solution is <em>optimal</em>? For example, Shubham is using at least <code>2</code> biased coin tosses to generate <code>1</code> fair coin toss. This seems wasteful in some cases (for example, if the coin is almost unbiased). Can you improve upon the <em>average</em> number of biased coin tosses needed to generate one fair coin toss?</p>
<p><strong>Solution</strong></p>
<p>Yes, Shubham's solution is not optimal. We can improve upon the <em>average</em> number of biased coin tosses by carefully utilizing the cases of <code>H,H</code> and <code>T,T</code> sequences instead of throwing them away. For those of you who are aware of entropy, this problem has close relationship to the entropy! <a href="https://stanforddatacompressionclass.github.io/notes/coinflipext.pdf">Here</a> is a detailed analysis of this problem by Michael Mitzenmacher.</p>
</li>
</ol>
<h3 id="q2-basic-information-theory--20-points-1"><a class="header" href="#q2-basic-information-theory--20-points-1">Q2: Basic Information theory  (<em>20 points</em>)</a></h3>
<ol>
<li>
<p>[5 points] Let $X$ be a random variable over positive integers with a distribution $$P_X(X=k) = 2^{-k}, k \geq 1$$
Compute the entropy $H(X)$ of random variable $X$. What are the optimal code-lengths for a prefix-free code designed for distribution $P_X$?</p>
<p><strong>Solution</strong></p>
<p>$$
H(X) = \sum_{k=1}^{\infty} 2^{-k} \log_2 \frac{1}{2^{-k}} = \sum_{k=1}^{\infty} k 2^{-k} = 2
$$</p>
<p>where the last equality comes from summing an <a href="https://en.wikipedia.org/wiki/Arithmetico-geometric_sequence">arithmetico-geometric progression</a>. The optimal code-lengths for a prefix-free code designed for distribution $P_X$ is $k$ for $X=k$ by using the $\left\lceil \log_2 \frac{1}{p(symbol)} \right\rceil$ rule.</p>
</li>
<li>
<p>[5 points] Provide an optimal prefix-free code design for the distribution $P_X$.</p>
<p><strong>Solution</strong>
One optimal prefix-free code design for the distribution $P_X$ is (note you can change <code>0</code> with <code>1</code> WLOG):
$$ \begin{array}{|c|c|} \hline
symbol &amp; code \ \hline
1 &amp; 0 \ \hline
2 &amp; 10 \ \hline
3 &amp; 110 \ \hline
4 &amp; 1110 \ \hline
\vdots &amp; \vdots \ \hline
\end{array}$$</p>
</li>
<li>
<p>[10 points] Let $Y$ be a random variable over positive integers such that $\mathbb{E}(Y) = 2$.
Then show that:$$ H(Y) \leq 2 $$
For what distribution of the random variable $Y$ does the equality hold, i.e. $H(Y) = 2$?
<strong>HINT:</strong> KL-Divergence and it's properties will be helpful to show this.</p>
<p><strong>Solution</strong>
Let's take KL divergence between the distribution $P_Y$ and the distribution $P_X$ defined in <code>Q1.1</code>.</p>
<p>$$
\begin{aligned}
D_{KL}(P_Y||P_X) &amp;= \sum_{k=1}^{\infty} P_Y(k) \log \frac{P_Y(k)}{P_X(k)} \
&amp;= \sum_{k=1}^{\infty} P_Y(k) \log \frac{P_Y(k)}{2^{-k}} \
&amp;= \sum_{k=1}^{\infty} k P_Y(k) - P_Y(k) \log \frac{1}{P_Y(k)} \
&amp;= \mathbb{E}(Y) - H(Y) \
\end{aligned}
$$
where the last line uses the definition of expectation and entropy.</p>
<p>Since we know that $D_{KL}(P_Y||P_X)$ is non-negative and $\mathbb{E}(Y) = 2$, we have that $H(Y) \leq 2$ showing the asked inequality.</p>
<p>We also know that the equality in KL divergence holds when $P_Y = P_X$ and hence the equality holds for geometric distribution as described in part <code>Q1.1</code>, i.e. $$ P_Y(Y=k) = 2^{-k}, k \geq 1$$</p>
</li>
</ol>
<h3 id="q3-uniquely-decodable-codes-25-points-1"><a class="header" href="#q3-uniquely-decodable-codes-25-points-1">Q3. Uniquely decodable codes (<em>25 points</em>)</a></h3>
<p>We mainly explored prefix-free codes in class. But there is a much broader class of codes, the uniquely decodable codes. A code is uniquely decodable if no two sequences (of length 1 or more) of input symbols (say $x^n$ and $y^m$) can produce the same encoding, and hence one can uniquely determine the input sequence from the encoding. In this question, we will show that even with the uniquely decodable codes, we cannot do better than the Entropy.</p>
<ol>
<li>
<p>[5 points] Consider the code <code>C1</code> below. Is <code>C1</code> uniquely decodable? Implement a decoder for the code below. Briefly justify your decoding procedure.</p>
<pre><code>A -&gt; 10
B -&gt; 00
C -&gt; 11
D -&gt; 110
</code></pre>
<p><strong>Solution</strong>
The code <code>C1</code> is uniquely decodable. The decoding procedure is as follows:</p>
<ul>
<li>read first two bits</li>
<li>if the read bits are <code>10</code>, then the decoded symbol is <code>A</code></li>
<li>if the read bits are <code>00</code>, then the decoded symbol is <code>B</code></li>
<li>if the read bits are <code>11</code>, then
<ul>
<li>calculate number of <code>0</code>s in the remaining encoded bitstream till you get first <code>1</code>. say this number is $0_k$.</li>
<li>if $0_k$ is <code>odd</code>, then output <code>D</code> followed by $\frac{0_k - 1}{2}$ <code>B</code>.</li>
<li>if $0_k$ is <code>even</code>, then output <code>C</code> followed by $\frac{0_k}{2}$ <code>B</code>.</li>
</ul>
</li>
<li>repeat the above procedure till you have decoded all the symbols.</li>
</ul>
<p>This procedure works because only <code>C</code> and <code>D</code> share a prefix but <code>C</code> is shorter than <code>D</code> but differ in only <code>1 bit</code> whereas there is no symbol which is being encoded using only <code>1 bit</code>. Hence, the decoding procedure is unambiguous.</p>
</li>
</ol>
<p>Consider an alphabet $\mathcal{U}$ and a uniquely decodable code with code lengths for $u \in \mathcal{U}$ denoted by $L(u)$. Also, we denote the codelength of the n-tuple $u^n$ as $L(u^n) = \sum_{i=1}^n L(u_i)$.</p>
<p><strong>HINT</strong>: For parts <code>Q3.2</code> and <code>Q3.3</code>, you might find it easier to gain intuition by starting with small values of $|\mathcal{U}|$ and $n$ (e.g., binary alphabet with $n=1$ or $2$) and manually expanding the summations.</p>
<ol start="2">
<li>
<p>[5 points] Show that $$\left( \sum_{u \in \mathcal{U}} 2^{-L(u)} \right)^n = \sum_{u^n \in \mathcal{U}^n} 2^{-L(u^n)} $$</p>
<p><strong>Solution</strong>
$$
\begin{align}
\left( \sum_{i=1}^{|\mathcal{U}|} 2^{-L(u_i)}\right)^{n} = \left( \sum\limits_{u \in \mathcal{U}} 2^{-L(u)}\right)^{n} &amp;= \left( \sum\limits_{u_{1}} 2^{-L(u_{1})}\right)\cdot \left( \sum\limits_{u_{2}} 2^{-L(u_{2})}\right)\cdot ... \cdot \left( \sum\limits_{u_n} 2^{-L(u_{n})}\right)
\&amp;=  \sum\limits_{u_{1}} \sum\limits_{u_{2}} ...  \sum\limits_{u_{n}}\prod\limits_{i=1}^n 2^{-L(u_{i})}
\&amp;=\sum\limits_{u_{1}, ... u_{n}} 2^{-\sum_{i=1}^n L(u_{i})}
\&amp;=\sum_{u^n \in \mathcal{U}^n} 2^{-L(u^n)}
\end{align}
$$
where</p>
<ul>
<li>first line just rewrites the $n$th power of the Kraft sum as a product of $n$ distinct sums by renaming the variable.</li>
<li>second line pulls the sums out since the summation variables are all different.</li>
<li>third line moves product into the exponent where it becomes a sum</li>
<li>fourth line just uses the fact the code length of $u^n$ is simply the sum of code lengths of the symbols $u_1,\dots,u_n$.</li>
</ul>
</li>
<li>
<p>[5 points] Let $l_{max} = \max_{u \in \mathcal{U}} L(u)$. Then we can rewrite the summation as: $$\sum_{u^n \in \mathcal{U}^n} 2^{-L(u^n)} = \sum_{j=1}^{n \cdot l_{max}} |{u^n| L(u^n) = j}| \cdot 2^{-j}$$</p>
<p>NOTE: Rewriting summation is a common proof trick, and is a useful one to watch out for!
Using <code>Q3.2</code> and the identity above, show that:</p>
<p>$$\left( \sum_{i=1}^{|\mathcal{U}|} 2^{-L(u_i)} \right)^n \leq n\cdot l_{max}$$</p>
<p><strong>Solution</strong></p>
<p>Let the maximum codelength be $l_{max} = \max_i l_k$. Then note that,</p>
<p>$$
\begin{align}
\sum_{u^n \in \mathcal{U}^n} 2^{-L(u^n)} &amp;=\sum\limits_{i=1}^{n\cdot l_{max}} \vert\lbrace u^{n} \vert L(u^{n}) = i \rbrace \vert \cdot 2^{-i}
\end{align}
$$</p>
<p>Here we have clubbed the sum according to values of $2^{-L(u^{n})}$. This implies using result from <code>Q2.2</code> that</p>
<p>$$
\begin{align}
\left( \sum_{i=1}^{|\mathcal{U}|} 2^{-L(u_i)} \right)^n &amp;= \sum_{u^n \in \mathcal{U}^n} 2^{-L(u^n)} \
&amp;= \sum\limits_{i=1}^{n\cdot l_{max}} \vert\lbrace u^{n} \vert L(u^{n}) = i \rbrace \vert \cdot 2^{-i} \
&amp;\leq \sum\limits_{i=1}^{n\cdot l_{max}} 2^i \cdot 2^{-i} = n\cdot l_{max}
\end{align}
$$</p>
<p>where the last inequality uses the fact that the code is uniquely decodable and for such a code, the set of $n$-tuples giving a $i$ length bit sequence can be at most $2^i$.</p>
</li>
<li>
<p>[5 points] Using <code>Q3.3</code> show that uniquely decodable codes satisfy Kraft's inequality i.e.</p>
<p>$$\left( \sum_{u\in\mathcal{U}} 2^{-L(u)} \right) \leq 1$$</p>
<p><strong>Solution</strong></p>
<p>Using <code>Q2.3</code>, we have that $\left( \sum_{i=1}^{|\mathcal{U}|} 2^{-L(u_i)} \right)^n \leq n.l_{max}$ <strong>for any $n$-tuple</strong> of a uniquely-decodable code! Therefore, taking $n$-th root on both sides and taking limit as $n$ goes to infinity, we get that</p>
<p>$$
\begin{align}
\left( \sum_{i=1}^{|\mathcal{U}|} 2^{-L(u_i)} \right) \leq \lim_{n \to \infty} (n\cdot l_{max})^{\frac{1}{n}} = 1
\end{align}
$$</p>
<p>therefore showing that Kraft's inequality holds for uniquely decodable codes.</p>
</li>
<li>
<p>[5 points] Now show that for any uniquely decodable code for $U$ with codeword lengths $L(U)$, $$\mathbb{E}[L(U)] \geq H(U) .$$
<strong>NOTE:</strong> We saw a similar proof for prefix-free codes in the class!<br>
<br>
Also argue that for any uniquely decodable code, it is possible to construct a prefix-free code with the same codeword lengths.</p>
<p>This means that uniquely decodable codes generally do not provide any benefits over prefix-free codes and instead have a more complicated decoding procedure!</p>
<p><strong>Solution</strong></p>
 <!-- **ToDo: expand**
 - Since uniquely decodable codes follow Kraft's inequality, same proof shows that expected codeword length is lower bounded by entropy.
 - Scheme to get a prefix-free code from uniquely decodable code would then be to extract the codelengths from the uniquely decodable code and then use the same code-lengths to construct a prefix-free code similar to Huffman tree. -->
<p>Let $q(u)=c 2^{-L(u)}$ be a PMF where the normalizing constant $c$ is chosen such that the probability sums to 1, i.e. $$c = \frac{1}{\sum_{u \in \mathcal{U}} 2^{-L(u)}}.$$ By the Kraft-McMillan Inequality, we know that $c \geq 1$.</p>
<p>Let $p(u)$ be a PMF of $U$. Consider</p>
<p>$$
\begin{aligned}
E[L(U)]-H(U)&amp;=\sum_{u \in \mathcal{U}} p(u) L(u)-\sum_{u \in \mathcal{U}} p(u) \log_2 \frac{1}{p(u)}\
&amp; = -\sum_{u \in \mathcal{U}} p(u) \log_2 2^{-L(u)}+\sum_{u \in \mathcal{U}} p(u) \log_2 p(u)\
&amp;=\sum_{u \in \mathcal{U}} p(u) \log_2 \frac{p(u)}{2^{-L(u)}} \
&amp;=\sum_{u \in \mathcal{U}} p(u) \log_2 \frac{p(u)}{q(u) / c} \
&amp;=\sum_{u \in \mathcal{U}} p(u) \log_2 \frac{p(u)}{q(u)} + \sum_{u \in \mathcal{U}} p(u) \log_2 c \
&amp;= D_{KL}(p \Vert q) + \log_2 c \geq 0
\end{aligned}
$$
where the last inequality holds since relative entropy is non-negative and $c \geq 1$. Thus $$E[L(U)] \geq H(U).$$</p>
<p>Scheme to get a prefix-free code from uniquely decodable code would then be to extract the codelengths from the uniquely decodable code and then use the same code-lengths to construct a prefix-free code similar to Huffman tree.</p>
</li>
</ol>
<h3 id="q4-shannon-codes-25-points-1"><a class="header" href="#q4-shannon-codes-25-points-1">Q4: Shannon Code(s) (<em>25 points</em>)</a></h3>
<p>In class, we saw one version of the Shannon codes; the <a href="https://stanforddatacompressionclass.github.io/notes/lossless_iid/prefix_free_codes.html#designing-prefix-free-codes">tree-based construction</a>.</p>
<ol>
<li>
<p>[5 points]  Let's call it the <code>ShannonTreeEncoder</code> and the <code>ShannonTreeDecoder</code>. Manually calculate what the codewords should be for the following distributions:</p>
<pre><code class="language-python">ProbabilityDist({"A": 0.25, "B": 0.25, "C": 0.25, "D": 0.25})
ProbabilityDist({"A": 0.5, "B": 0.25, "C": 0.12, "D": 0.13})
ProbabilityDist({"A": 0.9, "B": 0.1})
</code></pre>
<p><strong>Solution</strong></p>
<ul>
<li>For the first distribution, the codewords should be <code>{"A": BitArray("00"), "B": BitArray("01"), "C": BitArray("10"), "D": BitArray("11")}</code>.</li>
<li>For the second distribution, the codewords should be <code>{"A": BitArray("0"), "B": BitArray("10"), "C": BitArray("1110"), "D": BitArray("110")}</code>.</li>
<li>For the third distribution, the codewords should be <code>{"A": BitArray("0"), "B": BitArray("1000")}</code>.</li>
</ul>
</li>
<li>
<p>[10 points] Complete the code for the <code>ShannonTreeEncoder</code> in <code>hw1_p4.py</code>. Also, complete the <code>test_shannon_tree_coding_specific_case()</code> to check the correctness of your codewords in part 1 against your implemented code. If you encounter errors, you should identify the failing test case and fix any issues. Note: we will be using automated tests to grade your code, so it is important that you do not change the function signatures. We will have more test cases for grading, so feel free to add more test cases of your own of your own when working on the problem.</p>
<p><strong>Solution</strong></p>
<pre><code class="language-python">def generate_shannon_tree_codebook(cls, prob_dist):
    # sort the probability distribution in decreasing probability
    sorted_prob_dist = ProbabilityDist.get_sorted_prob_dist(
        prob_dist.prob_dict, descending=True
    )
    codebook = {}

    ############################################################
    # ADD CODE HERE
    # raise NotImplementedError
    import math
    from utils.bitarray_utils import uint_to_bitarray

    cur_state = 0 # tracks the next unused node
    cur_codelen = 1

    for s in sorted_prob_dist.prob_dict:
        codelen = math.ceil(sorted_prob_dist.neg_log_probability(s))
        cur_state = cur_state &lt;&lt; (codelen-cur_codelen)
        cur_codelen = codelen
        codebook[s] = uint_to_bitarray(cur_state, bit_width=codelen)
        cur_state += 1
    ############################################################

    return codebook   
</code></pre>
<pre><code class="language-python">def test_shannon_tree_coding_specific_case():
    # NOTE -&gt; this test must succeed with your implementation
    ############################################################
    # Add the computed expected codewords for distributions presented in part 1 to these list to improve the test
    # raise NotImplementedError
    distributions = [
            ProbabilityDist({"A": 0.5, "B": 0.5}),
            ProbabilityDist({"A": 0.25, "B": 0.25, "C": 0.25, "D": 0.25}),
            ProbabilityDist({"A": 0.5, "B": 0.25, "C": 0.12, "D": 0.13}),
            ProbabilityDist({"A": 0.9, "B": 0.1}),
        ]
    expected_codewords = [
            {"A": BitArray("0"), "B": BitArray("1")},
            {"A": BitArray("00"), "B": BitArray("01"), "C": BitArray("10"), "D": BitArray("11")},
            {"A": BitArray("0"), "B": BitArray("10"), "C": BitArray("1110"), "D": BitArray("110")},
            {"A": BitArray("0"), "B": BitArray("1000")},
        ]
    ############################################################

    def test_encoded_symbol(prob_dist, expected_codeword_dict):
        """
        test if the encoded symbol is as expected
        """
        encoder = ShannonTreeEncoder(prob_dist)
        for s in prob_dist.prob_dict.keys():
            assert encoder.encode_symbol(s) == expected_codeword_dict[s]

    for i, prob_dist in enumerate(distributions):
        test_encoded_symbol(prob_dist, expected_codeword_dict=expected_codewords[i])   
</code></pre>
</li>
</ol>
<hr />
<p>We have also studied the tree-parsing based decoding for Shannon codes (or prefix-free codes in general). This decoding has already been implemented in the <code>ShannonTreeDecoder</code>. The core tree-parsing logic can be found in <a href="https://stanforddatacompressionclass.github.io/notes/lossless_iid/prefix_free_codes.html">here</a>. The tree-parsing based decoder works well however includes too many if/else conditions which leads to branching -- this is pretty bad for modern hardware. (<a href="https://stackoverflow.com/questions/9820319/why-is-a-cpu-branch-instruction-slow#:~:text=A%20branch%20instruction%20is%20not,sequential%20instructions%20being%20executed%20simultaneously">Here</a> is some additional information about this inefficiency for those interested.)</p>
<p>In this next subproblem, we will implement another decoder which we call the <code>ShannonTableDecoder</code>. The <code>ShannonTableDecoder</code> works by creating a <em>decoding lookup table</em> and then using it for decoding symbols without branching.</p>
<p>As an example, lets consider the following simple code:</p>
<pre><code class="language-python">encoding_table = {"A": 0, "B": 10, "C": 110, "D": 111}
</code></pre>
<p>In this case, the <code>ShannonTableDecoder</code> keeps track of data using following tables:</p>
<pre><code class="language-python">decoding_table = {
    000: "A",
    001: "A",
    010: "A",
    011: "A",
    100: "B",
    101: "B",
    110: "C",
    111: "D",
}
codelen_table = {
    "A": 1,
    "B": 2,
    "C": 3,
    "D": 3,
}
max_codelen = 3
</code></pre>
<p>The tables can be then used for decoding as per the pseudo-code below.</p>
<pre><code class="language-python">def decode_symbol_table(encoded_bitarray):
    state = encoded_bitarray[:max_codelen] 
    decoded_symbol = decoding_table[str(state)]
    num_bits_consumed = codelen_table[decoded_symbol]
    return decoded_symbol, num_bits_consumed
</code></pre>
<ol start="3">
<li>
<p>[5 points] Based on the pseudo-code above, explain in a few sentences how the table based decoding works. Why do you need to output both the decoded symbol and the number of bits consumed?</p>
<p><strong>Solution</strong>
<code>decode_symbol_table(encoded_bitarray)</code> takes the first <code>max_codelen</code> bits from the <code>encoded_bitarray</code> and uses it as a key to look up the <code>decoding_table</code> to get the decoded symbol. It then uses the decoded symbol to look up the <code>codelen_table</code> to get the number of bits consumed. It then returns the decoded symbol and the number of bits consumed. The number of bits consumed is needed to know where to start the next decoding from.</p>
</li>
<li>
<p>[5 points] Complete the <code>ShannonTableDecoder</code> in <code>hw1_p4.py</code> by implementing the <code>create_decoding_table</code> function. You can verify if your implementation is correct by using <code>test_shannon_table_coding_end_to_end</code> function.</p>
<p><strong>Solution</strong></p>
<pre><code class="language-python">@staticmethod
def create_decoding_table(prob_dist: ProbabilityDist):
    """
    :param prob_dist: ProbabilityDist object
    :return:
        decoding_table: dictionary mapping str to symbols
        codelen_table: dictionary mapping symbols to code-length
        max_codelen: maximum code-length of any symbol in the codebook
    """
    # create the encoding table
    encoding_table = ShannonTreeEncoder.generate_shannon_tree_codebook(prob_dist)
    ############################################################
    # ADD CODE HERE
    # NOTE: You might find the following utility functions useful:
    # - ProbabilityDist.neg_log_probability
    # - scl.utils.bitarray_utils.uint_to_bitarray and scl.utils.bitarray_utils.bitarry_to_uint

    # NOTE: IMPORTANT - The keys of decoding_table must be strings (not BitArray).
    #                   This is because BitArray type is not hashable. You can
    #                   still use BitArray to construct the codeword but before
    #                   indexing into decoding_table simply convert to string like
    #                   str(bitarray).

    # we now create the decoding table based on the encoding table
    # first get the maximum length
    max_codelen = max(len(code) for code in encoding_table.values())

    # create a empty table of size 2^{max_codelen}
    decoding_table = {}
    codelen_table = {}
    # let's fill the decoding table
    for s, code in encoding_table.items():
        codelen = len(code)
        start_index = bitarray_to_uint(code + "0" * (max_codelen - codelen))
        num_indices = 1 &lt;&lt; (max_codelen - codelen)
        for ind in range(start_index, start_index + num_indices):
            decoding_table[str(uint_to_bitarray(ind, bit_width=max_codelen))] = s
        codelen_table[s] = codelen
    return decoding_table, codelen_table, max_codelen
</code></pre>
</li>
</ol>
<h3 id="q5-huffman-coding-on-real-data-30-points-1"><a class="header" href="#q5-huffman-coding-on-real-data-30-points-1">Q5. Huffman coding on real data (<em>30 points</em>)</a></h3>
<p>In class, we have studied compression for sources where we knew the distribution of data beforehand. In most real-life scenarios, this is not the case. Often times, we use the empirical distribution of the data and design a code for this distribution. Then we encode the data with this distribution. For the decoding to work, we also need to transmit the distribution or the code in some way. This problem focuses on compression of a provided file with Huffman coding.</p>
<p>Before we start with the code, let's look at how important it is to transmit the distribution faithfully to achieve lossless compression.</p>
<ol>
<li>
<p>[10 points] Consider the probability distribution represented as a mapping from a symbol to its probability: <code>{A: 0.11, B: 0.09, C: 0.09, D: 0.71}</code>.</p>
<p>a. What are the codeword lengths for the symbols in the Huffman code?</p>
<p><strong>Solution</strong>
Building the huffman tree,</p>
</li>
</ol>
<pre class="mermaid">    graph TD
        N1[&quot;B&lt;br/&gt;(p=0.09)&quot;]:::endnode
        N2(&quot;C&lt;br/&gt;(p=0.09)&quot;):::endnode
        N3(&quot;A&lt;br/&gt;(p=0.11)&quot;):::endnode
        N4(&quot;D&lt;br/&gt;(p=0.71)&quot;):::endnode
        N5(&quot;N1&lt;br/&gt;(p=0.18)&quot;) --&gt; N1
        N5 --&gt; N2
        N6(&quot;N2&lt;br/&gt;(p=0.29)&quot;) --&gt; N3
        N6 --&gt; N5
        N7(&quot;*&lt;br/&gt;(p=1.0)&quot;) --&gt; N4
        N7 --&gt; N6
        
        style N5 fill:#dddddd
        style N6 fill:#dddddd
        style N7 fill:#dddddd
</pre>
<pre><code>the codeword lengths are
`{A: 2, B: 3, C: 3, D: 1}`

After a harrowing cab experience in the US, Pulkit wants to send a codeword `BADCAB` to Shubham to describe his frustration. Shubham and Pulkit had agreed to use Huffman encoding for such communications in the past. He encodes the codeword `BADCAB` using a Huffman code for the original distribution `{A: 0.11, B: 0.09, C: 0.09, D: 0.71}`, and sends both the codeword and this distribution  to Shubham (so that Shubham is able to decode it on his end by building a Huffman tree on his own). 

b. Unfortunately,  Shubham decodes the message to be `CADBAC` instead of `BADCAB` making him confused. What might have gone wrong during this communication between the two?

**HINT:** notice the specific encoded and decoded symbols in the message.

**Solution**
When Shubham built his Huffman tree, he inverted the codewords for `B` and `C` since they have the same codelengths, and hence decoded the message incorrectly by switching the two letters in his decoding. Recall that, huffman codes are not unique and hence the codeword lengths are not enough to reconstruct the Huffman tree. The order of the symbols in the distribution is also important to reconstruct the Huffman tree. 

Pulkit-Shubham realized this issue and fixed it. 

c. In the spirit of frugality, Pulkit decides to transmit a rounded distribution with just one significant decimal (`{A: 0.1, B: 0.1, C: 0.1, D: 0.7}`). What are the codeword lengths for the symbols in the Huffman code for this distribution? Are there multiple possible Huffman code lengths? Why?

**Solution**

There are multiple possible Huffman code lengths since the distribution is not unique. For example, the distribution `{A: 2, B: 3, C: 3, D: 1}` or `{A: 3, B: 3, C: 2, D: 1}` are valid huffman codeword lengths for each character depending on how the ties were broken in merging the first symbol.

These issues suggest that it is important to transmit the distribution faithfully to achieve lossless compression. In addition, the encoder and the decoder must share the same deterministic algorithm to construct the Huffman tree from the distribution.
</code></pre>
<p>With this context, look at the provided starter code in <code>hw1_p5.py</code> which does the following:</p>
<ol>
<li>Loop over the file in blocks of size 50 KB.</li>
<li>For each block, compute the empirical distribution from the data. This simply means we get the counts of each possible byte value ${0, 1, 2, \ldots, 255}$ in the block and normalize by the block length to get an approximate empirical probability distribution.</li>
<li>Apply Huffman encoding on the block assuming this empirical distribution.</li>
<li>Encode the empirical probability distribution in bits so that decoder knows what probability distribution to use for decoding.</li>
<li>Final encoding is the encoding from step 4 concatenated with the encoding from step 3.</li>
</ol>
<p>To understand the overall process, you should go through the provided code and specifically the <code>encode_block</code> and <code>decode_block</code> functions.</p>
<ol start="2">
<li>[10 points] Implement the encoding and decoding of probability distribution step above (Step 4) in the functions <code>encode_prob_dist</code> &amp; <code>decode_prob_dist</code> respectively. There are multiple ways to encode this, and we are not looking for the most optimized implementation here. Note that we are working with 50 KB blocks here, while the alphabet size is just 256. So as long as your implementation is not absurdly bad, the overhead will be small. You might find the following pointers useful as you implement the function:
<ul>
<li>The input <code>bitarray</code> to <code>decode_prob_dist</code> can include more than the encoding of the probability distribution itself. Thus, it should only read as much as it needs and return the number of bits read so that the Huffman decoding can start from the next bit.</li>
<li>Python dictionaries are <a href="https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-3-6">OrderedDicts</a>. If you are using dictionaries to represent probability distribution, then it is critical to maintain this ordering while creating Huffman trees during encoding/decoding. See the Pulkit-Shubham communication fiasco above for an example of what can go wrong if the order is not preserved.</li>
<li>Python's <code>float</code> type is equivalent to the <code>double</code> type in C (see <a href="https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex">this</a> for a refresher). As long as you provide the same exact distribution to the encoder and decoder, all is well!
<ul>
<li><strong>IMPORTANT</strong>: Unless you are feeling particularly adventurous, we strongly recommend using the provided helper functions <code>float_to_bitarray64</code> and <code>bitarray64_to_float</code> for conversion between <code>float</code> and <code>BitArray</code>.</li>
</ul>
</li>
<li>Also, you will hear about <a href="https://en.wikipedia.org/wiki/Canonical_Huffman_code">Canonical Huffman code</a> in class, which provides an alternative to solve some of these issues in practice. You do not need it for this question.</li>
</ul>
</li>
</ol>
<p>Verify that your implementation is correct by running
<code>    py.test -s -v HWs/HW1/hw1_p5.py    </code>
If you encounter errors, you should identify the failing test case and fix any issues. Note: we will be using automated tests to grade your code, so it is important that you do not change the function signatures and that your code passes these tests. We will have more test cases for grading, so feel free to add more test cases of your own when working on the problem. But these should be sufficient to get you started.</p>
<p><strong>Solution</strong></p>
<p>We first encode the number of symbols with a 32-bit integer, followed by pairs of (symbol - 8 bit, probability - 64 bit). Students can have various different ways to serialize and deserialize the probability table, and we have accepted all of them as valid solutions as long as the pass they end-to-end encoding-decoding test.</p>
<pre><code class="language-python">def encode_prob_dist(prob_dist: ProbabilityDist) -&gt; BitArray:
    """Encode a probability distribution as a bit array

    Args:
        prob_dist (ProbabilityDist): probability distribution over 0, 1, 2, ..., 255
            (note that some probabilities might be missing if they are 0).

    Returns:
        BitArray: encoded bit array
    """
    prob_dict = prob_dist.prob_dict # dictionary mapping symbols to probabilities

    #########################
    # ADD CODE HERE
    # You can find int_to_bitarray8 and float_to_bitarray64 useful
    # to encode the symbols and the probabilities respectively.
    # uint_to_bitarray from utils.bitarray_utils can also come in handy
    # to encode any other integer values your solution requires.
    
    num_symbols = len(prob_dict)
    encoded_probdist_bitarray = uint_to_bitarray(num_symbols, bit_width=32)
    for symbol, prob in prob_dict.items():
        encoded_probdist_bitarray += int_to_bitarray8(symbol)
        encoded_probdist_bitarray += float_to_bitarray64(prob)

    #########################

    return encoded_probdist_bitarray
</code></pre>
<pre><code class="language-python">def decode_prob_dist(bitarray: BitArray) -&gt; Tuple[ProbabilityDist, int]:
    """Decode a probability distribution from a bit array

    Args:
        bitarray (BitArray): bitarray encoding probability dist followed by arbitrary data

    Returns:
        prob_dist (ProbabilityDist): the decoded probability distribution
        num_bits_read (int): the number of bits read from bitarray to decode probability distribution
    """
    #########################
    # ADD CODE HERE
    # You can find bitarray8_to_int and bitarray64_to_float useful
    # to decode the symbols and the probabilities respectively.
    # bitarray_to_uint from utils.bitarray_utils can also come in handy
    # to decode any other integer values your solution requires.
    prob_dict = {}
    num_bits_read = 0
    num_symbols = bitarray_to_uint(bitarray[num_bits_read:num_bits_read+32])
    num_bits_read += 32
    for _ in range(num_symbols):
        symbol = bitarray8_to_int(bitarray[num_bits_read:num_bits_read+8])
        num_bits_read += 8
        prob = bitarray64_to_float(bitarray[num_bits_read:num_bits_read+64])
        num_bits_read += 64
        prob_dict[symbol] = prob
    
    # raise NotImplementedError("You need to implement decode_prob_dist")
    #########################

    prob_dist = ProbabilityDist(prob_dict)
    return prob_dist, num_bits_read
</code></pre>
<ol start="3">
<li>
<p>[5 points] Now download the Sherlock Holmes novel "The Hound of the Baskervilles by Arthur Conan Doyle" using the following command.</p>
<pre><code class="language-sh">curl -o HWs/HW1/sherlock.txt https://www.gutenberg.org/files/2852/2852-0.txt
</code></pre>
<p>Run the following commands to compress the file using your newly developed compressor, decompress it back, and verify that the decompression succeeds.</p>
<pre><code class="language-sh">python HWs/HW1/hw1_p5.py -i HWs/HW1/sherlock.txt -o HWs/HW1/sherlock.txt.huffz
python HWs/HW1/hw1_p5.py -d -i HWs/HW1/sherlock.txt.huffz -o HWs/HW1/sherlock.txt.decompressed
cmp HWs/HW1/sherlock.txt HWs/HW1/sherlock.txt.decompressed
</code></pre>
<p>Nothing should be printed on the terminal as a result of the last command if the files match.</p>
<ul>
<li>Report the size of your compressed file. You can run <code>wc -c HWs/HW1/sherlock.txt.huffz</code> to print the size.</li>
<li>How much is the overhead due to storing the probability distribution? <strong>Note:</strong> One easy way to check this to replace <code>encode_prob_dist</code> with a version that just returns <code>BitArray()</code>. Obviously the decoding won't work with this change!</li>
<li>Print the obtained huffman tree on sherlock.txt by using the <code>print_huffman_tree</code> function (commented in <code>encode_block</code> function of <code>HuffmanEmpiricalEncoder</code>). What do you observe? Does the printed Huffman tree make sense? Why or why not?</li>
</ul>
</li>
</ol>
<p><strong>Solution</strong></p>
<ul>
<li>The size of the compressed file using implementation as given in solution above is <code>207797</code> bytes.</li>
<li>The size of the compressed file without probability table is <code>202392 bytes</code>. So the overhead using our approach is <code>5405 bytes</code> (~$2.67%$).</li>
<li>We get a huge Huffman tree, so we are skipping reproducing it here in the solution manual. But the observed Huffman tree makes sense with frequently occurring vowels and punctuation getting lower lengths compared to rarely occurring characters like <code>X</code> or unassigned ASCII codes.</li>
</ul>
<ol start="4">
<li>[3 points] What happens if you repeatedly use this compressor on the file (<code>sherlock.txt -&gt; sherlock.txt.huffz -&gt; sherlock.txt.huffz.huffz -&gt; ...</code>)? Does the file size keep getting smaller? Why or why not?</li>
</ol>
<p><strong>Solution</strong>
The size of <code>sherlock.txt.huffz.huffz</code> is <code>217122 bytes</code>, which is larger than <code>sherlock.txt.huffz</code> (<code>207797 bytes</code>). After the first compression, the data becomes more random and less compressible, leading to larger file sizes in subsequent compressions due to various overhead factors. Widely used compressors like gzip and zstd generally guarantee that the file size will not increase by more than some small number of bytes for any random input, which is useful to prevent blowup. This can be achieved by falling back to storing the input uncompressed if the compression does not reduce the size.</p>
<ol start="5">
<li>[2 points] Now run gzip on the file (command: <code>gzip &lt; HWs/HW1/sherlock.txt &gt; HWs/HW1/sherlock.txt.gz</code>) and report the size of the gzip file (<code>sherlock.txt.gz</code>).</li>
</ol>
<p>You will observe that gzip does significantly better than the Huffman coding, even if we ignore the overhead from part 3. While gzip uses Huffman coding internally, it also relies on other ideas that we will learn about in the coming lectures.</p>
<p><strong>Solution</strong></p>
<p>The size of the gzip file is <code>128141 bytes</code>, which is significantly smaller than our Huffman coded file of size <code>207797 bytes</code>.</p>
<p>One of the reasons for this is because gzip compresses data in blocks instead of at the level of single-symbol like we implemented in this problem. Another reason is that gzip uses a more sophisticated algorithm called LZ77. LZ77 is a lossless compression algorithm that is based on the observation that many files contain repeated patterns. LZ77 uses a sliding window to find repeated patterns and replaces them with a pointer to the location of the pattern. This is a very powerful technique that is used in many compression algorithms. We will study LZ77 in more detail in future class and next homework.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ee274-fall-25-homework-2"><a class="header" href="#ee274-fall-25-homework-2">EE274 (Fall 25): Homework-2</a></h1>
<ul>
<li><strong>Focus area:</strong> Lossless Compression</li>
<li><strong>Due Date:</strong> Oct 28, midnight (11:59 PM)</li>
<li><strong>Weightage:</strong> 15%</li>
<li><strong>Total Points:</strong> 115 (Written 85 + Programming 30)</li>
<li><strong>Submission Instructions:</strong> Provided at the end of HW (ensure you read these!)</li>
<li><strong>Submission link:</strong>
<ul>
<li>For written part (85 points): <a href="https://www.gradescope.com/courses/1140353/assignments/6957549">HW2-Written</a></li>
<li>For programming part (30 points): <a href="https://www.gradescope.com/courses/1140353/assignments/6957544">HW2-Code</a></li>
</ul>
</li>
</ul>
<p><em>Please ensure that you follow the <a href="https://communitystandards.stanford.edu/policies-guidance/honor-code">Stanford Honor Code</a> while doing the homework. You are encouraged to discuss the homework with your classmates, but you should write your own solutions and code. You are also encouraged to use the internet to look up the syntax of the programming language, but you should not copy-paste code from the internet. If you are unsure about what is allowed, please ask the instructors.</em></p>
<p><strong>Note for the coding part</strong><br>
Before starting the coding related questions ensure following instructions from HW1 are followed:</p>
<ul>
<li>Ensure you are using the latest version of the SCL <code>EE274/HWs</code> GitHub branch. To ensure run the following command in the SCL repository you cloned from HW1:
<pre><code class="language-sh">git status
</code></pre>
You should get an output saying <code>On branch EE274_Fall25/HWs</code>. If not, run the following command to switch to the correct branch:
<pre><code class="language-sh">git checkout EE274_Fall25/HWs
</code></pre>
Finally ensure you are on the latest commit by running:
<pre><code class="language-sh">git pull
</code></pre>
You should see a <code>HW2</code> folder in the <code>HWs</code> folder.</li>
<li>Ensure you are in the right conda environment you created in <code>HW1</code>. To ensure run the following command:
<pre><code class="language-sh">conda activate ee274_env
</code></pre>
</li>
<li>Before starting, ensure the previous tests are passing as expected. To do so, run following from <code>stanford_compression_library</code> folder:
<pre><code class="language-sh">find scl -name "*.py" -exec py.test -s -v {} +
</code></pre>
and ensure tests except in HW folders pass.</li>
</ul>
<h3 id="q1-camping-trip-20-points"><a class="header" href="#q1-camping-trip-20-points">Q1: Camping Trip (<em>20 points</em>)</a></h3>
<p>During one of the camping trips, Pulkit was given $n$ rope segments of lengths $l_1, l_2,\ldots, l_n$ by Kedar, and was asked to join all the ropes into a single long segment of length $\sum_{i=1}^n l_i$. Pulkit can only join two ropes at a time and the "effort" in joining ropes with length $l_i$ and $l_j$ using a knot is $l_i + l_j$. Pulkit is lazy and would like to minimize his "effort" to join all the segments together.</p>
<p>For instance, consider $n=3$, and $l_1, l_2, l_3 = 4, 5 ,6$. One could first combine $l_1$ and $l_2$ leading to an effort of $4+5=9$, followed by combining $l_1+l_2$ and $l_3$ with effort of $9+6=15$. This corresponds to a total effort of $9+15=24$. On the other hand, if one combines $l_2$ and $l_3$ first, that would lead to a first step cost of $5+6=11$, and total effort of $11+15=26$. Thus the first approach in this example would lead to a lower total effort.</p>
<ol>
<li>[5 points] Do you see any parallels between the problem and one of the prefix-free codes you have learnt about? Please justify your answer.</li>
<li>[5 points] Let $E_{opt}$ be the optimal (minimal) value for the effort required to join all the segments. Without solving the problem, can Pulkit get an estimate of what the $E_{opt}$ would be? Provide a lower bound and an upper bound as a function of the rope lengths.</li>
<li>[10 points] Implement the function <code>compute_minimum_effort()</code> in the file <code>hw2_p1.py</code>.<br />
HINT: One way to solve this is to use one of the prefix-free code implementations in the SCL Library. You may import it under TODO section.</li>
</ol>
<h3 id="q2-generating-random-non-uniform-data-25-points"><a class="header" href="#q2-generating-random-non-uniform-data-25-points">Q2: Generating random non-uniform data (<em>25 points</em>)</a></h3>
<p>Consider the problem of sampling a non-uniform discrete distribution, given samples from a uniform distribution.</p>
<p>Let's assume that we are given a single sample of random variable <code>U</code>, uniformly distributed in the unit interval <code>[0,1)</code> (e.g. <code>U = numpy.random.rand()</code>). The goal is to generate samples from a non-uniform discrete distribution $P$. We will assume that $P$ is a <em>rational distribution</em>. i.e. for any symbol $s$, $P(s) = n_s/M$, where $n_s, M$ are integers.</p>
<ol>
<li>
<p>[5 points] Given a non-uniform rational distribution, we can sample a single random value $X_1 \sim P$ from <code>U</code> by finding the cumulative distribution bin in which the sample <code>U</code> falls since the cumulative distribution also lies between <code>[0,1)</code>. For example, if $P$ is the distribution such as <code>P = {A: 2/7, B: 4/7, C: 1/7}</code>, then we output the symbols <code>A,B,C</code> based on the intervals in which they lie as follows:</p>
<pre><code>if U in [0.0, 2/7) -&gt; output A
if U in [2/7, 6/7) -&gt; output B
if U in [6/7, 1.0) -&gt; output C
</code></pre>
<p>Generalize the above algorithm to any given rational distribution, and describe it in a few lines. For a distribution of alphabet size $k$ (e.g. $k=3$ in the example above), what is the time/memory complexity of your algorithm wrt $k$?</p>
</li>
<li>
<p>[5 points] Complete the function <code>generate_samples_vanilla</code> in <code>hw2_p2.py</code> which takes in a non-uniform  rational distribution <code>P</code> and returns <code>data_size</code> number of samples from it. You can assume that the distribution is given as a dictionary, where the keys are the symbols and the values are the frequency of occurrence in the data. For example, the distribution <code>P = {A: 2/7, B: 4/7, C: 1/7}</code> can be represented as <code>P = Frequencies({'A': 2, 'B': 4, 'C': 1})</code>. Ensure that your code passes the <code>test_vanilla_generator</code> test in <code>hw2_p2.py</code>. Feel free to add more test cases.</p>
</li>
<li>
<p>[5 points] Given a single sample of a uniform random variable <code>U</code>, how can you extend your algorithm in part <code>2.1</code> above to sample $n=2$ i.i.d random values $X_1, X_2 \sim P$? Provide a concrete algorithm for <code>P= {A: 2/7, B: 4/7, C: 1/7}</code>. Generalize your method to an arbitrary number of samples $n$ and describe it in a few sentences.</p>
</li>
<li>
<p>[5 points] Pulkit suggested that we can slightly modify Arithmetic Entropy Coder (<code>AEC</code>) we learnt in the class to sample a potentially infinite number of i.i.d samples $X_1, X_2, \ldots, X_n \sim P$ given any rational distribution $P$ from a single uniform random variable sample <code>U</code>! You can look at Pulkit's implementation <code>generate_samples_aec</code> in <code>hw2_p2.py</code> to get an idea of how to exploit <code>AEC</code>. Can you justify the correctness of Pulkit's method in a few lines?</p>
<p><strong>Note</strong>: Even though we say that we can sample potentially infinite number of samples, in practice we are limited by the precision of floats.</p>
</li>
<li>
<p>[5 points] Now let's say that you have to sample data $X_0, X_1, \ldots, X_n$ from a Markov distribution $Q$. Recall for a Markov Chain, $Q(X_0, X_1, \ldots, X_n) = Q(X_0)Q(X_1|X_0) \ldots Q(X_n|X_{n-1})$. Can you use your technique from <code>Q2.3</code> or the technique suggested by Pulkit in <code>Q2.4</code> to sample any number of samples $n$ with Markov distribution $Q$ from a single uniform random variable sample <code>U in [0,1)</code>? Describe the algorithm in a few lines.</p>
</li>
</ol>
<h3 id="q3-conditional-entropy-20-points"><a class="header" href="#q3-conditional-entropy-20-points">Q3: Conditional Entropy (<em>20 points</em>)</a></h3>
<p>In this problem we will get familiar with conditional entropy and its properties.</p>
<p>We learnt a few properties of conditional entropy in class:</p>
<ul>
<li>$H(X) \geq H(X|Y)$</li>
<li>$H(X,Y) = H(Y) + H(X|Y)$</li>
</ul>
<p>We will use these properties to show some other useful properties about conditional entropy and solve some fun problems!</p>
<ol>
<li>
<p>[5 points] Let $f(X) = Z$ be an arbitrary function which maps $X \in \mathcal{X}$ to a discrete set $Z \in \mathcal{Z}$. Then show that: $H(f(X)|X) = 0$. Can you intuitively explain why this is the case? Make sure to provide both theoretical proof and intuitive explanation.</p>
</li>
<li>
<p>[5 points] Show that $H(f(X)) \leq H(X)$, i.e. processing the data in any form is just going to reduce the entropy. Also, show that the equality holds if the function $f$ is invertible, i.e. if there exists a function $g$ such that $g(f(X)) = X$. Can you intuitively explain why this is the case? Make sure to provide both theoretical proof and intuitive explanation.</p>
</li>
<li>
<p>[4 points] In the HW1 of the 2025 edition of EE274, Pulkit and Shubham had an assignment to compress the Sherlock novel (let's call it $x_{orig}$). Pulkit computed the empirical <code>0th</code> order distribution of the letters and used those with Arithmetic coding to compress $x_{orig}$, and received average codelength $L_1$. While working on the assignment, Shubham accidentally replaced all letters with lowercase (i.e <code>A -&gt; a</code>, <code>B -&gt; b</code> etc.). Let's call this modified text $x_{lowercase}$. Shubham compressed $x_{lowercase}$ with the same algorithm as Pulkit, and got average codelength $L_2$. Do you expect $L_1 \geq L_2$, or $L_2 \geq L_1$. Justify based on properties of Arithmetic coding and <code>Q3.2</code>.</p>
</li>
<li>
<p>[6 points] We say that random variables $X_1, X_2, \ldots, X_n$ are pairwise independent, if any pair of random variables $(X_i,X_j), i \neq j$ are independent. Let $X_1, X_2, X_3$ be three pairwise independent random variables, identically distributed as $Ber({1\over 2})$. Then show that:</p>
<ol>
<li>[3 points] $H(X_1,X_2,X_3) \leq 3$. When is equality achieved?</li>
<li>[3 points] $H(X_1,X_2,X_3) \geq 2$. When is equality achieved?</li>
</ol>
</li>
<li>
<p><em>[NOT GRADED, THINK FOR FUN!]</em> Let $Z_1, Z_2, \ldots, Z_k$ be i.i.d $Ber({1\over 2})$ random variables. Then show that using the $Z_i's$, you can generate $n=2^k - 1$ pairwise independent random variables, identically distributed as $Ber({1\over 2})$.</p>
</li>
</ol>
<h3 id="q4-bits-back-coding-and-rans-45-points"><a class="header" href="#q4-bits-back-coding-and-rans-45-points">Q4: Bits-Back coding and rANS (<em>45 points</em>)</a></h3>
<p>In class, we learnt about rANS. We started with the basic idea of encoding a uniform distribution on <code>{0, ..., 9}</code> (see Introduction section in <a href="https://stanforddatacompressionclass.github.io/notes/lossless_iid/ans.html">notes</a>) and then extended it to non-uniform distributions. In this problem we will look at a different way of thinking about rANS, which will help us understand the modern entropy coder.</p>
<p>Let's start by first generalizing our idea of encoding a uniform distribution on <code>{0, ..., 9}</code> to a uniform distribution on <code>{0, ..., M-1}</code>, where <code>M</code> can be thought of as number of symbols in the alphabet. The encoding works as following:</p>
<pre><code class="language-python">def encode_symbol(state, s, M):
    state = (state * M + s) 
    return state
</code></pre>
<p>The encoder maintains a single state, which we increase when we encode a symbol. Finally, we save the state by simply writing it out in the binary form. This function is implemented in <code>hw2_p4.py</code> as <code>UniformDistEncoder</code>.</p>
<ol>
<li>
<p>[2 points] Write a <code>decode_symbol</code> pseudocode which takes in the encoded state <code>state</code> and <code>M</code> and returns the symbol <code>s</code> and the updated state <code>state</code>. Show that your decoder along with above encoding is lossless, i.e. we can recover the original symbol <code>s</code> from the encoded state <code>state</code> and <code>M</code>.</p>
</li>
<li>
<p>[3 points] Show that given data in <code>{0, ..., M-1}</code>, the encoder/decoder pair can be used to
achieve lossless compression with <code>~ log2(M)</code> bits/symbol as we encode large number of symbols.</p>
</li>
<li>
<p>[5 points] Now, implement your decoder in <code>hw2_p4.py</code> as <code>UniformDistDecoder</code>. Specifically, you just need to implement <code>decode_op</code> function. Ensure that your code passes the <code>test_uniform_coder</code> test in <code>hw2_p4.py</code>. Feel free to add more test cases.</p>
</li>
</ol>
<p>Now, we want to implement a compressor which works well for non-uniform data. We will use the base encode/decode functions from <code>UniformDistEncoder</code>, <code>UniformDistDecoder</code> and approach this problem from a new perspective. In class, we learned $H(X,Z) = H(X) + H(Z|X)$ for any two random-variables $X$ and $Z$. We will use this property to encode the data $X$. Note that this identity implies
$$ H(X) = H(X,Z) - H(Z|X)$$</p>
<p>Interpreting this intuitively, it should be possible to encode data <code>X</code> in the following two steps:</p>
<p>a. Encode <code>X,Z</code> together using a joint distribution <code>P(X,Z)</code>. Assuming an ideal compressor, this will require <code>H(X,Z)</code> bits.</p>
<p>b. Decode <code>Z|X</code> from the encoded state using distribution <code>P(Z|X)</code>. Again, assuming an ideal compressor, this lets us <em>recover</em> <code>H(Z|X)</code> bits.</p>
<p>Step b gives you an intuition for the question name -- <code>bits-back</code>! Here <code>Z</code> is an additional random variable, typically latent variable (latent means hidden).
Be careful while reading this step, as it is not a compressor for <code>X</code> but a decompressor for <code>Z|X</code>! This compressor assumes knowledge of <code>X</code> and decodes <code>Z</code> from the encoded state.
More concretely, we will have an encoder-decoder pair which will work as follows:</p>
<pre><code class="language-python">def encode_symbol(state, X):
    state, Z = decode_zgx(state, X) # decode Z|X from state; knows X. returns Z and updated state.
    state = encode_xz(state, (X, Z)) # use this Z, and known X, to encode X,Z together. returns updated state.
    
    return state


def decode_symbol(state):
    state, (X, Z) = decode_xz(state) # decode X,Z from state. returns X,Z and updated state.
    state = encode_zgx(state, Z, X) # encode Z|X by updating state; knows X. returns updated state.
    
    return state, X
</code></pre>
<p>Note that how the encode step now involves both a decoding and an encoding step from another compressor! This is one of the key idea behind bits-back coding. We will now implement the above encoder/decoder pair for non-uniform data.</p>
<p>To see the idea, let's work out a simple case together. As usual let's assume that the non-uniform input is parametrized by frequencies <code>freq_list[s] for s in {1,..,num_symbols}</code> and <code>M = sum(freq_list)</code>. For instance, if we have 3 symbols <code>A,B,C</code> with probabilities <code>probs = [2/7, 4/7, 1/7]</code>, then we can think of frequencies as <code>freq_list = [2,4,1]</code> and <code>M = 7</code>.
We can also create cumulative frequency dict as we saw in class, i.e. <code>cumul = {A: 0, B: 2, C: 6}</code>.</p>
<p>We will assume <code>Z</code> to be a uniform random variable in <code>{0, ..., M-1}</code>. Then, we can think of <code>X</code> as a function of <code>Z</code> as follows:</p>
<pre><code class="language-python">def X(Z):
    for s in symbols:
        if cumul(s) &lt;= Z &lt; cumul(s) + freq_list[s]:
            return s
</code></pre>
<p>i.e. <code>X</code> is the symbol <code>s</code> such that <code>Z</code> lies in the interval <code>[cumul(s), cumul(s)+freq_list[s])</code>. This is shown in the figure below for the example above.</p>
<p><img src="homeworks/./BB_ANS.png" alt="BB_ANS_1" /></p>
<p>For example if <code>Z = 4</code>, then <code>X = B</code> as <code>cumul(B) = 2 &lt;= 4 &lt; 6 = cumul(B) + freq_list[B]</code>.</p>
<ol start="4">
<li>
<p>[2 points] What are the values of $H(X), H(Z), H(X|Z)$ in the above example?</p>
</li>
<li>
<p>[3 points] What is the distribution and entropy of $Z$ given $X$, i.e. $P(Z|X)$ and $H(Z|X=x)$ in the above example? Note that $H(Z|X=x)$ is a function of <code>x</code> and hence you need to report it for each symbol <code>x</code> in the alphabet <code>A,B,C</code>.</p>
</li>
</ol>
<p>Now, let's implement the encoder/decoder pair for the above example. We will use the following notation:</p>
<ul>
<li><code>state</code> is the encoded state</li>
<li><code>s</code> is the symbol to be encoded (<code>X</code>)</li>
<li><code>freq_list</code> is the list of frequencies of the symbols</li>
<li>We want to code the above scheme utilizing the joint compressors over <code>X, Z</code> (where <code>Z</code> is the latent variable). <code>Z</code> is uniformly distributed in <code>{0, ..., M-1}</code>.
<ul>
<li><code>combined_symbol</code> is the joint random variable <code>Y = (X, Z)</code>. In our example, we only need to encode <code>Z</code> as <code>X</code> is a deterministic function of <code>Z</code>, i.e. <code>combined_symbol = Z</code> and will be in <code>{0, ..., M-1}</code>. For above example, say we want to encode <code>Y = (B, 4)</code>, then we only need to encode <code>Z = 4</code> as <code>X=B</code> is implied.</li>
<li><code>fake_locator_symbol</code> is the value of <code>Z|X=x</code> relative to <code>cumul(x)</code>. This is a function of <code>X=x</code>. For above example of <code>X = B</code>, <code>Z|X=B</code> can be <code>{2, 3, 4, 5}</code> and hence <code>fake_locator_symbol</code> can be <code>{0, 1, 2, 3}</code> respectively as <code>cumul(B) = 2</code>.</li>
<li>Therefore, <code>combined_symbol</code> and <code>fake_locator_symbol</code> allows us to have encoder/decoder pair for both <code>(X,Z)</code> and <code>Z|X</code>. Continuing our example, if we were to encode <code>Y = (X=B, Z=4)</code>, we will encode it as <code>combined_symbol = 4</code> and <code>fake_locator_symbol = 2</code>. Conversely, if we were to decode <code>combined_symbol = 4</code>, we will decode it as <code>Y = (Z=4, X=B)</code> and infer that <code>fake_locator_symbol=2</code>.</li>
</ul>
</li>
</ul>
<ol start="6">
<li>
<p>[5 points] We have implemented the <code>encode_symbol</code> function for <code>NonUniformDistEncoder</code>. Looking at the pseudocode above and the example, explain briefly how it works. Specifically, explain how <code>encode_symbol</code> achieves the relevant <code>decode_zgx</code> and <code>encode_xz</code> functions given in pseudocode in the context of example above.</p>
</li>
<li>
<p>[10 points] Now implement the <code>decode_symbol</code> function for <code>NonUniformDistEncoder</code>. You can use the <code>encode_op</code> and the <code>decode_op</code> from uniform distribution code. Ensure that your code passes the <code>test_non_uniform_coder</code> test in <code>hw2_p4.py</code>.  Feel free to add more test cases.</p>
</li>
<li>
<p>[5 points] Show that for a symbol <code>s</code> with frequency <code>freq[s]</code>, the encoder uses <code>~ log2(M/freq[s])</code> bits and is hence optimal.</p>
</li>
</ol>
<p>Great, we have now implemented a bits-back encoder/decoder pair for a non-uniform distribution. Let us now see how it is equivalent to rANS we studied in class. As a reminder, the rANS base encoding step looks like</p>
<pre><code class="language-python">def rans_base_encode_step(x,s):
   x_next = (x//freq[s])*M + cumul[s] + x%freq[s]
   return x_next
</code></pre>
<ol start="9">
<li>[5 points] Justify that <code>encode_symbol</code> in <code>NonUniformDistEncoder</code> performs the same operation as above.</li>
</ol>
<p>Similarly, as a reminder, the rANS base decoding step looks like</p>
<pre><code class="language-python">def rans_base_decode_step(x):
   # Step I: find block_id, slot
   block_id = x//M
   slot = x%M
   
   # Step II: Find symbol s
   s = find_bin(cumul_array, slot) 
   
   # Step III: retrieve x_prev
   x_prev = block_id*freq[s] + slot - cumul[s]

   return (s,x_prev)
</code></pre>
<ol start="10">
<li>[5 points] Justify that <code>decode_symbol</code> in <code>NonUniformDistEncoder</code> performs the same operation as above.</li>
</ol>
<p>Therefore, bits-back coding is equivalent to rANS! Thinking again in terms of $H(X) = H(X,Z) - H(Z|X)$, it allows us to interpret rANS in terms of latent variables $Z$ in a more intuitive way. This question was highly motivated by the <a href="https://stanford.zoom.us/rec/play/8aIVScMUklFQB5fClrMox6oVUiaMcYqQuhaMotkKI55VWUXLO6nFF8hK8jDVSlvbohSTZ_yG4JUXOsfp.IFXkcq1pGLBPJvmL">IT-Forum talk by James Townsend in 2022</a>.</p>
<h3 id="q5-hw2-feedback-5-points"><a class="header" href="#q5-hw2-feedback-5-points">Q5: HW2 Feedback <em>(5 points)</em></a></h3>
<p>Please answer the following questions, so that we can adjust the difficulty/nature of the problems for the next HWs.</p>
<ol>
<li>How much time did you spent on the HW in total?</li>
<li>Which question(s) did you enjoy the most?</li>
<li>Are the programming components in the HWs helping you understand the concepts better?</li>
<li>Did the HW2 questions complement the lectures?</li>
<li>Any other comments?</li>
</ol>
<h3 id="submission-instructions-1"><a class="header" href="#submission-instructions-1">Submission Instructions</a></h3>
<p>Please submit both the written part and your code on Gradescope in their respective submission links. <strong>We will be using both autograder and manual code evaluation for evaluating the coding parts of the assignments.</strong> You can see the scores of the autograded part of the submissions immediately. For code submission ensure following steps are followed for autograder to work correctly:</p>
<ul>
<li>
<p>As with HW1, you only need to submit the modified files as mentioned in the problem statement.</p>
<ul>
<li>Compress the <code>HW2</code> folder into a zip file. One way to obtain this zip file is by running the following zip command in the <code>HWs</code> folder, i.e.
<pre><code class="language-sh">cd HWs
zip -r HW2.zip HW2
</code></pre>
Note: To avoid autograder errors, ensure that the directory structure is maintained and that you have compressed <code>HW2</code> folder containing the relevant files and not <code>HWs</code> folder, or files inside or something else. Ensure the name of the files inside the folder are exactly as provided in the starter code, i.e. <code>hw2_p2.py</code>, <code>hw2_p4.py</code> etc. In summary, your zip file should be uncompressed to following directory structure (with same names):
<pre><code>HW2
â”œâ”€â”€ hw2_p1.py
â”œâ”€â”€ hw2_p2.py
â””â”€â”€ hw2_p4.py
</code></pre>
</li>
</ul>
</li>
<li>
<p>Submit the zip file (<code>HW2.zip</code> obtained above) on Gradescope Programming Part Submission Link. Ensure that the autograded parts runs and give you correct scores.</p>
</li>
</ul>
<p><strong>Before submitting the programming part on Gradescope, we strongly recommend ensuring that the code runs correctly locally.</strong></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ee274-fall-25-homework-2-1"><a class="header" href="#ee274-fall-25-homework-2-1">EE274 (Fall 25): Homework-2</a></h1>
<ul>
<li><strong>Focus area:</strong> Lossless Compression</li>
<li><strong>Due Date:</strong> Oct 28, midnight (11:59 PM)</li>
<li><strong>Weightage:</strong> 15%</li>
<li><strong>Total Points:</strong> 115 (Written 85 + Programming 30)</li>
</ul>
<h3 id="q1-camping-trip-20-points-1"><a class="header" href="#q1-camping-trip-20-points-1">Q1: Camping Trip (<em>20 points</em>)</a></h3>
<p>During one of the camping trips, Pulkit was given $n$ rope segments of lengths $l_1, l_2,\ldots, l_n$ by Kedar, and was asked to join all the ropes into a single long segment of length $\sum_{i=1}^n l_i$. Pulkit can only join two ropes at a time and the "effort" in joining ropes with length $l_i$ and $l_j$ using a knot is $l_i + l_j$. Pulkit is lazy and would like to minimize his "effort" to join all the segments together.</p>
<p>For instance, consider $n=3$, and $l_1, l_2, l_3 = 4, 5 ,6$. One could first combine $l_1$ and $l_2$ leading to an effort of $4+5=9$, followed by combining $l_1+l_2$ and $l_3$ with effort of $9+6=15$. This corresponds to a total effort of $9+15=24$. On the other hand, if one combines $l_2$ and $l_3$ first, that would lead to a first step cost of $5+6=11$, and total effort of $11+15=26$. Thus the first approach in this example would lead to a lower total effort.</p>
<ol>
<li>
<p>[5 points] Do you see any parallels between the problem and one of the prefix-free codes you have learnt about? Please justify your answer.</p>
<p><strong>Solution</strong></p>
<p>Yes, the problem is connected to <code>Huffman Codes</code>. The cost of joining a rope into the final segment is proportional to number of times it is used in the knotting process. We will need to pay a larger penalty for a rope if it is used for creating a knot earlier, than later. We can visualize the process of creating the final long segment as a prefix-free tree construction where nodes represent the cost of joining the ropes and depth of the node indicates how many times the particular rope corresponding to this node was used for creating the final segment. Mathematically, the problem is same as</p>
<p>$$ \min_{\text{prefix-free code}} \sum_{i=1}^n l_i \cdot d_i $$</p>
<p>where $l_i$ is the length of the rope and $d_i$ is the number of times the rope is knotted.</p>
<p>We have seen this optimization problem before in class when we were constructing <code>Huffman tree</code>!</p>
</li>
<li>
<p>[5 points] Let $E_{opt}$ be the optimal (minimal) value for the effort required to join all the segments. Without solving the problem, can Pulkit get an estimate of what the $E_{opt}$ would be? Provide a lower bound and an upper bound as a function of the rope lengths.</p>
<p><strong>Solution</strong></p>
<p>To map the problem precisely to Huffman Tree construction as seen in class, we need to ensure that we are optimizing over probability distribution over ropes. This can be trivially done by normalizing the rope lengths. Let $p_i = \frac{l_i}{\sum_{i=1}^n l_i}$ be the probability corresponding to Huffman tree for rope $i$. Then, the problem can be mapped to the following optimization problem:</p>
<p>$$ \min_{\text{prefix-free code}} \left (\sum_{i=1}^n p_i \cdot d_i \right) \times \left( \sum_{i=1}^n l_i \right)$$</p>
<p>Note that $\sum_{i=1}^n p_i = l_i$ is constant given rope-lengths. We can now use the fact that the optimal value of the above optimization problem is equal to the entropy of the probability distribution over ropes $H(P)$ scaled by the sum of rope-lengths. Therefore,</p>
<p>$$\left(\sum_{i=1}^n l_i\right) \cdot H(P) \leq E_{opt} \leq \left(\sum_{i=1}^n l_i\right) \cdot \left(H(P) + 1 \right) $$</p>
<p>where $H(P) = \sum_i p_i \log_2 \frac{1}{p_i}$ is the entropy of the probability distribution over ropes as defined above.</p>
</li>
<li>
<p>[10 points] Implement the function <code>compute_minimum_effort()</code> in the file <code>hw2_p1.py</code>.<br />
HINT: One way to solve this is to use one of the prefix-free code implementations in the SCL Library. You may import it under TODO section.</p>
<p><strong>Solution</strong></p>
<pre><code class="language-python">def compute_minimal_effort(rope_lengths_arr: List[float]) -&gt; float:
    """
    rope_lengths_arr -&gt; list of rope lengths (positive floating points)
    output -&gt; the value of the minimum effort in joining all the ropes together 
    """
    effort = 0
    ###########################################################
    # ToDo: add code here to compute the minimal effort
    ###########################################################
    # raise NotImplementedError

    from scl.compressors.huffman_coder import HuffmanTree
    from scl.core.prob_dist import ProbabilityDist

    # create a prob_dist out of the rope_lengths_arr
    rope_lengths_dict = {f"rope_id_{ind}": val for ind, val in enumerate(rope_lengths_arr)}
    prob_dist = ProbabilityDist.normalize_prob_dict(rope_lengths_dict)
    encoding_table = HuffmanTree(prob_dist).get_encoding_table()

    # compute minimal effort as the average codelength of huffman code
    effort = 0
    for rope_id in encoding_table:
        effort += len(encoding_table[rope_id]) * rope_lengths_dict[rope_id]

    ###########################################################
    return effort
</code></pre>
</li>
</ol>
<h3 id="q2-generating-random-non-uniform-data-25-points-1"><a class="header" href="#q2-generating-random-non-uniform-data-25-points-1">Q2: Generating random non-uniform data (<em>25 points</em>)</a></h3>
<p>Consider the problem of sampling a non-uniform discrete distribution, given samples from a uniform distribution.</p>
<p>Let's assume that we are given a single sample of random variable <code>U</code>, uniformly distributed in the unit interval <code>[0,1)</code> (e.g. <code>U = numpy.random.rand()</code>). The goal is to generate samples from a non-uniform discrete distribution $P$. We will assume that $P$ is a <em>rational distribution</em>. i.e. for any symbol $s$, $P(s) = n_s/M$, where $n_s, M$ are integers.</p>
<ol>
<li>
<p>[5 points] Given a non-uniform rational distribution, we can sample a single random value $X_1 \sim P$ from <code>U</code> by finding the cumulative distribution bin in which the sample <code>U</code> falls since the cumulative distribution also lies between <code>[0,1)</code>. For example, if $P$ is the distribution such as <code>P = {A: 2/7, B: 4/7, C: 1/7}</code>, then we output the symbols <code>A,B,C</code> based on the intervals in which they lie as follows:</p>
<pre><code>if U in [0.0, 2/7) -&gt; output A
if U in [2/7, 6/7) -&gt; output B
if U in [6/7, 1.0) -&gt; output C
</code></pre>
<p>Generalize the above algorithm to any given rational distribution, and describe it in a few lines. For a distribution of alphabet size $k$ (e.g. $k=3$ in the example above), what is the time/memory complexity of your algorithm wrt $k$?</p>
<p><strong>Solution</strong></p>
<p>We can use the cumulative distribution function to find the bin in which the sample <code>U</code> falls and output the sample corresponding to that bin. The cumulative distribution function is defined as: $F(x) = \sum_{i=1}^{x} P(i)$. We can find the bin in which the sample <code>U</code> falls by finding the smallest $x$ such that $F(x) &gt; U$ and then output symbol <code>x</code>. We can use <code>binary search</code> for finding the bin. The time complexity of this algorithm is $O(log(k))$, where $k$ is the size of the alphabet. The memory complexity is $O(k)$, since we need to store the cumulative distribution function.</p>
</li>
<li>
<p>[5 points] Complete the function <code>generate_samples_vanilla</code> in <code>hw2_p2.py</code> which takes in a non-uniform  rational distribution <code>P</code> and returns <code>data_size</code> number of samples from it. You can assume that the distribution is given as a dictionary, where the keys are the symbols and the values are the frequency of occurrence in the data. For example, the distribution <code>P = {A: 2/7, B: 4/7, C: 1/7}</code> can be represented as <code>P = Frequencies({'A': 2, 'B': 4, 'C': 1})</code>. Ensure that your code passes the <code>test_vanilla_generator</code> test in <code>hw2_p2.py</code>. Feel free to add more test cases.</p>
<p><strong>Solution</strong></p>
<pre><code class="language-python">def generate_samples_vanilla(freqs: Frequencies, data_size):
    """
    Generate data samples with the given frequencies from uniform distribution [0, 1) using the basic approach
    :param freqs: frequencies of symbols (see Frequencies class)
    :param data_size: number of samples to generate
    :return: DataBlock object with generated samples
    """
    prob_dist = freqs.get_prob_dist()

    # some lists which might be useful
    symbol_list = list(prob_dist.cumulative_prob_dict.keys())
    cumul_list = list(prob_dist.cumulative_prob_dict.values())
    cumul_list.append(1.0)

    generated_samples_list = []  # &lt;- holds generated samples
    for _ in range(data_size):
        # sample a uniform random variable in [0, 1)
        u = np.random.rand()

        ###############################################
        # ADD DETAILS HERE
        ###############################################

        # NOTE: side="right" corresponds to search of type a[i-1] &lt;= t &lt; a[i]
        bin = np.searchsorted(cumul_list, u, side="right") - 1
        s = symbol_list[bin]
        generated_samples_list.append(s)
        ###############################################

    return DataBlock(generated_samples_list)
</code></pre>
</li>
<li>
<p>[5 points] Given a single sample of a uniform random variable <code>U</code>, how can you extend your algorithm in part <code>2.1</code> above to sample $n=2$ i.i.d random values $X_1, X_2 \sim P$? Provide a concrete algorithm for <code>P= {A: 2/7, B: 4/7, C: 1/7}</code>. Generalize your method to an arbitrary number of samples $n$ and describe it in a few sentences.</p>
<p><strong>Solution</strong></p>
<p>We can sample $n$ i.i.d random values $X_1, X_2, \ldots, X_n \sim P$ by first calculating the cumulative distribution corresponding to all possible symbols of block length <code>n</code> and then following the procedure as described above. Note this method scales very poorly with <code>n</code> since the size of the alphabet increases exponentially with <code>n</code>.</p>
</li>
<li>
<p>[5 points] Pulkit suggested that we can slightly modify Arithmetic Entropy Coder (<code>AEC</code>) we learnt in the class to sample a potentially infinite number of i.i.d samples $X_1, X_2, \ldots, X_n \sim P$ given any rational distribution $P$ from a single uniform random variable sample <code>U</code>! You can look at Pulkit's implementation <code>generate_samples_aec</code> in <code>hw2_p2.py</code> to get an idea of how to exploit <code>AEC</code>. Can you justify the correctness of Pulkit's method in a few lines?</p>
<p><strong>Note</strong>: Even though we say that we can sample potentially infinite number of samples, in practice we are limited by the precision of floats.</p>
<p><strong>Solution</strong></p>
<p>Pulkit's solution first samples a random number <code>U</code> in $[0,1)$ and then truncates it to 32 bits. Then it artificially considers the truncated sample as the output of an arithmetic encoder coming from data with same probability as distribution we want to sample from. Finally, it outputs a random bit sequence based on the output of arithmetic decoding on this encoded bitstream. The scheme works because we know that the probability of the uniform random variable lying in an interval in $[0,1)$  is proportional to the interval length and an arithmetic encoder-decoder pair works by dividing the number line between $[0,1)$ in interval lengths proportional to the probability of corresponding n-block symbols. Therefore, we can recover an arbitrary <code>n</code> bit symbol (upto precision of floats) with correct probabilities as given distribution by simply using arithmetic encoding followed by arithmetic decoding.</p>
</li>
<li>
<p>[5 points] Now let's say that you have to sample data $X_0, X_1, \ldots, X_n$ from a Markov distribution $Q$. Recall for a Markov Chain, $Q(X_0, X_1, \ldots, X_n) = Q(X_0)Q(X_1|X_0) \ldots Q(X_n|X_{n-1})$. Can you use your technique from <code>Q2.3</code> or the technique suggested by Pulkit in <code>Q2.4</code> to sample any number of samples $n$ with Markov distribution $Q$ from a single uniform random variable sample <code>U in [0,1)</code>? Describe the algorithm in a few lines.</p>
<p><strong>Solution</strong></p>
<p>We can use either of the techniques in <code>Q2.3</code> or <code>Q2.4</code> to sample from a markov chain by simply modifying the probability distribution of n-block symbols to be the markov chain distribution. For example, we can use technique in <code>Q2.3</code> by calculating the cumulative distribution corresponding to all possible symbols of block length <code>n</code> using the described markov chain probability decomposition and then following the procedure as described in <code>Q2.3</code>. We can use technique in <code>Q2.4</code> by considering the truncated sample as the output of an arithmetic encoder coming from data with same probability as markov chain distribution we want to sample from. In this case, we'll use the conditional distribution at each arithmetic decoding step as we did in lecture 9.</p>
</li>
</ol>
<h3 id="q3-conditional-entropy-20-points-1"><a class="header" href="#q3-conditional-entropy-20-points-1">Q3: Conditional Entropy (<em>20 points</em>)</a></h3>
<p>In this problem we will get familiar with conditional entropy and its properties.</p>
<p>We learnt a few properties of conditional entropy in class:</p>
<ul>
<li>$H(X) \geq H(X|Y)$</li>
<li>$H(X,Y) = H(Y) + H(X|Y)$</li>
</ul>
<p>We will use these properties to show some other useful properties about conditional entropy and solve some fun problems!</p>
<ol>
<li>
<p>[5 points] Let $f(X) = Z$ be an arbitrary function which maps $X \in \mathcal{X}$ to a discrete set $Z \in \mathcal{Z}$. Then show that: $H(f(X)|X) = 0$. Can you intuitively explain why this is the case? Make sure to provide both theoretical proof and intuitive explanation.</p>
<p><strong>Solution</strong></p>
<p>Given a value of $X=x$, $f(X)$ has a deterministic value $f(x)$. Therefore $H(f(X)|X=x) = 0$ and</p>
<p>$H(f(X)|X) = \sum_x p(x) H(f(X) | X = x) = \sum_x 0 $</p>
</li>
<li>
<p>[5 points] Show that $H(f(X)) \leq H(X)$, i.e. processing the data in any form is just going to reduce the entropy. Also, show that the equality holds if the function $f$ is invertible, i.e. if there exists a function $g$ such that $g(f(X)) = X$. Can you intuitively explain why this is the case? Make sure to provide both theoretical proof and intuitive explanation.</p>
<p><strong>Solution</strong></p>
<p>Let's use the chain rule of entropy in two different ways:</p>
<p>$$
\begin{align}
H(X, f(X)) &amp;= H(X) + H(f(X)|X) = H(X) \
H(X, f(X)) &amp;= H(f(X)) + H(X|f(X)) \geq H(f(X))
\end{align}
$$</p>
<p>since $H(f(X)|X) = 0$ and $H(X|f(X)) \geq 0$. Therefore, $H(X) \geq H(f(X))$.</p>
<p>The equality holds when $H(X|f(X)) = 0$. This holds when $X$ is a deterministic function of $f(X)$ implying that $f(X)$ is invertible. Intuitively, if there is a one-to-one mapping between $X$ and $f(X)$, then the entropy of $X$ is the same as the entropy of $f(X)$ as all we are doing is changing the alphabets but not the probability distribution across those alphabets!</p>
</li>
<li>
<p>[4 points] In the HW1 of the 2025 edition of EE274, Pulkit and Shubham had an assignment to compress the Sherlock novel (let's call it $x_{orig}$). Pulkit computed the empirical <code>0th</code> order distribution of the letters and used those with Arithmetic coding to compress $x_{orig}$, and received average codelength $L_1$. While working on the assignment, Shubham accidentally replaced all letters with lowercase (i.e <code>A -&gt; a</code>, <code>B -&gt; b</code> etc.). Let's call this modified text $x_{lowercase}$. Shubham compressed $x_{lowercase}$ with the same algorithm as Pulkit, and got average codelength $L_2$. Do you expect $L_1 \geq L_2$, or $L_2 \geq L_1$. Justify based on properties of Arithmetic coding and <code>Q3.2</code>.</p>
<p><strong>Solution</strong></p>
<p>We know that $H(X) \geq H(f(X))$. Since Shubham replaced all capital letters with lowercase, the operation can only reduce the entropy, i.e. $H(x_{orig}) \geq H(x_{lowercase})$. We also know that arithmetic coder achieves entropy of any given probability distribution, and hence, $L_1 \geq L_2$.</p>
</li>
<li>
<p>[6 points] We say that random variables $X_1, X_2, \ldots, X_n$ are pairwise independent, if any pair of random variables $(X_i,X_j), i \neq j$ are independent. Let $X_1, X_2, X_3$ be three pairwise independent random variables, identically distributed as $Ber({1\over 2})$. Then show that:</p>
<ol>
<li>[3 points] $H(X_1,X_2,X_3) \leq 3$. When is equality achieved?</li>
<li>[3 points] $H(X_1,X_2,X_3) \geq 2$. When is equality achieved?</li>
</ol>
<p><strong>Solution</strong></p>
<p>Using chain rule of entropy and the fact that $X_i$ are pairwise independent, we have:<br />
$$
\begin{align}
H(X_1,X_2,X_3) &amp;= H(X_1) + H(X_2 | X_1) + H(X_3 | X_1, X_2) \
&amp;= H(X_1) + H(X_2) + H(X_3 | X_1, X_2) \
&amp;= 1 + 1 + H(X_3 | X_1, X_2) \
&amp;= 2 + H(X_3 | X_1, X_2)
\end{align}
$$</p>
<p>where $H(X_2 | X_1) = H(X_1)$ since $X_2$ and $X_1$ are independent. But note that, $H(X_3 | X_1, X_2)$ need not be always $0$ as our random variables are pairwise independent but need not be mutually independent! Third line follows from the fact that $X_i$ are $Bern(0.5)$ and hence $H(X_i) = 1$.</p>
<p>a. To show this note that: $H(X_3 | X_1, X_2) \leq H(X_3) = 1$, since conditioning reduces entropy. This equality is achieved when $X_3$ is independent of $X_1$ and $X_2$, i.e. random variables are mutually independent.<br />
b. To show this note that: $H(X_3 | X_1, X_2) \geq 0$, since entropy is always positive. From <code>Q1.1</code> this equality is achieved when $X_3$ is a function of $X_1, X_2$. Since the marginal distribution of $X_3$ is $Bern(0.5)$, you can show that only function which satisfies this property is $X_3 = X_1 \oplus X_2$.</p>
</li>
<li>
<p><em>[NOT GRADED, THINK FOR FUN!]</em> Let $Z_1, Z_2, \ldots, Z_k$ be i.i.d $Ber({1\over 2})$ random variables. Then show that using the $Z_i's$, you can generate $n=2^k - 1$ pairwise independent random variables, identically distributed as $Ber({1\over 2})$.</p>
<p><strong>Solution</strong></p>
<p>Consider all possible xor combinations of $Z_i$, i.e. $Z_1, Z_2, \ldots, Z_k, Z_1 \oplus Z_2, Z_1 \oplus Z_3, Z_2 \oplus Z_3, \ldots, Z_1 \oplus Z_2 \oplus Z_3 \oplus \ldots \oplus Z_k$. We can see that each of these random variables are pairwise independent since $Z_i$ are i.i.d. and $Bern(0.5)$. Hence, we can generate $n=2^k - 1$ pairwise independent random variables, identically distributed as $Bern(0.5)$.</p>
</li>
</ol>
<h3 id="q4-bits-back-coding-and-rans-45-points-1"><a class="header" href="#q4-bits-back-coding-and-rans-45-points-1">Q4: Bits-Back coding and rANS (<em>45 points</em>)</a></h3>
<p>In class, we learnt about rANS. We started with the basic idea of encoding a uniform distribution on <code>{0, ..., 9}</code> (see Introduction section in <a href="https://stanforddatacompressionclass.github.io/notes/lossless_iid/ans.html">notes</a>) and then extended it to non-uniform distributions. In this problem we will look at a different way of thinking about rANS, which will help us understand the modern entropy coder.</p>
<p>Let's start by first generalizing our idea of encoding a uniform distribution on <code>{0, ..., 9}</code> to a uniform distribution on <code>{0, ..., M-1}</code>, where <code>M</code> can be thought of as number of symbols in the alphabet. The encoding works as following:</p>
<pre><code class="language-python">def encode_symbol(state, s, M):
    state = (state * M + s) 
    return state
</code></pre>
<p>The encoder maintains a single state, which we increase when we encode a symbol. Finally, we save the state by simply writing it out in the binary form. This function is implemented in <code>hw2_p4.py</code> as <code>UniformDistEncoder</code>.</p>
<ol>
<li>
<p>[2 points] Write a <code>decode_symbol</code> pseudocode which takes in the encoded state <code>state</code> and <code>M</code> and returns the symbol <code>s</code> and the updated state <code>state</code>. Show that your decoder along with above encoding is lossless, i.e. we can recover the original symbol <code>s</code> from the encoded state <code>state</code> and <code>M</code>.</p>
<p><strong>Solution</strong></p>
<pre><code class="language-python">def decode_symbol(state, M):
    s = state % M
    state = state // M
    return s, state
</code></pre>
<p>Since the state input into the decoder is <code>state = state_prev * M + s</code> with <code>0 &lt;= s &lt; M</code>, then <code>(state_prev * M + s) % M = s</code>. Therefore, we can recover the original symbol <code>s</code> from the encoded state state and <code>M</code>. And the previous state is then just <code>(state_prev * M + s) // M</code>.</p>
</li>
<li>
<p>[3 points] Show that given data in <code>{0, ..., M-1}</code>, the encoder/decoder pair can be used to
achieve lossless compression with <code>~ log2(M)</code> bits/symbol as we encode large number of symbols.</p>
<p><strong>Solution</strong></p>
<p>Just like in the case where <code>M=10</code> we discuss in the <a href="https://stanforddatacompressionclass.github.io/notes/lossless_iid/ans.html">course notes</a>, the size of the state increases by a factor of<code> M</code> for every symbol we encode, plus an addition of <code>s</code>. After some time, this <code>s</code> term will be small relative to the state, which is several powers of <code>M</code> larger, so in the limit of a large number so symbols, we can ignore the <code>s</code> term. and the number of bits required to encode the state is <code>log2(M)</code> times the number of symbols encoded.</p>
</li>
<li>
<p>[5 points] Now, implement your decoder in <code>hw2_p4.py</code> as <code>UniformDistDecoder</code>. Specifically, you just need to implement <code>decode_op</code> function. Ensure that your code passes the <code>test_uniform_coder</code> test in <code>hw2_p4.py</code>. Feel free to add more test cases.</p>
<p><strong>Solution</strong></p>
<pre><code class="language-python">def decode_op(state, num_symbols):
    '''
    :param state: state
    :param num_symbols: parameter M in HW write-up
    :return: s, state_prev: symbol and previous state
    '''
    s = state_prev = None

    ####################################################
    # ADD CODE HERE
    # raise NotImplementedError
    ####################################################
    s = state % num_symbols
    state_prev = state // num_symbols

    # Make sure you use integer operations and not float operations to avoid precision issues! 
    assert np.issubdtype(type(s), np.integer)
    assert np.issubdtype(type(state_prev), np.integer)

    return s, state_prev
</code></pre>
</li>
</ol>
<p>Now, we want to implement a compressor which works well for non-uniform data. We will use the base encode/decode functions from <code>UniformDistEncoder</code>, <code>UniformDistDecoder</code> and approach this problem from a new perspective. In class, we learned $H(X,Z) = H(X) + H(Z|X)$ for any two random-variables $X$ and $Z$. We will use this property to encode the data $X$. Note that this identity implies
$$ H(X) = H(X,Z) - H(Z|X)$$</p>
<p>Interpreting this intuitively, it should be possible to encode data <code>X</code> in the following two steps:</p>
<p>a. Encode <code>X,Z</code> together using a joint distribution <code>P(X,Z)</code>. Assuming an ideal compressor, this will require <code>H(X,Z)</code> bits.</p>
<p>b. Decode <code>Z|X</code> from the encoded state using distribution <code>P(Z|X)</code>. Again, assuming an ideal compressor, this lets us <em>recover</em> <code>H(Z|X)</code> bits.</p>
<p>Step b gives you an intuition for the question name -- <code>bits-back</code>! Here <code>Z</code> is an additional random variable, typically latent variable (latent means hidden).
Be careful while reading this step, as it is not a compressor for <code>X</code> but a decompressor for <code>Z|X</code>! This compressor assumes knowledge of <code>X</code> and decodes <code>Z</code> from the encoded state.
More concretely, we will have an encoder-decoder pair which will work as follows:</p>
<pre><code class="language-python">def encode_symbol(state, X):
    state, Z = decode_zgx(state, X) # decode Z|X from state; knows X. returns Z and updated state.
    state = encode_xz(state, (X, Z)) # use this Z, and known X, to encode X,Z together. returns updated state.
    
    return state


def decode_symbol(state):
    state, (X, Z) = decode_xz(state) # decode X,Z from state. returns X,Z and updated state.
    state = encode_zgx(state, Z, X) # encode Z|X by updating state; knows X. returns updated state.
    
    return state, X
</code></pre>
<p>Note that how the encode step now involves both a decoding and an encoding step from another compressor! This is one of the key idea behind bits-back coding. We will now implement the above encoder/decoder pair for non-uniform data.</p>
<p>To see the idea, let's work out a simple case together. As usual let's assume that the non-uniform input is parametrized by frequencies <code>freq_list[s] for s in {1,..,num_symbols}</code> and <code>M = sum(freq_list)</code>. For instance, if we have 3 symbols <code>A,B,C</code> with probabilities <code>probs = [2/7, 4/7, 1/7]</code>, then we can think of frequencies as <code>freq_list = [2,4,1]</code> and <code>M = 7</code>.
We can also create cumulative frequency dict as we saw in class, i.e. <code>cumul = {A: 0, B: 2, C: 6}</code>.</p>
<p>We will assume <code>Z</code> to be a uniform random variable in <code>{0, ..., M-1}</code>. Then, we can think of <code>X</code> as a function of <code>Z</code> as follows:</p>
<pre><code class="language-python">def X(Z):
    for s in symbols:
        if cumul(s) &lt;= Z &lt; cumul(s) + freq_list[s]:
            return s
</code></pre>
<p>i.e. <code>X</code> is the symbol <code>s</code> such that <code>Z</code> lies in the interval <code>[cumul(s), cumul(s)+freq_list[s])</code>. This is shown in the figure below for the example above.</p>
<p><img src="homeworks/./BB_ANS.png" alt="BB_ANS_1" /></p>
<p>For example if <code>Z = 4</code>, then <code>X = B</code> as <code>cumul(B) = 2 &lt;= 4 &lt; 6 = cumul(B) + freq_list[B]</code>.</p>
<ol start="4">
<li>
<p>[2 points] What are the values of $H(X), H(Z), H(X|Z)$ in the above example?</p>
<p><strong>Solution</strong>
$$H(X) = -(\frac{2}{7}\log{\frac{2}{7}}+\frac{4}{7}\log{\frac{4}{7}}+\frac{1}{7}\log{\frac{1}{7}}) \approx 1.378783$$
$$H(Z) = -\log{\frac{1}{M}} = \log{M} \approx 2.81$$
$$H(X|Z) = \sum_{z =0}^{M-1}p(z)H(X|Z=z) = -\sum_{z = 0}^{M-1}p(z)\sum_{x \in X}p(x|z)\log{p(x|z)}$$</p>
<p>Notice that $p(x|z) = 1$ if $x = X(z)$ and $0$ otherwise. If $p(x|z) = 0$, then the term $p(x|z)\log{p(x|z)}$ in the sum is also $0$, and if $p(x|z) = 1$, then $\log{p(x|z)}=0$, and the term also disappears, since this is true for all terms in the sum, $H(X|Z) = 0$. In general, for some variable, $Y$, $H(f(Y)|Y) = 0$ for any function $f$. This is because f(Y) is completely determined by $Y$, so given $Y$, there is no uncertainty in $f(Y)$ and the entropy is 0.</p>
</li>
<li>
<p>[3 points] What is the distribution and entropy of $Z$ given $X$, i.e. $P(Z|X)$ and $H(Z|X=x)$ in the above example? Note that $H(Z|X=x)$ is a function of <code>x</code> and hence you need to report it for each symbol <code>x</code> in the alphabet <code>A,B,C</code>.</p>
<p><strong>Solution</strong>
Notice that given a value of $X=x$, $Z$ is uniformly distributed over the range of $x$. So,</p>
<p>$$
\begin{align}
P(Z=z \mid X=A) &amp;= \begin{cases}\frac{1}{2} &amp; z \in[0,1] \ 0 &amp; \text { else }\end{cases}
\
P(Z=z \mid X=B) &amp;= \begin{cases}\frac{1}{4} &amp; z \in[2,3,4,5] \ 0 &amp; \text { else }\end{cases}
\
P(Z=z \mid X=C) &amp;= \begin{cases}1 &amp; z \in[6] \ 0 &amp; \text { else }\end{cases}
\end{align}
$$</p>
<p>This means that the conditional entropy $H(Z|X)$ for $X \in [A,B,C]$ is just that of a uniform distribution over 2, 4, and 1 possibilities, respectively. So,</p>
<p>$$
\begin{align}
H(Z|X=A) &amp;= -\log_2{\frac{1}{2}} = \log_2{2} = 1 \
H(Z|X=B) &amp;= -\log_2{\frac{1}{4}} = \log_2{4} = 2 \
H(Z|X=C) &amp;=-\log_2{1} = 0
\end{align}
$$</p>
<p>(bonus) The full calculation of $H(Z|X)$ is shown below:</p>
<p>$$
\begin{align}
H(Z|X) &amp;= \sum_{x \in \left[A,B,C\right]}p(x)H(Z|X=x) = -\sum_{x \in \left[A,B,C\right]}p(x)\sum_{z = 0}^{M-1}p(z|x)\log{p(z|x)}\
&amp;= p(x=A)H(Z|X=A) + p(x=B)H(Z|X=B) + p(x=C)H(Z|X=C)\
&amp;= -(\frac{2}{7}(\log_2{\frac{1}{2}})+\frac{4}{7}(\log_2{\frac{1}{4}})+\frac{1}{7}(\log_2{1}))\
&amp;= \frac{2}{7}+\frac{4}{7}*2 = \frac{10}{7} \approx 1.428571
\end{align}
$$</p>
</li>
</ol>
<p>Now, let's implement the encoder/decoder pair for the above example. We will use the following notation:</p>
<ul>
<li><code>state</code> is the encoded state</li>
<li><code>s</code> is the symbol to be encoded (<code>X</code>)</li>
<li><code>freq_list</code> is the list of frequencies of the symbols</li>
<li>We want to code the above scheme utilizing the joint compressors over <code>X, Z</code> (where <code>Z</code> is the latent variable). <code>Z</code> is uniformly distributed in <code>{0, ..., M-1}</code>.
<ul>
<li><code>combined_symbol</code> is the joint random variable <code>Y = (X, Z)</code>. In our example, we only need to encode <code>Z</code> as <code>X</code> is a deterministic function of <code>Z</code>, i.e. <code>combined_symbol = Z</code> and will be in <code>{0, ..., M-1}</code>. For above example, say we want to encode <code>Y = (B, 4)</code>, then we only need to encode <code>Z = 4</code> as <code>X=B</code> is implied.</li>
<li><code>fake_locator_symbol</code> is the value of <code>Z|X=x</code> relative to <code>cumul(x)</code>. This is a function of <code>X=x</code>. For above example of <code>X = B</code>, <code>Z|X=B</code> can be <code>{2, 3, 4, 5}</code> and hence <code>fake_locator_symbol</code> can be <code>{0, 1, 2, 3}</code> respectively as <code>cumul(B) = 2</code>.</li>
<li>Therefore, <code>combined_symbol</code> and <code>fake_locator_symbol</code> allows us to have encoder/decoder pair for both <code>(X,Z)</code> and <code>Z|X</code>. Continuing our example, if we were to encode <code>Y = (X=B, Z=4)</code>, we will encode it as <code>combined_symbol = 4</code> and <code>fake_locator_symbol = 2</code>. Conversely, if we were to decode <code>combined_symbol = 4</code>, we will decode it as <code>Y = (Z=4, X=B)</code> and infer that <code>fake_locator_symbol=2</code>.</li>
</ul>
</li>
</ul>
<ol start="6">
<li>
<p>[5 points] We have implemented the <code>encode_symbol</code> function for <code>NonUniformDistEncoder</code>. Looking at the pseudocode above and the example, explain briefly how it works. Specifically, explain how <code>encode_symbol</code> achieves the relevant <code>decode_zgx</code> and <code>encode_xz</code> functions given in pseudocode in the context of example above.</p>
<p><strong>Solution</strong>
encode_symbol is shown below:</p>
</li>
</ol>
<pre><code class="language-python">def encode_symbol(self, x, state):
    '''
    :param x: symbol
    :param state: current state
    :return: state: updated state
    '''
    # decode a "fake" uniform sample between [0, freq[x]]
    fake_locator_symbol, state = decode_op(state, self.freq.freq_dict[x])
    # print(s, fake_locator_symbol, state)
    # create a new symbol
    combined_symbol = self.freq.cumulative_freq_dict[x] + fake_locator_symbol

    # encode the new symbol
    state = encode_op(state, combined_symbol, self.freq.total_freq)
    # print(state)
    # print("*" * 5)
    return state
</code></pre>
<p>By using <code>decode_op</code> from our <code>UniformDistDecoder</code>, we can decode a "fake" uniform sample between <code>[0, freq[x]]</code>. We then add this to the cumulative frequency of <code>x</code> to get the value of <code>Z</code>, which we encode as combined_symbol (see the first bullet point above). We can now encode the state, <code>encode_op</code> with the value of <code>Z</code> being used for <code>s</code>. <code>decode_zgx</code> is accomplished by the first two lines where we pass the function the state and the value of <code>X</code> and decode <code>Z</code> (which is stored as <code>combined_symbol</code>), and update the state. Then, <code>encode_xz</code> is accomplished by the last line by feeding <code>encode_op</code> the state, <code>Z</code> and <code>X</code>.</p>
<ol start="7">
<li>
<p>[10 points] Now implement the <code>decode_symbol</code> function for <code>NonUniformDistEncoder</code>. You can use the <code>encode_op</code> and the <code>decode_op</code> from uniform distribution code. Ensure that your code passes the <code>test_non_uniform_coder</code> test in <code>hw2_p4.py</code>.  Feel free to add more test cases.</p>
<p><strong>Solution</strong></p>
<pre><code class="language-python">    def decode_symbol(self, state):
        '''
        :param state: current state
        :return: (s, state): symbol and updated state
        '''
        #################################################
        # ADD CODE HERE
        # a few relevant helper functions are implemented:
        # self.find_bin, self.freq.total_freq, self.freq.cumulative_freq_dict, self.freq.freq_dict, self.freq.alphabet

        # Step 1: decode (s, z) using joint distribution; (i) decode combined symbol, (ii) find s
        # Step 2: encode z given s; (i) find fake locator symbol, (ii) encode back the fake locator symbol

        # You should be able to use encode_op, decode_op to encode/decode the uniformly distributed symbols

        # decode combined symbol
        combined_symbol, state = decode_op(state, self.freq.total_freq)
        cum_prob_list = list(self.freq.cumulative_freq_dict.values())
        decoded_ind = self.find_bin(cum_prob_list, combined_symbol)
        s = self.freq.alphabet[decoded_ind]

        # find fake locator symbol
        fake_locator_symbol = combined_symbol - self.freq.cumulative_freq_dict[s]

        # encode back the fake locator symbol
        state = encode_op(state, fake_locator_symbol, self.freq.freq_dict[s])

        #################################################
        return s, state
</code></pre>
</li>
<li>
<p>[5 points] Show that for a symbol <code>s</code> with frequency <code>freq[s]</code>, the encoder uses <code>~ log2(M/freq[s])</code> bits and is hence optimal.</p>
<p><strong>Solution</strong></p>
<p>For a symbol, <code>s</code>, the function <code>encode_symbol(s,state)</code> first divides the input state by <code>freq[s]</code> in the line <code>fake_locator_symbol, state = decode_op(state, self.freq.freq_dict[s])</code>. Then, the state is multiplied by <code>M</code> in the line <code>state = encode_op(state, combined_symbol, self.freq.total_freq)</code>. So, in each step of the encoding, we scale by <code>M/freq[s]</code>, so in the limit of a large number of symbols, we require <code>log2[M/freq[s]]</code> bits to encode a state. The expected number of bits per symbol under the true distribution is therefore equal to the entropy of the source, which makes the encoder optimal.</p>
</li>
</ol>
<p>Great, we have now implemented a bits-back encoder/decoder pair for a non-uniform distribution. Let us now see how it is equivalent to rANS we studied in class. As a reminder, the rANS base encoding step looks like</p>
<pre><code class="language-python">def rans_base_encode_step(x,s):
   x_next = (x//freq[s])*M + cumul[s] + x%freq[s]
   return x_next
</code></pre>
<ol start="9">
<li>
<p>[5 points] Justify that <code>encode_symbol</code> in <code>NonUniformDistEncoder</code> performs the same operation as above.</p>
<p><strong>Solution</strong>
Let's walk through the transformation of the state in <code>encode_symbol</code> step-by-step. First off, <code>fake_locator_symbol, state = decode_op(state, self.freq.freq_dict[s])</code> proforms the transformation <code>state -&gt; state // freq[s]</code> and gives <code>fake_locator_symbol = state%freq[s]</code>. Then the line <code>state = encode_op(state, combined_symbol, self.freq.total_freq)</code> performs the transformation <code>state // freq[s] -&gt; (state // freq[s])*M + combined_symbol</code>, but <code>combined_symbol = self.freq.cumulative_freq_dict[s] + fake_locator_symbol = cumul[s] + state%freq[s]</code>, so overall we have <code>state -&gt; (state // freq[s])*M + cumul[s] + state%freq[s]</code>. This is exactly the same as the rANS base encoding step, so <code>encode_symbol</code> performs the same operation as the rANS base encoding step.</p>
</li>
</ol>
<p>Similarly, as a reminder, the rANS base decoding step looks like</p>
<pre><code class="language-python">def rans_base_decode_step(x):
   # Step I: find block_id, slot
   block_id = x//M
   slot = x%M
   
   # Step II: Find symbol s
   s = find_bin(cumul_array, slot) 
   
   # Step III: retrieve x_prev
   x_prev = block_id*freq[s] + slot - cumul[s]

   return (s,x_prev)
</code></pre>
<ol start="10">
<li>
<p>[5 points] Justify that <code>decode_symbol</code> in <code>NonUniformDistEncoder</code> performs the same operation as above.</p>
<p><strong>Solution</strong>
<code>combined_symbol = state % M</code> and <code>state = state//M</code>, so <code>combined_symbol = slot</code> and <code>state = block_id</code>. Then <code>decoded_ind = self.find_bin(cum_prob_list, combined_symbol)</code> and  <code>s = self.freq.alphabet[decoded_ind]</code> implement <code>s = find_bin(cumul_array, slot)</code>. Finally, <code>state = encode_op(state, fake_locator_symbol, self.freq.freq_dict[s])</code> performs the transformation <code>state -&gt; state*freq[s] + fake_locator_symbol</code>, but <code>fake_locator_symbol = combined_symbol - self.freq.cumulative_freq_dict[s]</code>, so we have <code>state -&gt; (state//M)*freq[s] + state % M - cumul[s]</code> for the whole function, which is the same as the rANS base decoding step.</p>
</li>
</ol>
<p>Therefore, bits-back coding is equivalent to rANS! Thinking again in terms of $H(X) = H(X,Z) - H(Z|X)$, it allows us to interpret rANS in terms of latent variables $Z$ in a more intuitive way. This question was highly motivated by the <a href="https://stanford.zoom.us/rec/play/8aIVScMUklFQB5fClrMox6oVUiaMcYqQuhaMotkKI55VWUXLO6nFF8hK8jDVSlvbohSTZ_yG4JUXOsfp.IFXkcq1pGLBPJvmL">IT-Forum talk by James Townsend in 2022</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ee274-fall-25-homework-3"><a class="header" href="#ee274-fall-25-homework-3">EE274 (Fall 25): Homework-3</a></h1>
<ul>
<li><strong>Focus area:</strong> Context-based compression and LZ77</li>
<li><strong>Due Date:</strong> Nov 11, midnight (11:59 PM)</li>
<li><strong>Weightage:</strong> 15%</li>
<li><strong>Total Points:</strong> 150</li>
<li><strong>Submission Instructions:</strong> Provided at the end of HW (ensure you read these!)</li>
<li><strong>Submission link:</strong>
<ul>
<li>For written part (110): <a href="https://www.gradescope.com/courses/1140353/assignments/7060416">HW3-Written</a></li>
<li>For programming part (40): <a href="https://www.gradescope.com/courses/1140353/assignments/7060414">HW3-Code</a></li>
</ul>
</li>
</ul>
<p><em>Please ensure that you follow the <a href="https://communitystandards.stanford.edu/policies-guidance/honor-code">Stanford Honor Code</a> while doing the homework. You are encouraged to discuss the homework with your classmates, but you should write your own solutions and code. You are also encouraged to use the internet to look up the syntax of the programming language, but you should not copy-paste code from the internet. If you are unsure about what is allowed, please ask the instructors.</em></p>
<p><strong>Note for the coding part</strong><br>
Before starting the coding related questions ensure following instructions from HW1 are followed:</p>
<ul>
<li>Ensure you are using the latest version of the SCL <code>EE274/HWs</code> GitHub branch. To ensure run the following command in the SCL repository you cloned from HW1:
<pre><code class="language-sh">git status
</code></pre>
You should get an output saying <code>On branch EE274_Fall25/HWs</code>. If not, run the following command to switch to the correct branch:
<pre><code class="language-sh">git checkout EE274_Fall25/HWs
</code></pre>
Finally ensure you are on the latest commit by running:
<pre><code class="language-sh">git pull
</code></pre>
You should see a <code>HW3</code> folder in the <code>HWs</code> folder.</li>
<li>Ensure you are in the right conda environment you created in <code>HW1</code>. To ensure run the following command:
<pre><code class="language-sh">conda activate ee274_env
</code></pre>
</li>
<li>Before starting, ensure the previous tests are passing as expected. To do so, run following from <code>stanford_compression_library</code> folder:
<pre><code class="language-sh">find scl -name "*.py" -exec py.test -s -v {} +
</code></pre>
and ensure tests except in HW folders pass.</li>
</ul>
<h3 id="q1-lz77-compression-for-small-data-35-points"><a class="header" href="#q1-lz77-compression-for-small-data-35-points">Q1 LZ77 compression for small data (<em>35 points</em>)</a></h3>
<p>In this problem, we will understand how LZ77 compression performs on small files and how to improve its performance. Recall that the LZ77 algorithm looks for matches in a window storing the previously seen data and then encodes the match lengths, match offsets and unmatched characters (literals). We use the LZ77 implementation provided in SCL for the experiments below, and you can take a look at the code <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/compressors/lz77.py">here</a> to understand the details better. We have provided a set of small json files in the <code>p1_data/github_data</code> directory. Run the script <code>python hw3_p1.py -i p1_data/github_data</code> in scl/HWs/HW3 folder, which produces the following output:</p>
<pre><code>Compressing without using a seed input
Number of files: 134
Total uncompressed size (in bits): 956272
Normalized uncompressed size (in avg. bits/file): 7136
Total size after compressing the files individually (in bits): 363774
Total size after compressing the files jointly (in bits): 71795
</code></pre>
<p>Ignore the first line for a moment. We see that we have <code>134</code> relatively small files with average size of <code>7136 bits</code>. If we compress the files individually and then sum the sizes, we get a total of <code>363774 bits</code>, whereas if we concatenate the files and then compress them as a single block ("jointly") the compressed size is just <code>71795 bits</code>!</p>
<ol>
<li>
<p>[4 points] Give two reasons why concatenating the files together provides a reduction in file size.</p>
<p><strong>HINT</strong>: One of the reasons is applicable even if the files were from completely distinct sources and had no similarity whatsoever.</p>
</li>
</ol>
<p>Ideally one would just combine these small files into bigger batches for the best compression. However, we sometimes need to store, transmit or compress small files. For example, we need to transmit short JSON messages/network packets between servers where latency is important, and we can't batch requests. Another common use case is when databases use small data pages for fast random access. Small files also show up when we are working with resource constrained devices that cannot hold large files.</p>
<p>To improve the compression on small files, we consider a slight modification of the LZ77 algorithm we studied in class. Both the compressor and the decompressor now take an additional seed input, which is just a sequence of bytes. The idea is simple: instead of starting the LZ77 compressor with an empty window, we instead initialize the window with a seed input (and appropriately update the other indexing data structures). The same seed input should be used during compression and decompression to enable recovery.</p>
<p>The overall system architecture needs to maintain these seed inputs (which might be specific to particular data categories), and make sure the encoders and decoders can access these. The seed inputs are usually constrained to be small to avoid extra overhead.</p>
<p>We provide a sample seed input for the above dataset in <code>p1_data/github_data_seed_input.txt</code>. Run <code>python hw3_p1.py -i p1_data/github_data -s p1_data/github_data_seed_input.txt</code> to obtain the following:</p>
<pre><code>Loading seed input from p1_data/github_data_seed_input.txt
Number of files: 134
Total uncompressed size (in bits): 956272
Normalized uncompressed size (in avg. bits/file): 7136
Total size after compressing the files individually (in bits): 224738
Total size after compressing the files jointly (in bits): 70678
</code></pre>
<ol start="2">
<li>
<p>[6 points] We see a significant reduction in the total size for compressing the files individually (<code>363774</code> bits to <code>224738</code> bits). Based on your understanding of the LZ77 algorithm and your answer to <code>Q1.1</code>, explain why this is the case. You might find it useful to look both at the json files in <code>p1_data/github_data/</code> and the seed input in <code>p1_data/github_data_seed_input.txt</code>.</p>
</li>
<li>
<p>[2 points] Why is the impact of using the seed input negligible when we compress the files jointly?</p>
</li>
<li>
<p>[3 points] The provided seed input file is less than 1 KB in size. If you were allowed to choose an arbitrarily large seed input, how might you design it to minimize the compressed size for this specific dataset.</p>
<p><strong>HINT</strong>: Think about the best case scenario for LZ77 parsing - longest possible matches and no literals.</p>
</li>
<li>
<p>[10 points] Now you will create a seed input for another dataset provided in the <code>p1_data/pokemon_data</code> directory. We will evaluate your submissions on a test dataset which has similar files as the <code>p1_data/pokemon_data</code> directory. Your submission should satisfy the following:</p>
<ul>
<li>name the seed input file as <code>p1_data/pokemon_data_seed_input.txt</code></li>
<li>the seed input file should be less than 1 KB (1000 B) large. You can check the file size in bytes by running <code>wc -c p1_data/pokemon_data_seed_input.txt</code>.</li>
<li>the total size for compressing the files individually should reduce to at least 2x when using the seed input (vs. when not using a seed input) for both the <code>pokemon_data</code> set and the autograder submission. For example, if the <code>Total size after compressing the files individually</code> is <code>308031 bits</code> without seed input, then with your seed input it should be at most <code>154015 bits</code>.</li>
<li>A couple hints to help you achieve best results: (i) try to use similar JSON format and formatting in your seed input file as in the pokemon data files - this includes using the same 2-space indendation, and (ii) (for Windows users) make sure your seed input uses LF (<code>\n</code>) line-endings and not CRLF (<code>\r\n</code>) - verify your editor is not changing this automatically.</li>
</ul>
</li>
<li>
<p>[10 points] Christmas is approaching and <a href="https://en.wikipedia.org/wiki/The_Gift_of_the_Magi">Jim knows that Della</a> is going to buy a thoughtful gift for him. Unable to control his curiosity and being a part-time hacker, he has decided to spy on Dellaâ€™s internet search patterns by executing a side-channel attack. As shown in the diagram below, he is able to inject strings into Dellaâ€™s http requests, and once Della makes a search request he can monitor the LZ77 compressed size of the response. Here the response includes his injected string along with the gift description, allowing him to infer some side-channel information about the search. <em>Don't take the story too seriously from a internet security standpoint! But do take a look at the references provided in the note below to learn more about real instances of such attacks.</em></p>
<p>Use the <a href="homeworks/gift_guessing_game.html">provided webpage</a> to help Jim select injected strings and based on that make a guess about the chosen gift. Include a screenshot of the injected strings you used and the â€œðŸŽ‰ Correct!â€ message.</p>
 <img src="homeworks/figures/crime_attack.png" alt="LZ77 side-channel attack diagram" width="500"/>
</li>
</ol>
<p><strong>Note:</strong></p>
<ul>
<li>To learn more about small data compression using seed inputs and how it is used in practice, you can have a look at Yann Collet's IT-forum talk (<a href="https://drive.google.com/file/d/1-nd9k9GghjR_rtSpTqNesSGaoMx1N0of/view?usp=sharing">Part 1</a>, <a href="https://drive.google.com/file/d/1Ps4PwGqX7douC5PClZp-8XCt7WS9rGJa/view?usp=drive_link">Part 2</a>).</li>
<li>zstd uses the term "dictionary" to refer to what we called seed inputs above.</li>
<li>To read more about LZ77 side-channel attacks mentioned in part 6, you can refer to the slides <a href="https://docs.google.com/presentation/d/11eBmGiHbYcHR9gL5nDyZChu_-lCa2GizeuOfaLU2HOU/edit?usp=sharing">here</a> or Fall 2023 course project report <a href="https://github.com/samanthaarcher0/Compression-Security-Project/blob/main/Project%20Report.pdf">here</a>.</li>
</ul>
<h3 id="q2-burrows-wheeler-transform-and-compression-50-points"><a class="header" href="#q2-burrows-wheeler-transform-and-compression-50-points">Q2: Burrows Wheeler Transform and compression (<em>50 points</em>)</a></h3>
<p><em>DISCLAIMER: This problem looks longer but is actually simpler :P</em></p>
<p>You might be familiar with Fourier transforms, DCT transform, wavelet transform, etc. for images and audio signals. These transforms are widely used as they are invertible and make the data easier to analyse, compress, etc.</p>
<p>In this problem, we will learn about a few lossless transforms for textual data, which have been used for various applications, including data compression.</p>
<p><strong>I. The BWT algorithm:</strong></p>
<p>In 1994, David Wheeler and Michael Burrows discovered (co-incidentally at the DEC Research Labs in Palo Alto!) an invertible transform for textual data, which supposedly made the data easier to compress. In this question, we will learn more about the BWT algorithm and its properties.</p>
<p>The BWT forward transform works the following way:</p>
<ul>
<li>
<p><em>STEP-I</em></p>
<p>Let's say you are given a sequence <code>BANANA</code>. The first thing you do is add a delimiter <code>~</code> to the end. Thus, our new sequence is now: <code>BANANA~</code>. Note that the delimiter is a unique character we are sure never occurs in the input sequence, and is useful to mark the ending of our sequence</p>
</li>
<li>
<p><em>STEP-II</em></p>
<p>In the next step, we form all cyclic rotations of the word <code>BANANA~</code>. As the sequence length is <code>n=7</code>, we will have <code>7</code> such rotations.</p>
<pre><code class="language-py">Input string: BANANA
# all cyclic rotations
BANANA~
~BANANA
A~BANAN
NA~BANA
ANA~BAN
NANA~BA
ANANA~B
</code></pre>
</li>
<li>
<p><em>STEP-III</em></p>
<p>Sort these strings lexico-graphically. This results in <code>n</code> permutations of a string of length <code>n</code> resulting in a <code>n X n</code> 2D table â€” called a Burrows Wheeler's Matrix (BWM). The <code>7 X 7</code> BWM for our example is shown below:</p>
<pre><code class="language-py"># sorted strings
ANANA~B
ANA~BAN
A~BANAN
BANANA~
NANA~BA
NA~BANA
~BANANA
</code></pre>
</li>
<li>
<p><em>STEP-IV</em></p>
<p>Now consider the string formed by last letters of the sorted strings. This new string is the BWT transform of the input!</p>
<pre><code>BANANA -&gt; BNN~AAA
</code></pre>
</li>
</ul>
<ol>
<li>
<p>[5 points] Here are some other examples of BWT:</p>
<pre><code>BANANA -&gt; BNN~AAA
abracadabraabracadabraabracadabra -&gt; rrdd~aadrrrcccraaaaaaaaaaaabbbbbba
hakunamatata -&gt; hnmtt~aauaaka
</code></pre>
<p>Notice that the BWT forward transform of <code>x_input = BANANA -&gt; BNN~AAA</code> has the letters of <code>BANANA~</code> permuted, i.e. <code>BWT(x_input)</code> is just reordering the letters in the input in a particular way. Justify in a few lines why <code>BWT(x_input)</code> is a permutation of the string <code>x_input~</code> (<code>x_input~</code> -&gt; <code>x_input</code> concatenated with the delimiter <code>~</code>).</p>
</li>
<li>
<p>[5 points] Manually compute and show the BWT transform for <code>PANAMA</code>, using the method above. Show your work to get credit (that is you can't just write the final transform but show the steps described above).</p>
</li>
<li>
<p>[10 points] Implement the BWT (forward) transform in the <code>hw3_p2.py</code> file, <code>BurrowsWheelerTransform::forward</code> function. Remember to add a delimiter in the input string (you can use <code>~</code> as delimiter as <code>~</code> has the highest ascii value). You may use the <code>test_bwt_transform()</code> (by commenting out the inverse bwt part) to test your implementation. What is the time complexity of your BWT forward transform implementation for an input of length <code>n</code>?</p>
</li>
</ol>
<p><strong>II. The Inverse BWT Algorithm</strong></p>
<p>The surprising part is that BWT is actually a fully invertible transform, i.e. we can fully retrieve back the original input from the BWT transform e.g. we can recover input string <code>BANANA~</code> from the BWT transform <code>BNN~AAA</code>. The inverse transform works by retrieving the Burrows Wheeler Matrix (BWM) one column at a time. The inverse BWT proceeds in the following way:</p>
<ul>
<li>
<p>In the beginning we only have the BWT transform which is the last column of the BWM. We show the BWT transform and the BWM below.</p>
<pre><code class="language-py"># STEP-0
------B
------N
------N
------~
------A
------A
------A
</code></pre>
</li>
<li>
<p>Notice that each column is a permutation of <code>BANANA~</code>. As the rows of BWM are lexicographically sorted, we can retrieve the first column on BWM by sorting the last column.</p>
<pre><code class="language-py"># STEP-1
A-----B
A-----N
A-----N
B-----~
N-----A
N-----A
~-----A
</code></pre>
</li>
<li>
<p>Due to the cyclic rotations which we used to form the BWM, we can now copy over the last column to the beginning, and we have the first two letters for each row of BWM (although in the incorrect order). We can sort these rows to get the first two columns of the BWM.</p>
<pre><code class="language-py"># STEP-2
A-----B  -&gt; BA-----                   AN-----
A-----N  -&gt; NA-----                   AN-----
A-----N  -&gt; NA-----                   A~-----
B-----~  -&gt; ~B-----   ===&gt; SORT ===&gt;  BA-----
N-----A  -&gt; AN-----                   NA-----
N-----A  -&gt; AN-----                   NA-----
~-----A  -&gt; A~-----                   ~B-----
</code></pre>
</li>
<li>
<p>We can now repeat the procedure, since we know the last column is the BWT transformed string:</p>
<pre><code class="language-py"># STEP 1,2 (repeated)
AN----B  -&gt; BAN----                   ANA----
AN----N  -&gt; NAN----                   ANA----
A~----N  -&gt; NA~----                   A~B----
BA----~  -&gt; ~BA----   ===&gt; SORT ===&gt;  BAN----
NA----A  -&gt; ANA----                   NAN----
NA----A  -&gt; ANA----                   NA~----
~B----A  -&gt; A~B----                   ~BA----
</code></pre>
</li>
<li>
<p>Continuing, this way, we can retrieve the full BWM, and then just pick the row which ends with <code>~</code></p>
<pre><code class="language-py"># BWM
ANANA~B
ANA~BAN
A~BANAN
BANANA~    ===&gt; BANANA~  ---&gt; :D
NANA~BA
NA~BANA
~BANANA
</code></pre>
</li>
</ul>
<ol start="4">
<li>
<p>[5 points] Manually compute the inverse of <code>TGCA~AA</code>. Show your work to get credit (that is you can't just write the final inverse transform but show the steps described above).</p>
</li>
<li>
<p>[10 points] Implement the BWT inverse transform in <code>hw3_p2.py</code> in the <code>BurrowsWheelerTransform::inverse</code> function. Please add (some) inline comments describing your algorithm. What is the time, memory complexity of your BWT inverse transform implementation for an input of length <code>n</code>?</p>
</li>
<li>
<p>[5 points] Notice that the BWT forward transform of typical english language words/sentences have lots of consecutive repeated letters. For example:</p>
<pre><code class="language-py"># BWT forward transforms of some english words
BANANA -&gt; BNN~AAA
abracadabraabracadabraabracadabra -&gt; rrdd~aadrrrcccraaaaaaaaaaaabbbbbba
hakunamatata -&gt; hnmtt~aauaaka

# first 300 letters of sherlock novel
The Project Gutenberg eBook of The Hound of the Baskervilles, by Arthur Conan DoyleThis eBook is for the use of anyone anywhere in the United States andmost other parts of the world at no cost and with almost no restrictionswhatsoever. You may copy it, give it away or re-use it under the termsof the
--&gt;
yernteedfe.htsfedt,yosgs,ekeeyuttkedsytrroefnrrfftedester  ee       ~e   n    pB th mwn   eielnnnnhhhhnrvsshhhr  ljtthdvhbtktlrohooooo r twtTTtttttwtTtrv th    nwgooosrylia ldraioeuauaoU  oaanns    srooCyiBBc fwcmm YHD oueoeoee  etoePAaeeiiteuuatmoooencsiassiiSiu ai     o rcsraoo h- Gierasy  bapaonne
</code></pre>
<p>So in a way BWT forward transform is approximately sorting the input strings. Can you think of why BWT has this property?</p>
<p><strong>HINT</strong>: In the BWM, the last column forms the BWT while the first $n-1$ columns form suffixes of the last column. For example, if the last entry in a particular row is $x_j$ ($j$ th value in original string), then the first $n-1$ entries in that row would be the cyclic suffix: $x_{j+1}\dots x_{n}  x_1 \dots x_{j-1}$ Given the rows are sorted lexicographically, what can you say about the suffixes of nearby entries in the BWT?</p>
</li>
</ol>
<p><strong>III. Using BWT for compression</strong></p>
<p>As we saw in previous part, BWT tends to club letters of the input string together. We will use this property to see if we can improve compression. To do this, we also implement another invertible transform called the Move to Front (MTF) transform. The Move To Front transform keeps a table of the symbols in the data, and moves the most recent symbol to the top of the table. The transform output is the index of the symbols in this changing table and thus the symbols that are more frequent in a local context get a lower index. The MTF forward and inverse transforms are already implemented and included in <code>hw3_p2.py</code> and are described quite well in the wikipedia article here: <a href="https://en.wikipedia.org/wiki/Move-to-front_transform">https://en.wikipedia.org/wiki/Move-to-front_transform</a>.</p>
<p>Here are some sample outputs on applying MTF transform:</p>
<pre><code>Input str: aaabbbaaacccaaa
MTF: [97, 0, 0, 98, 0, 0, 1, 0, 0, 99, 0, 0, 1, 0, 0]
</code></pre>
<pre><code>Input str: rrdd~aadrrrcccraaaaaaaaaaaabbbbbba
MTF: [114, 0, 101, 0, 126, 100, 0, 2, 3, 0, 0, 102, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102, 0, 0, 0, 0, 0, 1]
</code></pre>
<p>We use the BWT, MTF transforms to transform the first <code>50,000</code> characters (let's call this the <code>input_data</code>) of our sherlock novel and analyze the entropy of the empirical distribution of the transformed string. (this output can be obtained by running the test <code>test_bwt_mtf_entropy</code> if you are curious!). The output obtained is as below:</p>
<pre><code>Input data: 0-order Empirical Entropy: 4.1322
Input data + BWT: 0-order Empirical Entropy: 4.1325
Input data + MTF: 0-order Empirical Entropy: 4.6885
Input data + BWT + MTF: 0-order Empirical Entropy: 2.8649
</code></pre>
<ol start="7">
<li>
<p>[10 points] Let's try to understand the compression results above a bit better:</p>
<ol>
<li>Given a good implementation of an Arithmetic coder, what is approximate average codelength per symbol you might expect on compressing the <code>input_data</code> using its empirical 0-order distribution (i.e. just the symbol counts)?</li>
<li>Notice that the empirical entropy of <code>input_data</code> is almost the same as that of <code>bwt_data = BWT(input_data)</code>. In fact the empirical entropy of <code>bwt_data</code> is slightly higher than that of <code>input_data</code> (<code>4.1325 vs 4.1322</code>). Justify why is this the case.</li>
<li>Notice that empirical entropy of <code>mtf_bwt_data = MTF(BWT(input_data))</code> is much lower than that of the <code>input_data</code> (<code>2.8649</code> vs <code>4.1322</code>). Can you think of why this is the case?</li>
<li>Based on the numbers you have, describe a compression scheme which achieves average codelength of approximately <code>~2.87 bits/symbol</code> on the <code>input_data</code>. You don't need to implement this compressor, but ensure you clearly describe the steps involved in encoding and decoding. You are free to use any compressors you have learnt in the class.</li>
</ol>
<p>Feel free to experiment with the test functions, to print transformed data or to build better intuition with custom examples. Please include any relevant outputs in your submission which you found useful to answer these questions.</p>
</li>
</ol>
<p><em>NOTE:</em> compressors such as <code>bzip2</code>, and <code>bsc</code> in fact use BWT and MTF transforms along with a few other tricks to obtain great compression. BWT algorithms have also found another surprising use: they allow efficient searching over compressed text! Here are more references in case you are interested:</p>
<ol>
<li><a href="https://sourceware.org/bzip2/">bzip2</a>, <a href="http://libbsc.com/">bsc</a></li>
<li>wikipedia article: <a href="https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform">https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform</a></li>
<li>BWT for searching over compressed text: <a href="https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/bwt.pdf">slides</a></li>
</ol>
<h3 id="q3-compression-with-side-information-20-points"><a class="header" href="#q3-compression-with-side-information-20-points">Q3: Compression with side information (<em>20 points</em>)</a></h3>
<p>Let's consider a simple puzzle:</p>
<ul>
<li>Let $X$ be a 3 bit random variable where each bit is i.i.d $Ber(0.5)$, i.e. $X$ is uniformly distributed  in ${000,001,010,011,100,101,110,111}$ with probability $1/8$,</li>
<li>and $Y= X \oplus e$, where $e \in {000,001,010,100}$ and is independent of $X$. The distribution of $e$ is unknown, i.e. $Y$ can be different from $X$ in at most one bit position. Recall that $\oplus$ is the <a href="https://en.wikipedia.org/wiki/Bitwise_operation#XOR">bitwise-XOR operation</a>.</li>
</ul>
<ol>
<li>
<p>[5 points] Show that $H(X) = 3, H(X|Y) \leq 2$.</p>
<p><strong>HINT</strong>: Consider $H(X,e|Y)$ and write it in two different ways</p>
</li>
<li>
<p>[5 points] Kakashi wants to losslessly encode $X$ and send the encoded coderword to Sasuke. What is the optimal compression scheme for Kakashi?</p>
</li>
<li>
<p>[5 points] Now let's say both Kakashi and Sasuke have access to $Y$, i.e. $Y$ is the side-information available to both Kakashi and Sasuke (through side-information ninjutsu :P). In that case, show that Kakashi can do better than in <code>Q3.2</code>, by using <code>2 bits</code> to transmit $X$.</p>
</li>
<li>
<p>[5 points] Unfortunately Kakashi lost access to $Y$, and only knows $X$ but Sasuke still has access to the side information $Y$. Show that in this case Kakashi can still losslessly encode $X$ and send it to Sasuke using <code>2 bits</code>, i.e. surprisingly we just need the side-information at the decoder, and not necessarily at the encoder.</p>
</li>
</ol>
<p>NOTE: It is quite fascinating that we need side-information only at the decoder. This property can be generalized to more general scenarios, and is the foundation for distributed data compression. E.g. see <a href="https://en.wikipedia.org/wiki/Slepian%E2%80%93Wolf_coding">Slepian-Wolf coding</a> and <a href="https://en.wikipedia.org/wiki/Distributed_source_coding#:~:text=generate%20syndrome%20bits.-,Wyner%E2%80%93Ziv%20coding%20%E2%80%93%20lossy%20distributed%20coding,and%20corresponding%20reconstruction%20method%20design">Wyner-Ziv coding</a> for more details.</p>
<h3 id="q4-kth-order-adaptive-arithmetic-coding-40-points"><a class="header" href="#q4-kth-order-adaptive-arithmetic-coding-40-points">Q4: kth order adaptive arithmetic coding (<em>40 points</em>)</a></h3>
<p>Shubham wants to test the compression performance of the k-th order context models using Arithmetic Coding he has implemented. However, one problem is that for unknown sources it is very difficult to compute the fundamental limit on the average codelength. So, he decides to test his algorithms on a synthetic dataset.</p>
<p>He generates data using a noisy version of <a href="https://en.wikipedia.org/wiki/Linear-feedback_shift_register">Linear Feedback Shift Register based Pseudorandom generator</a>. The name sounds quite complicated, but it is a simple concept (you don't need to know anything about it to attempt this problem). The pseudocode to generate noisy-LFSR sequence is given below (also given in HW code <code>hw3_p4.py</code>):</p>
<pre><code class="language-py">def pseudo_random_LFSR_generator(data_size, tap, noise_prob=0):
    # initial sequence = [1,0,0,0,...] of length tap
    initial_sequence = [1] + [0] * (tap - 1)

    # output sequence
    output_sequence = list(initial_sequence)
    for _ in range(data_size - tap):
        s = output_sequence[-1] ^ output_sequence[-tap]  # xor
        if noise_prob &gt; 0: 
            s = s ^ Bernoulli(p=noise_prob) # add noise
        output_sequence.append(s)
    return output_sequence
</code></pre>
<p>Our <code>pseudo_random_LFSR_generator</code> generates an output sequence of length <code>data_size</code>. At each step, it calculates the next bit using XOR (<code>a ^ b</code> is <code>True</code>/<code>1</code> if exactly one of <code>a</code> and <code>b</code> is <code>True</code>/<code>1</code>) between the last bit in the sequence and the bit located <code>tap</code> positions before it (this simulates LFSR behavior). Optionally, if <code>noise_prob</code> is greater than <code>0</code>, it adds noise to the calculated bit by XOR-ing it with a random binary value generated with a given probability (<code>noise_prob</code>). The calculated (possibly noisy) bit is appended to the sequence in each iteration. To initialize the sequence, we always start it with <code>[1,0,0,0,...]</code> so that we have <code>tap</code> number of bits available to calculate the next bit.</p>
<p>For concreteness let's take a specific parameter setting (function in <code>hw3_p4.py</code>):</p>
<pre><code class="language-py">pseudo_random_LFSR_generator(data_size=10000, tap=3, noise_prob=0)
</code></pre>
<p>We see that the first 12 symbols of the sequence looks like:</p>
<pre><code>1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, ...
</code></pre>
<p>Notice that the sequence looks random-ish but starts repeating after 8 symbols. In fact if we look at the first 7 3-mers (substrings of length 3), i.e. <code>100</code>, <code>001</code>, ..., <code>010</code>, then we see that they are all unique and do not repeat! The LFSR sequence with <code>noise_prob=0</code>, is in fact a simple pseudo-random generator used in the past. We will use this source to further our understanding of context based compression.</p>
<p>Let's start by defining the $k^{\text{th}}$ order <code>empirical entropy</code> of the sequence. Recall, if your sequence is $x_0, x_1, x_2, \ldots, x_n$, the  $k^{\text{th}}$-order empirical entropy of the sequence can be computed by making a joint and conditional probability distribution table using counts of the previously seen symbols (we saw this in class in L9 on context based AC lecture). Specifically, assuming the alphabet is $\mathcal{X}$, for any $(x_0, x_1, x_2, \ldots x_k) \in \mathcal{X}^{k+1}$, we can define the empirical joint probability as:
$$
p(x_0, x_1, x_2, \ldots x_k) = \frac{\text{count}(x_0, x_1, x_2, \ldots x_k)}{\text{total count of k+1-tuples}}
$$</p>
<p>the empirical conditional probability as:
$$
p(x_k | x_0, x_1, x_2, \ldots x_{k-1}) = \frac{p(x_0, x_1, x_2, \ldots x_k)}{p(x_0, x_1, x_2, \ldots x_{k-1})} = \frac{p(x_0, x_1, x_2, \ldots x_k)}{\sum_{x' \in \mathcal{X}} p(x_0, x_1, x_2, \ldots x_{k-1}, x')}
$$</p>
<p>and the $k^{\text{th}}$-order empirical entropy is then given by summing over all possible $(k+1)$-tuples:</p>
<p>$$H_k(X) = \sum_{(x_0, x_1, x_2, \ldots, x_k) \in \mathcal{X}^{k+1}} p(x_0, x_1, x_2, \ldots, x_{k}) \log {\frac{1} {p(x_k | x_0, x_1, x_2, \ldots, x_{k-1})}}$$</p>
<ol>
<li>
<p>[5 points] Show that for infinite data ($n \rightarrow \infty$) and $k_1 &gt; k_2$,
$$H_{k_1}(X) \leq H_{k_2}(X)$$
i.e. $k_1^{\text{th}}$-order empirical entropy is always less than (or equal to) $k_2^{\text{th}}$-order empirical entropy.</p>
<p>You can assume the data distribution is stationary and ergodic (or in simple words you can assume the empirical probabilities converge to the stationary distribution).</p>
</li>
<li>
<p>[10 points] Next let's calculate $H_k(X)$ for the above given sequence with <code>tap=3</code>, <code>noise_prob=0</code>.
(Recall the sequence from above: <code>1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, ...</code>)</p>
<ol>
<li>Compute the $0^{\text{th}}$ order count-distribution and empirical entropy of the sequence. <strong>HINT</strong>: sequence repeats itself!</li>
<li>Compute the $1^{\text{st}}$ order count-distribution and empirical entropy of the sequence.</li>
<li>Finally, argue that for $k \geq 3$, the $k^{\text{th}}$-order empirical entropy of the sequence is <code>0</code>.</li>
</ol>
</li>
<li>
<p>[5 points] Now, Shubham decides to use Adaptive Arithmetic coding to compress data from this source. For the case with no added noise (<code>noise_prob=0</code>), the number of bits/symbol we achieve is as follows for different context size <code>k</code>. You can use the test code in <code>hw3_p4.py</code> to generate these numbers by changing and choosing appropriate parameters. Use the <code>-s</code> flag in pytest to dump the output even for successful tests.</p>
<pre><code>----------
Data generated as: X[n] = X[n-1] âŠ• X[n-3]
data_size=10000
AdaptiveArithmeticCoding, k=0, avg_codelen: 0.989
AdaptiveArithmeticCoding, k=1, avg_codelen: 0.969
AdaptiveArithmeticCoding, k=2, avg_codelen: 0.863
AdaptiveArithmeticCoding, k=3, avg_codelen: 0.012
AdaptiveArithmeticCoding, k=4, avg_codelen: 0.011
AdaptiveArithmeticCoding, k=7, avg_codelen: 0.011
AdaptiveArithmeticCoding, k=15, avg_codelen: 0.012
AdaptiveArithmeticCoding, k=22, avg_codelen: 0.013
</code></pre>
<p>Argue why these results make sense. In particular, argue that Adaptive Arithmetic coding with order <code>k=0,1,2</code> cannot do much better than <code>1 bit/symbol</code>. Also, argue that for <code>k &gt;= 3</code>, the compression should be <code>~0 bits/symbol</code>.</p>
</li>
</ol>
<p>Next, Shubham decides to encode noisy-LFSR sources with <code>noise_prob=0.01</code> and <code>TAP=3,7,15</code>. The noisy LFSR is very similar to the non-noisy version, except we add a bit of noise after xoring the two past symbols as shown in the pseduo-code above.</p>
<ol start="4">
<li>
<p>[5 points] For <code>TAP=3, noise_prob=0.01</code>, what is the fundamental limit to which you can compress this sequence, i.e. what is $k^{\text{th}}$-order empirical entropy of the sequence for $k \geq 3$ and large $n$?</p>
</li>
<li>
<p>[5 points] Next, we encode noisy-LFSR sequence with parameters <code>noise_prob=0.01</code> and <code>TAP=3,7,15</code> using Adaptive Arithmetic coding. The number of bits/symbol we achieve is as follows for different context size <code>k</code>. You can again use the test code in <code>hw3_p4.py</code> to generate these numbers by changing and choosing appropriate parameters. In these cases we observe the following average codelength, for a sequence of length <code>10000</code></p>
<pre><code class="language-py">----------
Data generated as: X[n] = X[n-1] âŠ• X[n-3] âŠ• Bern_noise(0.01)
DATA_SIZE=10000
AdaptiveArithmeticCoding, k=0, avg_codelen: 1.003
AdaptiveArithmeticCoding, k=1, avg_codelen: 1.002
AdaptiveArithmeticCoding, k=2, avg_codelen: 0.999
AdaptiveArithmeticCoding, k=3, avg_codelen: 0.087
AdaptiveArithmeticCoding, k=4, avg_codelen: 0.089
AdaptiveArithmeticCoding, k=7, avg_codelen: 0.097
AdaptiveArithmeticCoding, k=15, avg_codelen: 0.121
AdaptiveArithmeticCoding, k=22, avg_codelen: 0.146
AdaptiveArithmeticCoding, k=24, avg_codelen: 0.152

----------
Data generated as: X[n] = X[n-1] âŠ• X[n-7] âŠ• Bern_noise(0.01)
DATA_SIZE=10000
AdaptiveArithmeticCoding, k=0, avg_codelen: 1.004
AdaptiveArithmeticCoding, k=1, avg_codelen: 1.004
AdaptiveArithmeticCoding, k=2, avg_codelen: 1.005
AdaptiveArithmeticCoding, k=3, avg_codelen: 1.007
AdaptiveArithmeticCoding, k=4, avg_codelen: 1.009
AdaptiveArithmeticCoding, k=7, avg_codelen: 0.139
AdaptiveArithmeticCoding, k=15, avg_codelen: 0.192
AdaptiveArithmeticCoding, k=22, avg_codelen: 0.240
AdaptiveArithmeticCoding, k=24, avg_codelen: 0.254

----------
Data generated as: X[n] = X[n-1] âŠ• X[n-15] âŠ• Bern_noise(0.01)
DATA_SIZE=10000
AdaptiveArithmeticCoding, k=0, avg_codelen: 1.004
AdaptiveArithmeticCoding, k=1, avg_codelen: 1.004
AdaptiveArithmeticCoding, k=2, avg_codelen: 1.005
AdaptiveArithmeticCoding, k=3, avg_codelen: 1.006
AdaptiveArithmeticCoding, k=4, avg_codelen: 1.008
AdaptiveArithmeticCoding, k=7, avg_codelen: 1.031
AdaptiveArithmeticCoding, k=15, avg_codelen: 0.949
AdaptiveArithmeticCoding, k=22, avg_codelen: 0.955
AdaptiveArithmeticCoding, k=24, avg_codelen: 0.957
</code></pre>
<p>Notice that the average codelength for <code>TAP=3,7</code> is at its minima for <code>k=TAP</code> value and then increases as we further increase <code>k</code> which seems to suggest the result we saw in <code>Q4.1</code> is wrong. Argue:</p>
<ol>
<li>Why do these results still make sense and do not contradict theory?</li>
<li>For <code>TAP=15</code>, even for context size <code>k=15</code>, why is the model not able to compress the data very well?</li>
</ol>
</li>
<li>
<p>[10 points] Instead of using Adaptive Arithmetic coding, Jiwon suggested that if we know the source parameters (i.e. <code>TAP</code> and <code>noise_prob</code>), then we could predict a better probability distribution of the next symbol based on the past, and use this for performing Arithmetic coding. Jiwon thinks this implementation should work for arbitrary values of <code>TAP</code> and input sequence lengths. Complete Jiwon's <code>update_model</code> logic in <code>NoisyLFSRFreqModel</code> provided in <code>hw3_p4.py</code>. For the first <code>TAP</code> symbols you can use arbitrary predictions (e.g., uniform distribution) without affecting the results for large sequences.</p>
<p>Once you have implemented the model, run the tests (<code>py.test -s hw3_p4.py</code>) for <code>TAP=3,7,15</code> and <code>noise_prob=0.01</code> again and report the average codelength obtained for the LFSR model. How do these results compare with the Adaptive Arithmetic coding results (from part 5) and to the expected theoretical limit (from part 4)? Explain why this is the case.</p>
</li>
</ol>
<p>NOTE: References provided with Lecture 9 on Context-based Arithmetic Coding will be useful in attempting this problem (SCL implementation of adaptive arithmetic coding, notes, slides, etc.)</p>
<h3 id="q5-hw3-feedback-5-points"><a class="header" href="#q5-hw3-feedback-5-points">Q5: HW3 Feedback <em>(5 points)</em></a></h3>
<p>Please answer the following questions, so that we can adjust the difficulty/nature of the problems for the next HWs.</p>
<ol>
<li>How much time did you spent on the HW in total?</li>
<li>Which question(s) did you enjoy the most?</li>
<li>Are the programming components in the HWs helping you understand the concepts better?</li>
<li>Did the HW3 questions complement the lectures?</li>
<li>Any other comments?</li>
</ol>
<h3 id="submission-instructions-2"><a class="header" href="#submission-instructions-2">Submission Instructions</a></h3>
<p>Please submit both the written part and your code on Gradescope in their respective submission links. <strong>We will be using both autograder and manual code evaluation for evaluating the coding parts of the assignments.</strong> You can see the scores of the autograded part of the submissions immediately. For code submission ensure following steps are followed for autograder to work correctly:</p>
<ul>
<li>
<p>As with HW1, you only need to submit the modified files as mentioned in the problem statement.</p>
<ul>
<li>Compress the <code>HW3</code> folder into a zip file. One way to obtain this zip file is by running the following zip command in the <code>HWs</code> folder, i.e.
<pre><code class="language-sh">cd HWs
zip -r HW3.zip HW3
</code></pre>
Note: To avoid autograder errors, ensure that the directory structure is maintained and that you have compressed <code>HW3</code> folder containing the relevant files and not <code>HWs</code> folder, or files inside or something else. Ensure the name of the files inside the folder are exactly as provided in the starter code, i.e. <code>hw3_p1.py</code>, <code>hw3_p2.py</code> etc. In summary, your zip file should be uncompressed to following directory structure (with same names):
<pre><code>HW3
â”œâ”€â”€ p1_data
    â””â”€â”€pokemon_data_seed_input.txt
â”œâ”€â”€ hw3_p1.py
â”œâ”€â”€ hw3_p2.py
â””â”€â”€ hw3_p4.py
â””â”€â”€ sherlock_ascii.txt
</code></pre>
</li>
</ul>
</li>
<li>
<p>Submit the zip file (<code>HW3.zip</code> obtained above) on Gradescope Programming Part Submission Link. Ensure that the autograded parts runs and give you correct scores.</p>
</li>
</ul>
<p><strong>Before submitting the programming part on Gradescope, we strongly recommend ensuring that the code runs correctly locally.</strong></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ee274-fall-25-homework-3-1"><a class="header" href="#ee274-fall-25-homework-3-1">EE274 (Fall 25): Homework-3</a></h1>
<ul>
<li><strong>Focus area:</strong> Context-based compression and LZ77</li>
<li><strong>Due Date:</strong> Nov 11, midnight (11:59 PM)</li>
<li><strong>Weightage:</strong> 15%</li>
<li><strong>Total Points:</strong> 150</li>
</ul>
<h3 id="q1-lz77-compression-for-small-data-35-points-1"><a class="header" href="#q1-lz77-compression-for-small-data-35-points-1">Q1 LZ77 compression for small data (<em>35 points</em>)</a></h3>
<p>In this problem, we will understand how LZ77 compression performs on small files and how to improve its performance. Recall that the LZ77 algorithm looks for matches in a window storing the previously seen data and then encodes the match lengths, match offsets and unmatched characters (literals). We use the LZ77 implementation provided in SCL for the experiments below, and you can take a look at the code <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/compressors/lz77.py">here</a> to understand the details better. We have provided a set of small json files in the <code>p1_data/github_data</code> directory. Run the script <code>python hw3_p1.py -i p1_data/github_data</code> in scl/HWs/HW3 folder, which produces the following output:</p>
<pre><code>Compressing without using a seed input
Number of files: 134
Total uncompressed size (in bits): 956272
Normalized uncompressed size (in avg. bits/file): 7136
Total size after compressing the files individually (in bits): 363774
Total size after compressing the files jointly (in bits): 71795
</code></pre>
<p>Ignore the first line for a moment. We see that we have <code>134</code> relatively small files with average size of <code>7136 bits</code>. If we compress the files individually and then sum the sizes, we get a total of <code>363774 bits</code>, whereas if we concatenate the files and then compress them as a single block ("jointly") the compressed size is just <code>71795 bits</code>!</p>
<ol>
<li>
<p>[4 points] Give two reasons why concatenating the files together provides a reduction in file size.</p>
<p><strong>HINT</strong>: One of the reasons is applicable even if the files were from completely distinct sources and had no similarity whatsoever.</p>
<p><strong>Solution</strong></p>
<ol>
<li>The first reason is that the LZ77 algorithm works best if it can find longer matches within the past symbols in a file. If we concatenate the files, then the LZ77 algorithm will be able to find longer matches within the concatenated file compared to the original small file.</li>
<li>The second reason is that now we have increased the block size for LZ77, and thus the LZ77 algorithm will be able to converge closer to the entropy (see Slide 29 in <a href="https://stanforddatacompressionclass.github.io/Fall22/static_files/L9_slide.pdf">LZ77 slides</a>). Another related reason is that compressed files have a bunch of overhead due to headers, etc. which gets amortized for larger files.</li>
</ol>
</li>
</ol>
<p>Ideally one would just combine these small files into bigger batches for the best compression. However, we sometimes need to store, transmit or compress small files. For example, we need to transmit short JSON messages/network packets between servers where latency is important, and we can't batch requests. Another common use case is when databases use small data pages for fast random access. Small files also show up when we are working with resource constrained devices that cannot hold large files.</p>
<p>To improve the compression on small files, we consider a slight modification of the LZ77 algorithm we studied in class. Both the compressor and the decompressor now take an additional seed input, which is just a sequence of bytes. The idea is simple: instead of starting the LZ77 compressor with an empty window, we instead initialize the window with a seed input (and appropriately update the other indexing data structures). The same seed input should be used during compression and decompression to enable recovery.</p>
<p>The overall system architecture needs to maintain these seed inputs (which might be specific to particular data categories), and make sure the encoders and decoders can access these. The seed inputs are usually constrained to be small to avoid extra overhead.</p>
<p>We provide a sample seed input for the above dataset in <code>p1_data/github_data_seed_input.txt</code>. Run <code>python hw3_p1.py -i p1_data/github_data -s p1_data/github_data_seed_input.txt</code> to obtain the following:</p>
<pre><code>Loading seed input from p1_data/github_data_seed_input.txt
Number of files: 134
Total uncompressed size (in bits): 956272
Normalized uncompressed size (in avg. bits/file): 7136
Total size after compressing the files individually (in bits): 224738
Total size after compressing the files jointly (in bits): 70678
</code></pre>
<ol start="2">
<li>
<p>[6 points] We see a significant reduction in the total size for compressing the files individually (<code>363774</code> bits to <code>224738</code> bits). Based on your understanding of the LZ77 algorithm and your answer to <code>Q1.1</code>, explain why this is the case. You might find it useful to look both at the json files in <code>p1_data/github_data/</code> and the seed input in <code>p1_data/github_data_seed_input.txt</code>.</p>
<p><strong>Solution</strong></p>
<p>Seed file consists of data which is similar to the input files. Thus, LZ77 algorithm is able to find more matches in the seed file while compressing the individual files resulting in better compression of the individual files better.</p>
</li>
<li>
<p>[2 points] Why is the impact of using the seed input negligible when we compress the files jointly?</p>
<p><strong>Solution</strong>
Since the concatenated file already consists of the data present in individual files, adding a seed file with similar data as the individual files does not help in finding significantly more matches. Therefore, we get negligible benefit on using the seed file when we compress the files jointly. We might get some benefit for the initial part of the concatenated file, but for most later parts the benefit is minimal.</p>
</li>
<li>
<p>[3 points] The provided seed input file is less than 1 KB in size. If you were allowed to choose an arbitrarily large seed input, how might you design it to minimize the compressed size for this specific dataset.</p>
<p><strong>HINT</strong>: Think about the best case scenario for LZ77 parsing - longest possible matches and no literals.</p>
<p><strong>Solution</strong></p>
<p>Concatenating all files and using them as a seed file can be a good choice. This will result in the LZ77 algorithm finding the longest possible matches and no literals. Thus, we will get the best possible compression.</p>
</li>
<li>
<p>[10 points] Now you will create a seed input for another dataset provided in the <code>p1_data/pokemon_data</code> directory. We will evaluate your submissions on a test dataset which has similar files as the <code>p1_data/pokemon_data</code> directory. Your submission should satisfy the following:</p>
<ul>
<li>name the seed input file as <code>p1_data/pokemon_data_seed_input.txt</code></li>
<li>the seed input file should be less than 1 KB (1000 B) large. You can check the file size in bytes by running <code>wc -c p1_data/pokemon_data_seed_input.txt</code>.</li>
<li>the total size for compressing the files individually should reduce to at least 2x when using the seed input (vs. when not using a seed input) for both the <code>pokemon_data</code> set and the autograder submission. For example, if the <code>Total size after compressing the files individually</code> is <code>308031 bits</code> without seed input, then with your seed input it should be at most <code>154015 bits</code>.</li>
<li>A couple hints to help you achieve best results: (i) try to use similar JSON format and formatting in your seed input file as in the pokemon data files - this includes using the same 2-space indendation, and (ii) (for Windows users) make sure your seed input uses LF (<code>\n</code>) line-endings and not CRLF (<code>\r\n</code>) - verify your editor is not changing this automatically.</li>
</ul>
<p><strong>Solution</strong></p>
<p>The seed file we used can be found at <a href="homeworks/p1_data/pokemon_data_seed_input.txt"><code>p1_data/pokemon_data_seed_input.txt</code></a>. The total size for compressing the files individually reduces by $\sim2\times$ when using the seed input for both the <code>pokemon_data</code> set and the autograder submission.</p>
</li>
<li>
<p>[10 points] Christmas is approaching and <a href="https://en.wikipedia.org/wiki/The_Gift_of_the_Magi">Jim knows that Della</a> is going to buy a thoughtful gift for him. Unable to control his curiosity and being a part-time hacker, he has decided to spy on Dellaâ€™s internet search patterns by executing a side-channel attack. As shown in the diagram below, he is able to inject strings into Dellaâ€™s http requests, and once Della makes a search request he can monitor the LZ77 compressed size of the response. Here the response includes his injected string along with the gift description, allowing him to infer some side-channel information about the search. <em>Don't take the story too seriously from a internet security standpoint! But do take a look at the references provided in the note below to learn more about real instances of such attacks.</em></p>
<p>Use the <a href="homeworks/gift_guessing_game.html">provided webpage</a> to help Jim select injected strings and based on that make a guess about the chosen gift. Include a screenshot of the injected strings you used and the â€œðŸŽ‰ Correct!â€ message.</p>
 <img src="homeworks/figures/crime_attack.png" alt="LZ77 side-channel attack diagram" width="500"/>
<p><strong>Solution</strong>
The idea is to use injected strings that are substrings (e.g., a sentence) of the gift descriptions. By monitoring the compressed sizes for different injected strings, we can infer which gift description is most likely.</p>
</li>
</ol>
<p><strong>Note:</strong></p>
<ul>
<li>To learn more about small data compression using seed inputs and how it is used in practice, you can have a look at Yann Collet's IT-forum talk (<a href="https://drive.google.com/file/d/1-nd9k9GghjR_rtSpTqNesSGaoMx1N0of/view?usp=sharing">Part 1</a>, <a href="https://drive.google.com/file/d/1Ps4PwGqX7douC5PClZp-8XCt7WS9rGJa/view?usp=drive_link">Part 2</a>).</li>
<li>zstd uses the term "dictionary" to refer to what we called seed inputs above.</li>
<li>To read more about LZ77 side-channel attacks mentioned in part 6, you can refer to the slides <a href="https://docs.google.com/presentation/d/11eBmGiHbYcHR9gL5nDyZChu_-lCa2GizeuOfaLU2HOU/edit?usp=sharing">here</a> or Fall 2023 course project report <a href="https://github.com/samanthaarcher0/Compression-Security-Project/blob/main/Project%20Report.pdf">here</a>.</li>
</ul>
<h3 id="q2-burrows-wheeler-transform-and-compression-50-points-1"><a class="header" href="#q2-burrows-wheeler-transform-and-compression-50-points-1">Q2: Burrows Wheeler Transform and compression (<em>50 points</em>)</a></h3>
<p><em>DISCLAIMER: This problem looks longer but is actually simpler :P</em></p>
<p>You might be familiar with Fourier transforms, DCT transform, wavelet transform, etc. for images and audio signals. These transforms are widely used as they are invertible and make the data easier to analyse, compress, etc.</p>
<p>In this problem, we will learn about a few lossless transforms for textual data, which have been used for various applications, including data compression.</p>
<p><strong>I. The BWT algorithm:</strong></p>
<p>In 1994, David Wheeler and Michael Burrows discovered (co-incidentally at the DEC Research Labs in Palo Alto!) an invertible transform for textual data, which supposedly made the data easier to compress. In this question, we will learn more about the BWT algorithm and its properties.</p>
<p>The BWT forward transform works the following way:</p>
<ul>
<li>
<p><em>STEP-I</em></p>
<p>Let's say you are given a sequence <code>BANANA</code>. The first thing you do is add a delimiter <code>~</code> to the end. Thus, our new sequence is now: <code>BANANA~</code>. Note that the delimiter is a unique character we are sure never occurs in the input sequence, and is useful to mark the ending of our sequence</p>
</li>
<li>
<p><em>STEP-II</em></p>
<p>In the next step, we form all cyclic rotations of the word <code>BANANA~</code>. As the sequence length is <code>n=7</code>, we will have <code>7</code> such rotations.</p>
<pre><code class="language-py">Input string: BANANA
# all cyclic rotations
BANANA~
~BANANA
A~BANAN
NA~BANA
ANA~BAN
NANA~BA
ANANA~B
</code></pre>
</li>
<li>
<p><em>STEP-III</em></p>
<p>Sort these strings lexico-graphically. This results in <code>n</code> permutations of a string of length <code>n</code> resulting in a <code>n X n</code> 2D table â€” called a Burrows Wheeler's Matrix (BWM). The <code>7 X 7</code> BWM for our example is shown below:</p>
<pre><code class="language-py"># sorted strings
ANANA~B
ANA~BAN
A~BANAN
BANANA~
NANA~BA
NA~BANA
~BANANA
</code></pre>
</li>
<li>
<p><em>STEP-IV</em></p>
<p>Now consider the string formed by last letters of the sorted strings. This new string is the BWT transform of the input!</p>
<pre><code>BANANA -&gt; BNN~AAA
</code></pre>
</li>
</ul>
<ol>
<li>
<p>[5 points] Here are some other examples of BWT:</p>
<pre><code>BANANA -&gt; BNN~AAA
abracadabraabracadabraabracadabra -&gt; rrdd~aadrrrcccraaaaaaaaaaaabbbbbba
hakunamatata -&gt; hnmtt~aauaaka
</code></pre>
<p>Notice that the BWT forward transform of <code>x_input = BANANA -&gt; BNN~AAA</code> has the letters of <code>BANANA~</code> permuted, i.e. <code>BWT(x_input)</code> is just reordering the letters in the input in a particular way. Justify in a few lines why <code>BWT(x_input)</code> is a permutation of the string <code>x_input~</code> (<code>x_input~</code> -&gt; <code>x_input</code> concatenated with the delimiter <code>~</code>).</p>
<p><strong>Solution</strong>
Since we are taking circular rotations of the string, each character of the string <code>x_input~</code> will be present in the last column of list of strings (output of Step-II). Sorting this list of strings only changes the order of the last characters (output of Step-I). Thus, <code>BWT(x_input)</code> is a permutation of the string <code>x_input~</code>.</p>
</li>
<li>
<p>[5 points] Manually compute and show the BWT transform for <code>PANAMA</code>, using the method above. Show your work to get credit (that is you can't just write the final transform but show the steps described above).</p>
<p><strong>Solution:</strong></p>
<ul>
<li>Step-II: calculate cyclic rotations of <code>PANAMA~</code></li>
</ul>
<pre><code class="language-py">Input string: PANAMA~
# all cyclic rotations
PANAMA~
~PANAMA
A~PANAM
MA~PANA
AMA~PAN
NAMA~PA
ANAMA~P
</code></pre>
<ul>
<li>STEP-III: Sort these strings lexico-graphically.</li>
</ul>
<pre><code class="language-py"># sorted strings
AMA~PAN
ANAMA~P
A~PANAM
MA~PANA
NAMA~PA
PANAMA~
~PANAMA
</code></pre>
<ul>
<li>Step-IV: BWT forward transform is last letters of the sorted strings.</li>
</ul>
<p>Therefore, BWT forward transform of <code>PANAMA</code> is <code>NPMAA~A</code></p>
</li>
<li>
<p>[10 points] Implement the BWT (forward) transform in the <code>hw3_p2.py</code> file, <code>BurrowsWheelerTransform::forward</code> function. Remember to add a delimiter in the input string (you can use <code>~</code> as delimiter as <code>~</code> has the highest ascii value). You may use the <code>test_bwt_transform()</code> (by commenting out the inverse bwt part) to test your implementation. What is the time complexity of your BWT forward transform implementation for an input of length <code>n</code>?</p>
<p><strong>Solution</strong></p>
<pre><code class="language-python">def forward(self, data_block: DataBlock):
    """
    Generates the forward transform of BWT
    NOTE: for consistency all forward and inverse functions take in as input
    a DataBlock
    """

    # create a string using data_block
    input_block_str = "".join(data_block.data_list)

    ###############################################
    # ADD DETAILS HERE
    # to generate bwt_str (BWT transformed string)
    # Note: remember to add the delimiter to the string!
    ###############################################
    bwt_str = ""

    # STEP-1: add a delimiter
    input_block_str += self.delimiter

    # STEP-2: get all cyclic rotations, and sort
    N = len(input_block_str)
    cyclic_rotations = []
    cur_str = input_block_str
    for _ in range(N):
        cur_str = cur_str[-1] + cur_str[:-1]
        cyclic_rotations.append(cur_str)
    cyclic_rotations.sort()

    # STEP-3: pick the last column and make a single string
    bwt_str = "".join([rot_str[-1] for rot_str in cyclic_rotations])

    ###############################################

    data_bwt_block = DataBlock(list(bwt_str))
    return data_bwt_block
</code></pre>
<p>The forward transform involves creating a $n$-length list of $O(n)$ length strings, followed by
sorting the $O(n)$ length strings. Assuming sorting is $O(n\log n)$ comparisons and each comparison is $O(n)$ (since we are working with $n$-length strings), the overall complexity will be $O(n^2\log n)$.</p>
</li>
</ol>
<p><strong>II. The Inverse BWT Algorithm</strong></p>
<p>The surprising part is that BWT is actually a fully invertible transform, i.e. we can fully retrieve back the original input from the BWT transform e.g. we can recover input string <code>BANANA~</code> from the BWT transform <code>BNN~AAA</code>. The inverse transform works by retrieving the Burrows Wheeler Matrix (BWM) one column at a time. The inverse BWT proceeds in the following way:</p>
<ul>
<li>
<p>In the beginning we only have the BWT transform which is the last column of the BWM. We show the BWT transform and the BWM below.</p>
<pre><code class="language-py"># STEP-0
------B
------N
------N
------~
------A
------A
------A
</code></pre>
</li>
<li>
<p>Notice that each column is a permutation of <code>BANANA~</code>. As the rows of BWM are lexicographically sorted, we can retrieve the first column on BWM by sorting the last column.</p>
<pre><code class="language-py"># STEP-1
A-----B
A-----N
A-----N
B-----~
N-----A
N-----A
~-----A
</code></pre>
</li>
<li>
<p>Due to the cyclic rotations which we used to form the BWM, we can now copy over the last column to the beginning, and we have the first two letters for each row of BWM (although in the incorrect order). We can sort these rows to get the first two columns of the BWM.</p>
<pre><code class="language-py"># STEP-2
A-----B  -&gt; BA-----                   AN-----
A-----N  -&gt; NA-----                   AN-----
A-----N  -&gt; NA-----                   A~-----
B-----~  -&gt; ~B-----   ===&gt; SORT ===&gt;  BA-----
N-----A  -&gt; AN-----                   NA-----
N-----A  -&gt; AN-----                   NA-----
~-----A  -&gt; A~-----                   ~B-----
</code></pre>
</li>
<li>
<p>We can now repeat the procedure, since we know the last column is the BWT transformed string:</p>
<pre><code class="language-py"># STEP 1,2 (repeated)
AN----B  -&gt; BAN----                   ANA----
AN----N  -&gt; NAN----                   ANA----
A~----N  -&gt; NA~----                   A~B----
BA----~  -&gt; ~BA----   ===&gt; SORT ===&gt;  BAN----
NA----A  -&gt; ANA----                   NAN----
NA----A  -&gt; ANA----                   NA~----
~B----A  -&gt; A~B----                   ~BA----
</code></pre>
</li>
<li>
<p>Continuing, this way, we can retrieve the full BWM, and then just pick the row which ends with <code>~</code></p>
<pre><code class="language-py"># BWM
ANANA~B
ANA~BAN
A~BANAN
BANANA~    ===&gt; BANANA~  ---&gt; :D
NANA~BA
NA~BANA
~BANANA
</code></pre>
</li>
</ul>
<ol start="4">
<li>
<p>[5 points] Manually compute the inverse of <code>TGCA~AA</code>. Show your work to get credit (that is you can't just write the final inverse transform but show the steps described above).</p>
<p><strong>Solution</strong></p>
<p>We fill in the BWM matrix as described above.</p>
<ul>
<li>
<p>STEP-0:</p>
<pre><code class="language-py"># STEP-0
------T
------G
------C
------A
------~
------A
------A
</code></pre>
</li>
<li>
<p>STEP-1:</p>
<pre><code class="language-py">A-----T
A-----G
A-----C
C-----A
G-----~
T-----A
~-----A
</code></pre>
</li>
<li>
<p>STEP-2:</p>
<pre><code class="language-py">A-----T -&gt; TA-----                   AC-----
A-----G -&gt; GA-----                   AT-----
A-----C -&gt; CA-----                   A~-----
C-----A -&gt; AC-----   ===&gt; SORT ===&gt;  CA-----
G-----~ -&gt; ~G-----                   GA-----
T-----A -&gt; AT-----                   TA-----
~-----A -&gt; A~-----                   ~G-----
</code></pre>
</li>
<li>
<p>Repeating steps 1-2 above we get:</p>
<pre><code class="language-py"># STEP 1,2 (repeated)
ACA~GAT  
ATACA~G
A~GATAC
CA~GATA
GATACA~
TACA~GA
~GATACA
</code></pre>
</li>
</ul>
<p>Therefore, the inverse-BWT of <code>TGCA~AA</code> is <code>GATACA</code>.</p>
</li>
<li>
<p>[10 points] Implement the BWT inverse transform in <code>hw3_p2.py</code> in the <code>BurrowsWheelerTransform::inverse</code> function. Please add (some) inline comments describing your algorithm. What is the time, memory complexity of your BWT inverse transform implementation for an input of length <code>n</code>?</p>
<p><strong>Solution</strong></p>
<pre><code class="language-python">def inverse(self, bwt_block: DataBlock):
    """
    Generates the inverse of the BWT.
    NOTE: for consistency all forward and inverse functions take in as input
    a DataBlock
    """
    bwt_block_str = "".join(bwt_block.data_list)
    N = len(bwt_block_str)

    ###############################################
    # ADD DETAILS HERE
    # to generate output_str
    # Note: remember to remove the delimiter from the string!
    ###############################################
    output_str = ""

    # decoding loop
    bwm_list = ["" for i in range(N)]
    for _ in range(N):
        bwm_list = [bwt_block_str[i] + bwm_list[i] for i in range(N)]
        bwm_list.sort()

    # find the string which ends with delimiter
    output_str = ""
    for _str in bwm_list:
        if _str.endswith(self.delimiter):
            output_str = _str
            break
    # strip the delimiter and return the input
    output_str = output_str.strip(self.delimiter)

    ###############################################

    return DataBlock(list(output_str))   
</code></pre>
<p>The reverse transform involves $n$ sorting operations, each applied on $n$ strings of length $O(n)$. Each sort operation has complexity $O(n\times n\log n)$ because we operate with $O(n)$ length strings. Thus the overall time complexity is $O(n^3\log n)$.</p>
<p>The memory complexity is $O(n^2)$ because we need to store the matrix.</p>
</li>
<li>
<p>[5 points] Notice that the BWT forward transform of typical english language words/sentences have lots of consecutive repeated letters. For example:</p>
<pre><code class="language-py"># BWT forward transforms of some english words
BANANA -&gt; BNN~AAA
abracadabraabracadabraabracadabra -&gt; rrdd~aadrrrcccraaaaaaaaaaaabbbbbba
hakunamatata -&gt; hnmtt~aauaaka

# first 300 letters of sherlock novel
The Project Gutenberg eBook of The Hound of the Baskervilles, by Arthur Conan DoyleThis eBook is for the use of anyone anywhere in the United States andmost other parts of the world at no cost and with almost no restrictionswhatsoever. You may copy it, give it away or re-use it under the termsof the
--&gt;
yernteedfe.htsfedt,yosgs,ekeeyuttkedsytrroefnrrfftedester  ee       ~e   n    pB th mwn   eielnnnnhhhhnrvsshhhr  ljtthdvhbtktlrohooooo r twtTTtttttwtTtrv th    nwgooosrylia ldraioeuauaoU  oaanns    srooCyiBBc fwcmm YHD oueoeoee  etoePAaeeiiteuuatmoooencsiassiiSiu ai     o rcsraoo h- Gierasy  bapaonne
</code></pre>
<p>So in a way BWT forward transform is approximately sorting the input strings. Can you think of why BWT has this property?</p>
<p><strong>HINT</strong>: In the BWM, the last column forms the BWT while the first $n-1$ columns form suffixes of the last column. For example, if the last entry in a particular row is $x_j$ ($j$ th value in original string), then the first $n-1$ entries in that row would be the cyclic suffix: $x_{j+1}\dots x_{n}  x_1 \dots x_{j-1}$ Given the rows are sorted lexicographically, what can you say about the suffixes of nearby entries in the BWT?</p>
<p><strong>Solution</strong></p>
<p>As the hint suggests, the first $n-1$ rows of the BWM form cyclic suffixes of the last column. Since the first $n-1$ rows are sorted, this means the last column will consist of consecutive characters which share the same suffix. Since English has lots of repeated words, this results in lots of repeated letters in the last column of the cyclic BWM and hence in the BWT forward transform. For instance, consider a large english sentence with frequent occurrence of the word <code>the</code>, say <code>Arguably one of the most popular anime is One Piece. The characters, the arc, the manga illustrations are all amazing</code>. The BWT forward transform on this sentence results in clustering of all the suffixes of every character in this string. This results in <code>t</code> present in all occurrences of word<code>the</code> being clustered together in the last column everytime there is <code>he</code> in the first two columns of BWM.</p>
<pre><code class="language-py">...
...
he most popular anime...t
he characters, the ar...t
he arc, the manga ill...t
he manga illustration...t
...
...
</code></pre>
</li>
</ol>
<p><strong>III. Using BWT for compression</strong></p>
<p>As we saw in previous part, BWT tends to club letters of the input string together. We will use this property to see if we can improve compression. To do this, we also implement another invertible transform called the Move to Front (MTF) transform. The Move To Front transform keeps a table of the symbols in the data, and moves the most recent symbol to the top of the table. The transform output is the index of the symbols in this changing table and thus the symbols that are more frequent in a local context get a lower index. The MTF forward and inverse transforms are already implemented and included in <code>hw3_p2.py</code> and are described quite well in the wikipedia article here: <a href="https://en.wikipedia.org/wiki/Move-to-front_transform">https://en.wikipedia.org/wiki/Move-to-front_transform</a>.</p>
<p>Here are some sample outputs on applying MTF transform:</p>
<pre><code>Input str: aaabbbaaacccaaa
MTF: [97, 0, 0, 98, 0, 0, 1, 0, 0, 99, 0, 0, 1, 0, 0]
</code></pre>
<pre><code>Input str: rrdd~aadrrrcccraaaaaaaaaaaabbbbbba
MTF: [114, 0, 101, 0, 126, 100, 0, 2, 3, 0, 0, 102, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102, 0, 0, 0, 0, 0, 1]
</code></pre>
<p>We use the BWT, MTF transforms to transform the first <code>50,000</code> characters (let's call this the <code>input_data</code>) of our sherlock novel and analyze the entropy of the empirical distribution of the transformed string. (this output can be obtained by running the test <code>test_bwt_mtf_entropy</code> if you are curious!). The output obtained is as below:</p>
<pre><code>Input data: 0-order Empirical Entropy: 4.1322
Input data + BWT: 0-order Empirical Entropy: 4.1325
Input data + MTF: 0-order Empirical Entropy: 4.6885
Input data + BWT + MTF: 0-order Empirical Entropy: 2.8649
</code></pre>
<ol start="7">
<li>
<p>[10 points] Let's try to understand the compression results above a bit better:</p>
<ol>
<li>Given a good implementation of an Arithmetic coder, what is approximate average codelength per symbol you might expect on compressing the <code>input_data</code> using its empirical 0-order distribution (i.e. just the symbol counts)?</li>
<li>Notice that the empirical entropy of <code>input_data</code> is almost the same as that of <code>bwt_data = BWT(input_data)</code>. In fact the empirical entropy of <code>bwt_data</code> is slightly higher than that of <code>input_data</code> (<code>4.1325 vs 4.1322</code>). Justify why is this the case.</li>
<li>Notice that empirical entropy of <code>mtf_bwt_data = MTF(BWT(input_data))</code> is much lower than that of the <code>input_data</code> (<code>2.8649</code> vs <code>4.1322</code>). Can you think of why this is the case?</li>
<li>Based on the numbers you have, describe a compression scheme which achieves average codelength of approximately <code>~2.87 bits/symbol</code> on the <code>input_data</code>. You don't need to implement this compressor, but ensure you clearly describe the steps involved in encoding and decoding. You are free to use any compressors you have learnt in the class.</li>
</ol>
<p>Feel free to experiment with the test functions, to print transformed data or to build better intuition with custom examples. Please include any relevant outputs in your submission which you found useful to answer these questions.</p>
<p><strong>Solution</strong></p>
<ol>
<li>The average codelength will be approximately <code>4.1322</code> bits per symbol as Arithmetic coder is a lossless compression algorithm which can compress to entropy.</li>
<li>Since BWT only permutes the characters in a string (<code>Q5.1</code>), the frequency count to 0th order is still the same for the input and the BWT transformed data. In-fact, we add a character to the list of characters in input string during the BWT transform: the delimiter character <code>~</code>. Hence, the entropy of <code>input_data</code> and <code>bwt_data</code> is such that $H(input) \lesssim H(BWT)$.</li>
<li>As seen in <code>Q2.5</code> and <code>Q2.6</code>, BWT forward transform results in repeating characters which are present in frequent words or group of words because of sorting along suffixes. Since MTF symbol reduces the locally most frequent symbols to lower indices, the MTF transform on BWT transformed data results in a string with lower entropy.</li>
<li>We can use BWT followed by MTF and finally Arithmetic coding to achieve the desired average codelength. The steps involved in encoding and decoding are as follows:
<ol>
<li>BWT transform the input data.</li>
<li>MTF transform the BWT transformed data.</li>
<li>Encode the MTF transformed data using Arithmetic coding.</li>
<li>Decode the Arithmetic coded data.</li>
<li>Inverse MTF transform the decoded data.</li>
<li>Inverse BWT transform the MTF transformed data.</li>
</ol>
</li>
</ol>
</li>
</ol>
<p><em>NOTE:</em> compressors such as <code>bzip2</code>, and <code>bsc</code> in fact use BWT and MTF transforms along with a few other tricks to obtain great compression. BWT algorithms have also found another surprising use: they allow efficient searching over compressed text! Here are more references in case you are interested:</p>
<ol>
<li><a href="https://sourceware.org/bzip2/">bzip2</a>, <a href="http://libbsc.com/">bsc</a></li>
<li>wikipedia article: <a href="https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform">https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform</a></li>
<li>BWT for searching over compressed text: <a href="https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/bwt.pdf">slides</a></li>
</ol>
<h3 id="q3-compression-with-side-information-20-points-1"><a class="header" href="#q3-compression-with-side-information-20-points-1">Q3: Compression with side information (<em>20 points</em>)</a></h3>
<p>Let's consider a simple puzzle:</p>
<ul>
<li>Let $X$ be a 3 bit random variable where each bit is i.i.d $Ber(0.5)$, i.e. $X$ is uniformly distributed  in ${000,001,010,011,100,101,110,111}$ with probability $1/8$,</li>
<li>and $Y= X \oplus e$, where $e \in {000,001,010,100}$ and is independent of $X$. The distribution of $e$ is unknown, i.e. $Y$ can be different from $X$ in at most one bit position. Recall that $\oplus$ is the <a href="https://en.wikipedia.org/wiki/Bitwise_operation#XOR">bitwise-XOR operation</a>.</li>
</ul>
<ol>
<li>
<p>[5 points] Show that $H(X) = 3, H(X|Y) \leq 2$.</p>
<p><strong>HINT</strong>: Consider $H(X,e|Y)$ and write it in two different ways</p>
<p><strong>Solution</strong>
$$ H(X) = -\sum_{x \in {000,001,010,011,100,101,110,111}} P(x) \log_2 P(x) = 3 $$</p>
<p>For computing $H(X|Y)$, let's consider $H(X,e|Y)$ in two different ways as suggested in the Hint:</p>
<p>$$
\begin{align}
H(X,e|Y) &amp;= H(X|Y) + H(e|X, Y) \
&amp;= H(X|Y)\
\end{align}
$$</p>
<p>since $e= X \oplus Y$ and thus is a function of $X$ and $Y$ implying $H(e|X, Y) = 0$ from <code>Q1.1</code>.
Similarly, we can write $H(X,e|Y)$ as:</p>
<p>$$
\begin{align}
H(X,e|Y) &amp;= H(e|Y) + H(X|e, Y) \
&amp;= H(e|Y) \
&amp;\leq H(e) \
&amp;= 2
\end{align}
$$</p>
<p>where again since $X$ is a function of $e$ and $Y$, therefore $H(X|e, Y) = 0$ from <code>Q1.1</code>. Inequality follows from conditioning inequality $\left(H(X|Y) \leq H(X)\right)$ and $H(e) = 2$ since $e$ is uniformly distributed  in ${000,001,010,100}$ with probability $1/4$.</p>
</li>
<li>
<p>[5 points] Kakashi wants to losslessly encode $X$ and send the encoded coderword to Sasuke. What is the optimal compression scheme for Kakashi?</p>
<p><strong>Solution</strong></p>
<p>Kakashi needs 3 bits to encode $X$ since $H(X) = 3$. Kakashi can just send $X$ as is to Sasuke as none-of-the-lossless encoder can do better than this.</p>
</li>
<li>
<p>[5 points] Now let's say both Kakashi and Sasuke have access to $Y$, i.e. $Y$ is the side-information available to both Kakashi and Sasuke (through side-information ninjutsu :P). In that case, show that Kakashi can do better than in <code>Q3.2</code>, by using <code>2 bits</code> to transmit $X$.</p>
<p><strong>Solution</strong></p>
<p>Now since the entropy of $H(X|Y)\leq2$, Kakashi should be able to get away by just transmitting 2 bits instead. The hint is in the derivation of $H(X|Y)$ in <code>Q3.3</code>. Since both Kakashi and Sasuke has access to both $X$ and $Y$, Kakashi can just transmit $e= X \oplus Y$, and Sasuke can recover $X$ from $e$ by $X = Y \oplus e$. We need 2 bits to encode $e$, e.g. by using the following scheme:</p>
</li>
</ol>
<pre><code>    | e   | 2-bit encoding |
    |-----|----------------|
    | 000 | 00             |
    | 001 | 01             |
    | 010 | 10             |
    | 100 | 11             |
</code></pre>
<ol start="4">
<li>
<p>[5 points] Unfortunately Kakashi lost access to $Y$, and only knows $X$ but Sasuke still has access to the side information $Y$. Show that in this case Kakashi can still losslessly encode $X$ and send it to Sasuke using <code>2 bits</code>, i.e. surprisingly we just need the side-information at the decoder, and not necessarily at the encoder.</p>
<p><strong>Solution</strong>
This problem has a very elegant visualization by noticing that $e$ only has <code>1</code> or <code>0</code> ones and thus flips at most one bit of $X$ (or <a href="https://en.wikipedia.org/wiki/Hamming_distance">hamming distance</a> $\leq 1$). Let us represent $X$ on a unit cube where each axis represents one bit of $X$</p>
<p><img src="homeworks/./figures/puzzle_sol_1.png" alt="Figure 1" /></p>
<p>Now notice that $e$ flips at most one bit of $X$ and thus can be represented by a line segment on the unit cube. The following figure shows the possible $Y$ values for $X=000$ (red) and $X=111$ (blue).</p>
<p><img src="homeworks/./figures/puzzle_sol_2.png" alt="Figure 2" /></p>
<p>Note how these are distinct sets, hence if you knew $Y$ and also knew that $X$ is either $000$ or $111$, you are able to decode. Thus Kakashi can send $X$ using 2 bits by basically pairing symbols with hamming distance $&gt; 2$. For instace one possible encoding is:</p>
</li>
</ol>
<pre><code>    | X          | 2-bit encoding |
    |------------|----------------|
    | 000 or 111 | 00             |
    | 001 or 110 | 01             |
    | 010 or 101 | 10             |
    | 100 or 011 | 11             |
</code></pre>
<p>NOTE: It is quite fascinating that we need side-information only at the decoder. This property can be generalized to more general scenarios, and is the foundation for distributed data compression. E.g. see <a href="https://en.wikipedia.org/wiki/Slepian%E2%80%93Wolf_coding">Slepian-Wolf coding</a> and <a href="https://en.wikipedia.org/wiki/Distributed_source_coding#:~:text=generate%20syndrome%20bits.-,Wyner%E2%80%93Ziv%20coding%20%E2%80%93%20lossy%20distributed%20coding,and%20corresponding%20reconstruction%20method%20design">Wyner-Ziv coding</a> for more details.</p>
<h3 id="q4-kth-order-adaptive-arithmetic-coding-40-points-1"><a class="header" href="#q4-kth-order-adaptive-arithmetic-coding-40-points-1">Q4: kth order adaptive arithmetic coding (<em>40 points</em>)</a></h3>
<p>Shubham wants to test the compression performance of the k-th order context models using Arithmetic Coding he has implemented. However, one problem is that for unknown sources it is very difficult to compute the fundamental limit on the average codelength. So, he decides to test his algorithms on a synthetic dataset.</p>
<p>He generates data using a noisy version of <a href="https://en.wikipedia.org/wiki/Linear-feedback_shift_register">Linear Feedback Shift Register based Pseudorandom generator</a>. The name sounds quite complicated, but it is a simple concept (you don't need to know anything about it to attempt this problem). The pseudocode to generate noisy-LFSR sequence is given below (also given in HW code <code>hw3_p4.py</code>):</p>
<pre><code class="language-py">def pseudo_random_LFSR_generator(data_size, tap, noise_prob=0):
    # initial sequence = [1,0,0,0,...] of length tap
    initial_sequence = [1] + [0] * (tap - 1)

    # output sequence
    output_sequence = list(initial_sequence)
    for _ in range(data_size - tap):
        s = output_sequence[-1] ^ output_sequence[-tap]  # xor
        if noise_prob &gt; 0: 
            s = s ^ Bernoulli(p=noise_prob) # add noise
        output_sequence.append(s)
    return output_sequence
</code></pre>
<p>Our <code>pseudo_random_LFSR_generator</code> generates an output sequence of length <code>data_size</code>. At each step, it calculates the next bit using XOR (<code>a ^ b</code> is <code>True</code>/<code>1</code> if exactly one of <code>a</code> and <code>b</code> is <code>True</code>/<code>1</code>) between the last bit in the sequence and the bit located <code>tap</code> positions before it (this simulates LFSR behavior). Optionally, if <code>noise_prob</code> is greater than <code>0</code>, it adds noise to the calculated bit by XOR-ing it with a random binary value generated with a given probability (<code>noise_prob</code>). The calculated (possibly noisy) bit is appended to the sequence in each iteration. To initialize the sequence, we always start it with <code>[1,0,0,0,...]</code> so that we have <code>tap</code> number of bits available to calculate the next bit.</p>
<p>For concreteness let's take a specific parameter setting (function in <code>hw3_p4.py</code>):</p>
<pre><code class="language-py">pseudo_random_LFSR_generator(data_size=10000, tap=3, noise_prob=0)
</code></pre>
<p>We see that the first 12 symbols of the sequence looks like:</p>
<pre><code>1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, ...
</code></pre>
<p>Notice that the sequence looks random-ish but starts repeating after 8 symbols. In fact if we look at the first 7 3-mers (substrings of length 3), i.e. <code>100</code>, <code>001</code>, ..., <code>010</code>, then we see that they are all unique and do not repeat! The LFSR sequence with <code>noise_prob=0</code>, is in fact a simple pseudo-random generator used in the past. We will use this source to further our understanding of context based compression.</p>
<p>Let's start by defining the $k^{\text{th}}$ order <code>empirical entropy</code> of the sequence. Recall, if your sequence is $x_0, x_1, x_2, \ldots, x_n$, the  $k^{\text{th}}$-order empirical entropy of the sequence can be computed by making a joint and conditional probability distribution table using counts of the previously seen symbols (we saw this in class in L9 on context based AC lecture). Specifically, assuming the alphabet is $\mathcal{X}$, for any $(x_0, x_1, x_2, \ldots x_k) \in \mathcal{X}^{k+1}$, we can define the empirical joint probability as:
$$
p(x_0, x_1, x_2, \ldots x_k) = \frac{\text{count}(x_0, x_1, x_2, \ldots x_k)}{\text{total count of k+1-tuples}}
$$</p>
<p>the empirical conditional probability as:
$$
p(x_k | x_0, x_1, x_2, \ldots x_{k-1}) = \frac{p(x_0, x_1, x_2, \ldots x_k)}{p(x_0, x_1, x_2, \ldots x_{k-1})} = \frac{p(x_0, x_1, x_2, \ldots x_k)}{\sum_{x' \in \mathcal{X}} p(x_0, x_1, x_2, \ldots x_{k-1}, x')}
$$</p>
<p>and the $k^{\text{th}}$-order empirical entropy is then given by summing over all possible $(k+1)$-tuples:</p>
<p>$$H_k(X) = \sum_{(x_0, x_1, x_2, \ldots, x_k) \in \mathcal{X}^{k+1}} p(x_0, x_1, x_2, \ldots, x_{k}) \log {\frac{1} {p(x_k | x_0, x_1, x_2, \ldots, x_{k-1})}}$$</p>
<ol>
<li>
<p>[5 points] Show that for infinite data ($n \rightarrow \infty$) and $k_1 &gt; k_2$,
$$H_{k_1}(X) \leq H_{k_2}(X)$$
i.e. $k_1^{\text{th}}$-order empirical entropy is always less than (or equal to) $k_2^{\text{th}}$-order empirical entropy.</p>
<p>You can assume the data distribution is stationary and ergodic (or in simple words you can assume the empirical probabilities converge to the stationary distribution).</p>
<p><strong>Solution</strong>
Given the convergence guarantees, we can assume that the empirical probabilities converge to the true probabilities. Now the $k^{\text{th}}$-order empirical entropy can be written as
$H_k(X) = H(X_k|X_0,X_1,\ldots,X_{k-1})$.</p>
<p>Using stationarity, let's rewrite $H_{k_1}(X)$ and $ H_{k_2}(X)$ (recall that $k_1 &gt; k_2$):
$$H_{k_1}(X) = H(X_{k_1}|X_0,X_1,\ldots,X_{k_1-1})$$
$$H_{k_2}(X) = H(X_{k_2}|X_0,X_1,\ldots,X_{k_2-1}) = H(X_{k_1}|X_{k_1-k_2},X_{k_1-k_2+1},\ldots,X_{k_1-1})$$</p>
<p>In class we learned that conditioning reduces entropy (i.e. $H(X|Y) \leq H(X)$). We can clearly see above that $H_{k_1}(X) = H(X_{k_1}|X_0,X_1,\ldots,X_{k_1-1})$ has more conditioning variables than $H_{k_2}(X) = H(X_{k_1}|X_{k_1-k_2},X_{k_1-k_2+1},\ldots,X_{k_1-1})$. Thus, by the conditioning reduces entropy property, we have
$$H_{k_1}(X) \leq H_{k_2}(X)$$</p>
<p>(NOT REQUIRED FOR GRADING)</p>
<p>If you look carefully, it doesn't seem immediately obvious that the conditioning reduces entropy property: $H(X|Y) \leq H(X)$ you saw previously directly implies the result when another conditioning variable is added (i.e., $H(X|Y,Z) \leq H(X|Z)$). As you see in our case, the $Z$ corresponds to $X_{k_1-k_2},X_{k_1-k_2+1},\ldots,X_{k_1-1}$ and the $Y$ corresponds to $X_0,X_1,\ldots,X_{k_1-k_2-1}$. But it's easy enough to prove by first fixing $Z=z$ and then applying the conditioning reduces entropy property on $H(X|Y,Z=z) \leq H(X|Z=z)$, and then averaging over $Z$.</p>
<p>(If you are curious) There are several ways to prove conditioning reduces entropy. The textbook <em>Elements of Information Theory</em> by Cover and Thomas has a proof in Chapter 2. That relies on the fact that $I(X;Y) = H(X) - H(X|Y)$ and $I(X;Y) \geq 0$, therfore $H(X) \geq H(X|Y)$. An alternative proof, which uses Jensen's inequality can be found <a href="https://www.cim.mcgill.ca/~langer/423/lecture16.pdf">here</a>, and proceeds as follows:</p>
<p>$$
\begin{aligned}
H(X \mid Y)-H(X) &amp; =\sum_{X, Y} p(X=i, Y=j) \log \frac{1}{p(X=i \mid Y=j)}-\sum_X p(X=i) \log \frac{1}{p(X=i)} \
&amp; =\sum_{X, Y} p(X=i, Y=j)\left(\log \frac{p(Y=j)}{p(X=i, Y=j)}+\log p(X=i)\right) \
&amp; =\sum_{X, Y} p(X=i, Y=j)\left(\log \frac{p(Y=j) p(X=i)}{p(X=i, Y=j)}\right) \
&amp; \leq \log \sum_{X, Y} p(X=i, Y=j) \frac{p(Y=j) p(X=i)}{p(X=i, Y=j)}, \text { by Jensen's inequality } \
&amp; =0 .
\end{aligned}
$$
Of course, the positivity of mutual information is a consequence of the non-negativity of the Kullback-Leibler divergence, which is itself a consequence of Jensen's inequality.</p>
</li>
<li>
<p>[10 points] Next let's calculate $H_k(X)$ for the above given sequence with <code>tap=3</code>, <code>noise_prob=0</code>.
(Recall the sequence from above: <code>1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, ...</code>)</p>
<ol>
<li>Compute the $0^{\text{th}}$ order count-distribution and empirical entropy of the sequence. <strong>HINT</strong>: sequence repeats itself!</li>
<li>Compute the $1^{\text{st}}$ order count-distribution and empirical entropy of the sequence.</li>
<li>Finally, argue that for $k \geq 3$, the $k^{\text{th}}$-order empirical entropy of the sequence is <code>0</code>.</li>
</ol>
<p><strong>Solution</strong></p>
<ol>
<li>Since the sequence repeats after 7 bits, the empirical probabilities are just given by their respective frequencies in the first 7 bits (i.e. $p(x_0=0) = \frac{3}{7}$ and $p(x_0=1) = \frac{4}{7}$).
$$-\frac{4}{7}\log_2{\frac{4}{7}} - \frac{3}{7}\log_2{\frac{3}{7}} \approx 0.985$$</li>
<li>For $k=1$, we have to consider all tuples of length 2 (i.e. $(00,01,10,11)$). Before the pattern repeats, we record the following frequency counts: ${00: 1, 01: 2, 10: 2, 11: 2}$. The total count of 2 tuples is 7. Thus, the empirical joint probabilities are given by ${00: \frac{1}{7}, 01: \frac{2}{7}, 10: \frac{2}{7}, 11: \frac{2}{7}}$. Marginalizing across these, we see that</li>
</ol>
<p>$$
p(x_1|x_0=0) =
\begin{cases}
\frac{1}{3} &amp; x_1 = 0\
\frac{2}{3} &amp; x_1 = 1
\end{cases}
$$</p>
<p>$$
p(x_1|x_0=1) =
\begin{cases}
\frac{1}{2} &amp; x_1 = 0\
\frac{1}{2} &amp; x_1 = 1
\end{cases}
$$</p>
<p>Then, we have $H_1(X) = -(\frac{3}{7} (\frac{1}{3}\log_2{\frac{1}{3}}+\frac{2}{3}\log_2{\frac{2}{3}})+\frac{4}{7}(\frac{1}{2}\log_2{\frac{1}{2}}+\frac{1}{2}\log_2{\frac{1}{2}})) \approx 0.965 $</p>
<ol start="3">
<li>For $k \geq 3$, The conditional probabilities, $p(x_k|x_0,x_1,\ldots,x_{k-1})$ are all 1 for the non-zero $p(x_0,x_1,x_2,\ldots,x_k)$ because the the $k$th bit is completely determined by the preceeding <code>TAP</code> bits, so when <code>TAP=3</code> conditioning on the previous 3 bits is enough to give us complete information about the $k$th bit. Of course, conditioning on more than the 3 previous bits won't hurt us since all we need for the $k$th bit to be deterministic are the previous 3, so conditioning on $k&gt;3$ will also do the trick. Thus, $H_k(X) = 0$ for $k \geq 3$.</li>
</ol>
</li>
<li>
<p>[5 points] Now, Shubham decides to use Adaptive Arithmetic coding to compress data from this source. For the case with no added noise (<code>noise_prob=0</code>), the number of bits/symbol we achieve is as follows for different context size <code>k</code>. You can use the test code in <code>hw3_p4.py</code> to generate these numbers by changing and choosing appropriate parameters. Use the <code>-s</code> flag in pytest to dump the output even for successful tests.</p>
<pre><code>----------
Data generated as: X[n] = X[n-1] âŠ• X[n-3]
data_size=10000
AdaptiveArithmeticCoding, k=0, avg_codelen: 0.989
AdaptiveArithmeticCoding, k=1, avg_codelen: 0.969
AdaptiveArithmeticCoding, k=2, avg_codelen: 0.863
AdaptiveArithmeticCoding, k=3, avg_codelen: 0.012
AdaptiveArithmeticCoding, k=4, avg_codelen: 0.011
AdaptiveArithmeticCoding, k=7, avg_codelen: 0.011
AdaptiveArithmeticCoding, k=15, avg_codelen: 0.012
AdaptiveArithmeticCoding, k=22, avg_codelen: 0.013
</code></pre>
<p>Argue why these results make sense. In particular, argue that Adaptive Arithmetic coding with order <code>k=0,1,2</code> cannot do much better than <code>1 bit/symbol</code>. Also, argue that for <code>k &gt;= 3</code>, the compression should be <code>~0 bits/symbol</code>.</p>
<p><strong>Solution</strong>
From our answer to the previous part of the question, we see that the empirical entropy of the sequence is approximately 1 bit/symbol for <code>k=0,1</code>. We expect the same for <code>k=2</code> since the $k$th bit is completely determined by the bit located <code>TAP</code> indices earlier, so in the case of <code>TAP=3</code>, the previous 2 bits don't really give us much predictive information. However, as we explained in our solution to the previous part of the question, the $k$th bit is completely determined by the bit located <code>TAP</code> indcies earlier, so in the case of <code>TAP=3</code>, the previous 3 bits give us complete information about the $k$th bit. Thus, we expect the empirical entropy to be 0 bits/symbol for <code>k &gt;= 3</code>. Assuming our Arithmetic coder is optimal, we expect the average codelength to be close to the empirical entropy, so for <code>k=0,1,2</code> we shouldn't be able to do much better than <code>1 bit/symbol</code>, and for <code>k &gt;= 3</code> we should achieve close to <code>0 bits/symbol</code>.</p>
</li>
</ol>
<p>Next, Shubham decides to encode noisy-LFSR sources with <code>noise_prob=0.01</code> and <code>TAP=3,7,15</code>. The noisy LFSR is very similar to the non-noisy version, except we add a bit of noise after xoring the two past symbols as shown in the pseduo-code above.</p>
<ol start="4">
<li>
<p>[5 points] For <code>TAP=3, noise_prob=0.01</code>, what is the fundamental limit to which you can compress this sequence, i.e. what is $k^{\text{th}}$-order empirical entropy of the sequence for $k \geq 3$ and large $n$?</p>
<p><strong>Solution</strong>
Entropy is a measure of uncertainty. Let's think about where that uncertainty is coming from. We have seen above that for $k \geq 3$ when there is no noise in the LFSR sequence, $H_k(X)=0$. Now however, we have added a Bernoulli random variable to the mix. Each bit has a <code>noise_prob</code> chance of being flipped. So given the past $k \geq 3$ bits, the only remaining uncertainty is described via a Bernoulli random variable with probability <code>noise_prob</code>. So the empirical entropy will just be the entropy of this Bernoulli random variable  Thus, the $k^{\text{th}}$-order empirical entropy of the sequence is $H_k(X) = -\text{noise_prob}\times \log_2{\text{noise_prob}} - (1-\text{noise_prob})\times\log_2{(1-\text{noise_prob})}$. For <code>noise_prob=0.01</code>, this is approximately <code>0.081 bits/symbol</code>.</p>
</li>
<li>
<p>[5 points] Next, we encode noisy-LFSR sequence with parameters <code>noise_prob=0.01</code> and <code>TAP=3,7,15</code> using Adaptive Arithmetic coding. The number of bits/symbol we achieve is as follows for different context size <code>k</code>. You can again use the test code in <code>hw3_p4.py</code> to generate these numbers by changing and choosing appropriate parameters. In these cases we observe the following average codelength, for a sequence of length <code>10000</code></p>
<pre><code class="language-py">----------
Data generated as: X[n] = X[n-1] âŠ• X[n-3] âŠ• Bern_noise(0.01)
DATA_SIZE=10000
AdaptiveArithmeticCoding, k=0, avg_codelen: 1.003
AdaptiveArithmeticCoding, k=1, avg_codelen: 1.002
AdaptiveArithmeticCoding, k=2, avg_codelen: 0.999
AdaptiveArithmeticCoding, k=3, avg_codelen: 0.087
AdaptiveArithmeticCoding, k=4, avg_codelen: 0.089
AdaptiveArithmeticCoding, k=7, avg_codelen: 0.097
AdaptiveArithmeticCoding, k=15, avg_codelen: 0.121
AdaptiveArithmeticCoding, k=22, avg_codelen: 0.146
AdaptiveArithmeticCoding, k=24, avg_codelen: 0.152

----------
Data generated as: X[n] = X[n-1] âŠ• X[n-7] âŠ• Bern_noise(0.01)
DATA_SIZE=10000
AdaptiveArithmeticCoding, k=0, avg_codelen: 1.004
AdaptiveArithmeticCoding, k=1, avg_codelen: 1.004
AdaptiveArithmeticCoding, k=2, avg_codelen: 1.005
AdaptiveArithmeticCoding, k=3, avg_codelen: 1.007
AdaptiveArithmeticCoding, k=4, avg_codelen: 1.009
AdaptiveArithmeticCoding, k=7, avg_codelen: 0.139
AdaptiveArithmeticCoding, k=15, avg_codelen: 0.192
AdaptiveArithmeticCoding, k=22, avg_codelen: 0.240
AdaptiveArithmeticCoding, k=24, avg_codelen: 0.254

----------
Data generated as: X[n] = X[n-1] âŠ• X[n-15] âŠ• Bern_noise(0.01)
DATA_SIZE=10000
AdaptiveArithmeticCoding, k=0, avg_codelen: 1.004
AdaptiveArithmeticCoding, k=1, avg_codelen: 1.004
AdaptiveArithmeticCoding, k=2, avg_codelen: 1.005
AdaptiveArithmeticCoding, k=3, avg_codelen: 1.006
AdaptiveArithmeticCoding, k=4, avg_codelen: 1.008
AdaptiveArithmeticCoding, k=7, avg_codelen: 1.031
AdaptiveArithmeticCoding, k=15, avg_codelen: 0.949
AdaptiveArithmeticCoding, k=22, avg_codelen: 0.955
AdaptiveArithmeticCoding, k=24, avg_codelen: 0.957
</code></pre>
<p>Notice that the average codelength for <code>TAP=3,7</code> is at its minima for <code>k=TAP</code> value and then increases as we further increase <code>k</code> which seems to suggest the result we saw in <code>Q4.1</code> is wrong. Argue:</p>
<ol>
<li>Why do these results still make sense and do not contradict theory?</li>
<li>For <code>TAP=15</code>, even for context size <code>k=15</code>, why is the model not able to compress the data very well?</li>
</ol>
<p><strong>Solution</strong></p>
<ol>
<li>In this case, the conditional empirical entropy for infinite data will be around 1 bit/symbol for <code>k</code> less than <code>TAP</code> and the binary entropy of <code>noise_prob</code> for <code>k</code> greater than <code>TAP</code>. However in practice for finite data, you might not achieve the empirical entropy for infinite data since you will not get enough counts for each context, especially at higher <code>k</code>.
The minor increase for higher <code>k</code> is due to the fact that we are using a finite length sequence, so the empirical probabilities are not exactly equal to the true probabilities (especially at higher <code>k</code> where counts are sparse). So we are not violating the "conditioning reduces entropy" principle used in part 4.1.</li>
<li>With finite data, you might not achieve the empirical entropy limit for infinite data since you will not get enough counts for each context, especially at higher <code>k</code>. In this case $2^{15}$ is much higher than the length of the sequence, so we don't have enough counts for each context, and hence the adaptive arithmetic coder is not able to learn the distribution well.</li>
</ol>
</li>
<li>
<p>[10 points] Instead of using Adaptive Arithmetic coding, Jiwon suggested that if we know the source parameters (i.e. <code>TAP</code> and <code>noise_prob</code>), then we could predict a better probability distribution of the next symbol based on the past, and use this for performing Arithmetic coding. Jiwon thinks this implementation should work for arbitrary values of <code>TAP</code> and input sequence lengths. Complete Jiwon's <code>update_model</code> logic in <code>NoisyLFSRFreqModel</code> provided in <code>hw3_p4.py</code>. For the first <code>TAP</code> symbols you can use arbitrary predictions (e.g., uniform distribution) without affecting the results for large sequences.</p>
<p>Once you have implemented the model, run the tests (<code>py.test -s hw3_p4.py</code>) for <code>TAP=3,7,15</code> and <code>noise_prob=0.01</code> again and report the average codelength obtained for the LFSR model. How do these results compare with the Adaptive Arithmetic coding results (from part 5) and to the expected theoretical limit (from part 4)? Explain why this is the case.</p>
<p><strong>Solution</strong>
As seen in the code below, we update the model based on the past <code>TAP</code> symbols. Specifically, we predict the next symbol using the XOR of the last symbol and the symbol located <code>TAP</code> positions before it. We then assign probabilities to the next symbol based on this prediction and the given <code>noise_prob</code>. If we have not yet seen <code>TAP</code> symbols, we use a uniform distribution.</p>
<p>When we run the tests for <code>TAP=3,7,15</code> and <code>noise_prob=0.01</code>, we observe that the average codelengths are much closer to the theoretical limit compared to the Adaptive Arithmetic coding results from part 5. This is because by knowing the source parameters, we can make more accurate predictions about the next symbol, leading to a better probability distribution for arithmetic coding. This allows us to achieve compression rates that are closer to the empirical entropy of the source, especially for larger values of <code>TAP</code> where adaptive methods struggle due to sparse data.</p>
<pre><code>LFSR Model, tap=3, avg_codelen: 0.080
LFSR Model, tap=7, avg_codelen: 0.081
LFSR Model, tap=15, avg_codelen: 0.081
</code></pre>
<pre><code class="language-python">def update_model(self, s):
    """
    Updates the freq model (`self.freqs_current`) for the next symbol
    based on the latest encoded symbol s

    :param s: latest encoded symbol
    """
    # ###############################
    # ADD CODE HERE FOR: updating self.freqs_current, this is the probability
    # distribution to be used for next symbol.
    #
    # The sequence of operations is encode(s1), update_model(s1), encode(s2), update_model(s2), ...
    # and so after calling update_model(s), the model should be ready to encode the next symbol.
    #
    # Check the implementation in `AdaptiveIIDFreqModel` and
    # `AdaptiveOrderKFreqModel` in scl.compressors.probability_models for inspiration.
    #
    # HINTS:
    # (1) you need to keep track of the context, in this case past self.tap number of symbols.
    # You can save the past TAP symbols in a buffer: self.past_few_symbols.
    # You may use these to update the model and to set `self.freqs_current`
    # (2) you should use the `Frequencies` class to create the self.freqs_current;
    # similar to what is done in the __init__ function above.
    # (3) frequencies are stored as integers for float probability values as we have seen in class
    # and probabilities are invariant to the total frequency, so you can set the
    # total frequency to any value you want. For the autograder purposes, you can assume noise_prob to be in
    # multiples of 0.001 (e.g., 0.001, 0.002, 0.003, etc.), i.e. noise_prob = 0.001 * noise_prob_int.
    # You can also use the helper function `convert_float_prob_to_int` with `M=1000` 
    # to convert the float probability to a valid int by scaling by 1000.
    # raise NotImplementedError
    ###############################
    self.past_few_symbols.append(s)
    if len(self.past_few_symbols) &gt; self.tap:
        self.past_few_symbols.pop(0)
    if len(self.past_few_symbols) == self.tap:
        predicted_symbol = self.past_few_symbols[-1] ^ self.past_few_symbols[0]
        prob_1 = (1 - self.noise_prob) if predicted_symbol == 1 else self.noise_prob
        prob_0 = 1 - prob_1
        freq_1 = convert_float_prob_to_int(prob_1, M=1000)
        freq_0 = convert_float_prob_to_int(prob_0, M=1000)
        self.freqs_current = Frequencies({0: freq_0, 1: freq_1})
    else:
        self.freqs_current = Frequencies({0: 1, 1: 1}) # uniform distribution for first TAP symbols
    ################################

    aec_params = AECParams() # params used for arithmetic coding in SCL
    assert self.freqs_current.total_freq &lt;= aec_params.MAX_ALLOWED_TOTAL_FREQ, (
        f"Total freq {self.freqs_current.total_freq} is greater than "
        f"max allowed total freq {aec_params.MAX_ALLOWED_TOTAL_FREQ} for arithmetic coding in SCL. This leads to"
        f"precision and speed issues. Try reducing the total freq by a factor of 2 or more."
    )
    self.freqs_current._validate_freq_dist(self.freqs_current.freq_dict) # check if freqs are valid datatype
</code></pre>
</li>
</ol>
<p>NOTE: References provided with Lecture 9 on Context-based Arithmetic Coding will be useful in attempting this problem (SCL implementation of adaptive arithmetic coding, notes, slides, etc.)</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ee274-fall-25-homework-4"><a class="header" href="#ee274-fall-25-homework-4">EE274 (Fall 25): Homework-4</a></h1>
<ul>
<li><strong>Focus area:</strong> Lossy compression</li>
<li><strong>Due Date:</strong> 12/02, <em>Tue</em>, midnight (11:59 PM)</li>
<li><strong>Weightage:</strong> 15%</li>
<li><strong>Total Points:</strong> 120</li>
<li><strong>Submission Instructions:</strong> Provided at the end of HW (ensure you read these!)</li>
<li><strong>Submission link:</strong>
<ul>
<li>For written part (70 Points): <a href="https://www.gradescope.com/courses/1140353/assignments/7098945">HW4-Written</a></li>
<li>For programming part (50 Points): <a href="https://www.gradescope.com/courses/1140353/assignments/7098941">HW4-Code</a></li>
</ul>
</li>
</ul>
<p><em>Please ensure that you follow the <a href="https://communitystandards.stanford.edu/policies-guidance/honor-code">Stanford Honor Code</a> while doing the homework. You are encouraged to discuss the homework with your classmates, but you should write your own solutions and code. You are also encouraged to use the internet to look up the syntax of the programming language, but you should not copy-paste code from the internet. If you are unsure about what is allowed, please ask the instructors.</em></p>
<p><strong>Note for the coding part</strong><br>
Before starting the coding related questions ensure following instructions from HW1 are followed:</p>
<ul>
<li>Ensure you are using the latest version of the SCL <code>EE274/HWs</code> GitHub branch. To ensure run the following command in the SCL repository you cloned from HW1:
<pre><code class="language-sh">git status
</code></pre>
You should get an output saying <code>On branch EE274_Fall25/HWs</code>. If not, run the following command to switch to the correct branch:
<pre><code class="language-sh">git checkout EE274_Fall25/HWs
</code></pre>
Finally ensure you are on the latest commit by running:
<pre><code class="language-sh">git pull
</code></pre>
You should see a <code>HW4</code> folder in the <code>HWs</code> folder.</li>
<li>Ensure you are in the right conda environment you created in <code>HW1</code>. To ensure run the following command:
<pre><code class="language-sh">conda activate ee274_env
</code></pre>
</li>
<li>Before starting, ensure the previous tests are passing as expected. To do so, run following from <code>stanford_compression_library</code> folder:
<pre><code class="language-sh">find scl -name "*.py" -exec py.test -s -v {} +
</code></pre>
and ensure tests except in HW folders pass.</li>
</ul>
<h3 id="q1-mutual-information-and-rate-distortion-20-points"><a class="header" href="#q1-mutual-information-and-rate-distortion-20-points">Q1: Mutual Information and Rate Distortion <em>(20 points)</em></a></h3>
<ol>
<li>
<p>[5 points] Consider random variables $X$ and $Y$ and let $f$ be an arbitrary function applied to $Y$. Show that $$I(X; Y) \geq I(X; f(Y))$$
When does equality hold?</p>
<p><strong>Note</strong>: This is a special case of the <strong>data-processing inequality</strong>, which more generally holds whenever $X-Y-Z$ form a Markov chain (i.e., $X$ and $Z$ are conditionally independent given $Y$) and says that $I(X; Y) \geq I(X; Z)$. Intuitively, this says that no processing of $Y$, deterministic or random, can increase the information that $Y$ contains about $X$. You can't boost information about $X$ contained in $Y$ just by processing it further!</p>
</li>
<li>
<p>[15 points] Consider a source $X \sim \mathrm{i.i.d.}\ Ber({1\over 2})$ which we wish to lossily compress. The reconstruction $Y$ is allowed to take values in ${0,1,e}$ where $e$ represents an erasure symbol. The distortion function is given by $$
d(x,y)=\begin{cases}
0, \ \ y=x\
1, \ \ y=e\
\infty, \ \mathrm{otherwise}
\end{cases}$$
In words, erasures incur a distortion of $1$, but making a mistake is not allowed (infinite distortion).</p>
<p>a. [10 points] Show that $R(D) = \min_{E[d(x,y)]\leq D} I(X;Y) = 1-D$ for $D\in [0,1]$.</p>
<p><strong>Hint:</strong> You can first show that $I(X;Y) = H(X) - P(Y=e)H(X|Y=e)$ under the condition $E[d(x,y)]\leq D$. Can you relate $P(Y=e)$ to $E[d(X,Y)]$?</p>
<p>b. [5 points] Suggest a simple and efficient scheme for some finite block size $k$ which achieves the optimal rate-distortion performance for this setting. You can assume that $D$ is rational, and may choose an appropriate value of $k$ accordingly.</p>
<p><strong>Hint:</strong> You can start working with a simple example where $D=\frac{1}{2}, k=2$ and the optimal rate is $\frac{1}{2}$, i.e. you encode two input bits into a single codeword bit.</p>
</li>
</ol>
<h3 id="q2-vector-quantization-20-points"><a class="header" href="#q2-vector-quantization-20-points">Q2: Vector Quantization <em>(20 points)</em></a></h3>
<p>We have studied vector quantization (VQ) in class. In this question we will first implement part of vector quantization and then study the performance of vector quantization for a correlated gaussian process.</p>
<ol>
<li>
<p>[10 points] Implement the <code>build_kmeans_codebook()</code> method in <code>vector_quantizer.py</code>. You cannot use the existing kmeans implementations for this part (e.g. <code>build_kmeans_codebook_scipy()</code> method) and must implement your own version. You can use the methods pre-implemented in <code>vector_quantizer</code> module. It will be instructive to look at the provided test cases in the module. Ensure that the test case <code>test_vector_quantization</code> passes on the provided data. You can use the following command to run the test cases:</p>
<pre><code class="language-python">py.test -s -v vector_quantizer.py -k test_vector_quantization               
</code></pre>
</li>
<li>
<p>[10 points] Now we use the VQ to encode a correlated source. Consider a simple gaussian process where $$X_n = \rho X_{n-1} + \sqrt{1 - \rho^2} \mathcal{N}(0, \sigma^2) $$
the second term is independent of $X_0,\dots,X_{n-1}$, and $$X_0 \sim \mathcal{N}(0, \sigma^2)$$</p>
<p>Below we visualize the Rate-Distortion performance as well as encoding complexity for different values of $k$ (block sizes) and $\rho$ (parameter in the gaussian process), for a given rate of <code>1 bits per symbol</code>. Based on these plots and what we have learnt in the class, answer following questions:</p>
<p><img src="homeworks/plots/VQ_expts.png" alt="VQ" /></p>
<p><em>How to read the plots?</em> We fix rate = 1 and use vector quantization. The left plot shows the MSE distortion (y-axis) vs. the block size $k$ (x-axis) for different values of $\rho$. The right plot shows the MSE distortion (y-axis) vs. the Encoding Time (x-axis) for different values of $\rho$. Note that the points in the second plot correspond to different block sizes which can be inferred based on the matching MSE values from the first plot.</p>
<p>a. [2 points] What is the theoretical distortion limit for $\rho=0$ at rate of <code>1 bits per symbol</code>?</p>
<p>b. [4 points] What do you observe as $k$ increases for a given $\rho$? Comment on the performance and complexity of the encoder as $k$ increases. Justify your observations in a few lines.</p>
<p>c. [4 points] Focusing on the left plot, what do you observe as $\rho$ increases? Comment on how the change in performance of the encoder with increasing $k$ depends on the value of $\rho$. Justify your observations in a few lines.</p>
</li>
</ol>
<h3 id="q3-lower-bounds-via-information-theory-35-points"><a class="header" href="#q3-lower-bounds-via-information-theory-35-points">Q3: Lower Bounds via Information Theory <em>(35 points)</em></a></h3>
<p>At the annual <em>Claude Shannon rowing contest</em>, there are $n$ participants, with $n-1$ out of them having exactly same strength but one of the rowers is exceptionally strong. The contest aims at finding that one strongest rower. The competition organizers are unfortunately limited on the funds, and so want to minimize the number of rounds to declare the winner.</p>
<p>As a world renowned information theorist and compression expert, you have been roped in as a consultant, and have been tasked with deciding the match fixtures so that the exceptionally strong rower can be figured out in minimal number of matches. You have the following information:</p>
<ul>
<li>The matches can be between any two teams, where both team consists of an equal number of rowers. E.g. you can decide to have match between Rower 1 and Rower 2, or between (Rower 1, Rower 5) and (Rower 3, Rower 4) or between (Rower 1, Rower 4, Rower 5) and (Rower 2, Rower 3, Rower 6), etc.</li>
<li>Each match can result in 3 outcomes: either the first team wins, or the second team wins, or it results in a draw. The team with the exceptionally strong rower always wins the match. If both the teams don't contain the strong rower, then the result is always a draw. Thus, the outcome of a fixture is deterministic given the team composition.</li>
<li>Note that you are allowed to decide the teams in the second match based on the outcome of the first match, i.e. your match-deciding scheme can be <em>adaptive</em>.</li>
<li>The teams in the matches should be chosen deterministically. They can depend on the outcome of previous matches, but not on any other random variable (e.g., you can't toss a coin to choose the teams!).</li>
</ul>
<ol>
<li>
<p>[10 points] Let the number of players be $n=9$. Show that you can determine the strongest rower using just 2 matches.</p>
</li>
<li>
<p>[5 points] Generalize your strategy to $n$ rowers. Show that you can find one strongest rower in $\lceil \log_3 n \rceil$ matches.</p>
</li>
</ol>
<p>To get your full fees, you have also been asked to <em>prove</em> that your strategy is indeed the optimal. Let $X$ be the random variable representing which player is the stronger rower. $X$ is uniformly distributed in the set of all participants ${1, 2, \ldots, n}$
$$P_X(x=i) = \frac{1}{n}, \forall i = {1, 2, \ldots, n}$$</p>
<p>Let $Y_1, Y_2, \ldots, Y_k$ be the random variable representing the outcome of the $k$ matches you organize.</p>
<ol start="3">
<li>
<p>[5 points] Answer the following sub-problems.</p>
<ul>
<li>Show that $H(X) = \log_2 n$, and $H(Y_i) \leq log_2 3$ for each $i$. When is equality achieved?</li>
<li>Explain why $H(Y_1,Y_2,\ldots,Y_k|X) = 0$.</li>
</ul>
</li>
<li>
<p>[10 points] We want to show that we will at least need $\log_3 n$ matches to determine the strongest rower.</p>
<ul>
<li>Show that we can determine the strongest rower from $k$ matches, if and only if $H(X|Y_1, Y_2, \ldots, Y_k) = 0$.</li>
<li>Using the sub-problem above, show that $k \geq \log_3 n$ matches are required to determine the strongest rower. Thus, proving that the scheme in Q4.1 is indeed optimal!
(<em>HINT: think about using $I(X; Y_1, \ldots, Y_k)$, and its properties</em>).</li>
</ul>
</li>
<li>
<p>[5 points] Let's get back to the $n=9$ rowers scenario. To simplify the logistics, the organizers wanted to pre-decide what the matches are going to be, even before the contest began. That is, the teams for the $i$ th match are pre-decided, and do not depend upon the outcome of the first $i-1$ matches. In other words the match deciding strategy is <em>non-adaptive</em>. Show that even in this <em>non-adaptive</em> case, you can find the strongest rower using the outcome of 2 pre-decided matches!</p>
</li>
</ol>
<h3 id="q4-image-compression-40-points"><a class="header" href="#q4-image-compression-40-points">Q4: Image Compression <em>(40 points)</em></a></h3>
<p>This question is about image compression. We will implement the transform coding ideas and use it to build a simple image compression scheme. Here are the instructions for this question:</p>
<ul>
<li>We have provided the code for this question both in the SCL HW repo as <code>EE274_HW4_ImageCompressor.ipynb</code> as well as a <a href="https://drive.google.com/file/d/1ZxHsqbc1CtwxNMd34UY8nIX63LA3j3cy/view?usp=sharing">Google Colab notebook</a>.</li>
<li>We have tested the code on the Colab notebook, and we recommend working on the Colab notebook for this question. You should make a copy of the Colab notebook and work on the copy.</li>
<li>The question has both coding and written parts. The written parts should be answered in the Colab notebook itself. You will need to run this notebook on Google Colab to answer some of the written parts.</li>
<li>After working on the Colab notebook, you should download the notebook as a <code>.ipynb</code> file and submit it on Gradescope by replacing the existing <code>EE274_HW4_ImageCompressor.ipynb</code> file. This will be graded manually.</li>
</ul>
<h3 id="q5-hw4-feedback-5-points"><a class="header" href="#q5-hw4-feedback-5-points">Q5: HW4 Feedback <em>(5 points)</em></a></h3>
<p>Please answer the following questions, so that we can adjust the difficulty/nature of the problems for the next HWs.</p>
<ol>
<li>How much time did you spent on the HW in total?</li>
<li>Which question(s) did you enjoy the most?</li>
<li>Are the programming components in the HWs helping you understand the concepts better?</li>
<li>Did the HW4 questions complement the lectures?</li>
<li>Any other comments?</li>
</ol>
<h3 id="submission-instructions-3"><a class="header" href="#submission-instructions-3">Submission Instructions</a></h3>
<p>Please submit both the written part and your code on Gradescope in their respective submission links. <strong>We will be using both autograder and manual code evaluation for evaluating the coding parts of the assignments.</strong> You can see the scores of the autograded part of the submissions immediately. For code submission ensure following steps are followed for autograder to work correctly:</p>
<ul>
<li>
<p>As with HW1, you only need to submit the modified files as mentioned in the problem statement.</p>
<ul>
<li>Compress the <code>HW</code> folder into a zip file. One way to obtain this zip file is by running the following zip command in the <code>HWs</code> folder, i.e.
<pre><code class="language-sh">cd HWs
zip -r HW4.zip HW4
</code></pre>
Note: To avoid autograder errors, ensure that the directory structure is maintained and that you have compressed <code>HW4</code> folder containing the relevant files and not <code>HWs</code> folder, or files inside or something else. Ensure the name of the files inside the folder are exactly as provided in the starter code, i.e. <code>vector_quantizer.py</code>, etc. In summary, your zip file should be uncompressed to following directory structure (with same names):
<pre><code>HW4
â””â”€â”€ vector_quantizer.py
â””â”€â”€ EE274_HW4_ImageCompressor.ipynb
</code></pre>
</li>
</ul>
</li>
<li>
<p>Submit the zip file (<code>HW4.zip</code> obtained above) on Gradescope Programming Part Submission Link. Ensure that the autograded parts runs and give you correct scores.</p>
</li>
</ul>
<p><strong>Before submitting the programming part on Gradescope, we strongly recommend ensuring that the code runs correctly locally.</strong></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="projects"><a class="header" href="#projects">Projects</a></h1>
<p>The project is an important part of the course (and constitutes <code>30%</code> of the course grade). This documentation serves as an introduction to the project expectations, deliverables etc. More details will be added as we go along.</p>
<h2 id="project-logistics"><a class="header" href="#project-logistics">Project Logistics</a></h2>
<p>Following are the due dates for different components of the project. All (except final presentation) will be due on Gradescope. More details for each component will be provided in the subsequent sections.</p>
<div class="table-wrapper"><table><thead><tr><th></th><th>Weightage</th><th>Description</th><th>Due Date <br/> (midnight PT)</th></tr></thead><tbody>
<tr><td><a href="projects.html#i-project-proposal">Proposal</a></td><td>5%</td><td>One page proposal</td><td>2025/10/31</td></tr>
<tr><td><a href="projects.html#ii-project-milestone">Milestone Report</a></td><td>5%</td><td>Upto three pages progress report + code (if any)</td><td>2025/11/20</td></tr>
<tr><td><a href="projects.html#iii-final-presentation">Final Presentation</a></td><td>5%</td><td>5 min presentation + QnA</td><td>2025/12/04 (last class)</td></tr>
<tr><td><a href="projects.html#iv-final-report-and-code">Final Report and Code</a></td><td>15%</td><td>Detailed report and code submission</td><td>2025/12/12 (no late days)</td></tr>
</tbody></table>
</div>
<h2 id="project-expectations"><a class="header" href="#project-expectations">Project Expectations</a></h2>
<p>The main goal of the project is to give you all some experience of working on a problem related to data compression. Ideally, the project will involve some component of reading, understanding and also implementation of a compression technique.</p>
<p>The expectation for the project is as follows:</p>
<ul>
<li>Literature review of a compression method/area. You are expected to work with your mentor in understanding the topic you choose, and narrow down on a concrete problem to work on for the project.</li>
<li>Implement the compression method (either using SCL or otherwise). You will work with your project mentor to ensure that your implementation is well documented and tested.</li>
<li>Finally, write a report explaining the theoretical ideas + implementation details behind your method.</li>
</ul>
<p>Given that the quarter is short, the problem on which you will work on will be a bit concrete. We will release a list of topics suggested by instructors <a href="projects.html#project-suggestions">below</a>. You are welcome to come up with your own ideas applying compression to your domain of interest or simply exploring a particular theoretical result or practical technique. Please work with the instructors to ensure the feasibility of the project in the given time frame.</p>
<p>The expected team size for each project is 1-2 students. Groups of 3 are also ok in exceptional circumstances given the scope of the project. Each project will also be assigned a mentor from the teaching staff. The project team and the mentor can decide to meet as per need.</p>
<h2 id="project-deliverables"><a class="header" href="#project-deliverables">Project Deliverables</a></h2>
<h3 id="i-project-proposal"><a class="header" href="#i-project-proposal">I. Project Proposal</a></h3>
<p><strong>Due: 2025/10/31, Friday, 11:59pm</strong></p>
<p>Please use the time till the deadline to explore and decide what project you would like to work on. <strong>Before you submit the proposal, ensure to have at least one 10 minute 1-on-1 chat with the teaching staff (as a team), and finalize on the project idea.</strong> The list of finalized project ideas will be maintained here: <a href="https://docs.google.com/spreadsheets/d/1NTAICA1RWmRBlxM5-ZzjHz_OHxdQutCH2fTmRqCloYE/edit?gid=0#gid=0">Link</a>. Once the project is finalized, we will assign a project mentor (Kedar/Shubham/Tsachy/Jiwon) who can help you with references for the project, help with programming, etc. As we are a small class, ideally we would not like to repeat the exact same project. The teaching team will help you modify your idea appropriately in case someone else is working on the exact same project.</p>
<p>For deliverable, we will follow a similar template as our friends from <a href="http://cs231n.stanford.edu/project.html">CS231N</a>. For the project proposal please submit a 1-page summary on what your project idea is, and an approximate timeline as to what you are planning to achieve by week-x etc. Some questions the proposal should answer:</p>
<ul>
<li>
<p>What is the problem that you will be investigating? Why is it interesting?</p>
</li>
<li>
<p>What reading will you examine to provide context and background?</p>
</li>
<li>
<p>What method or algorithm are you proposing? If there are existing implementations and/or theoretical justifications, will you use them and how? How do you plan to improve or modify such implementations? You don't have to have an exact answer at this point, but you should have a general sense of how you will approach the problem you are working on.</p>
</li>
<li>
<p>How will you evaluate your results? Qualitatively, what kind of results do you expect (e.g. plots or figures)? Quantitatively, what kind of analysis will you use to evaluate and/or compare your results (e.g. what performance metrics or statistical tests)?</p>
</li>
</ul>
<h3 id="ii-project-milestone"><a class="header" href="#ii-project-milestone">II. Project Milestone</a></h3>
<p><strong>Due: 2025/11/20, Thu, midnight PT</strong></p>
<p>For the project milestone, please submit a 2-3 page write-up on the technique/method you chose and link to your in-progress code as a GitHub repo (if any). If possible, you can use GitHub markdown (.md) file as your milestone report, put the .md it on your code repo, and provide us a link to that. That way you will find it easy to later modify it to get the final report. The milestone should roughly include the following sections:</p>
<ul>
<li>
<p>Introduction: what is the problem, why is it interesting?</p>
</li>
<li>
<p>Literature/Code review: summarize any existing papers/implementations that you referred to</p>
</li>
<li>
<p>Methods: what do you plan to implement as part of this project? What end result do you expect to achieve and how will you evaluate it qualitatively and quantitatively?</p>
</li>
<li>
<p>Progress report: what have you already finished (please include code link where relevant)? What is the plan for the remaining weeks?</p>
</li>
</ul>
<h3 id="iii-final-presentation"><a class="header" href="#iii-final-presentation">III. Final presentation</a></h3>
<p><strong>2025/12/04 Thu 1:30 pm - 4:30 pm PT, Packard Atrium</strong></p>
<p><strong>Poster due: 2025/12/04, Thu, midnight PT</strong></p>
<p>The Final presentation will be during the last class (note we will have a longer last class slot!). <strong>The presentation will be held as a poster session.</strong></p>
<p>Attendance is mandatory for the presentation. <!--In case you are not able to make it after 5:50pm on `12/06`, please inform us beforehand, and we will try to schedule your presentation earlier.--> You will work with your mentor on making sure the presentation is interesting and useful to your classmates :).</p>
<p><strong>Guidelines</strong>:</p>
<ul>
<li>You need to submit your poster file by midnight PT on Thursday (12/04)--&gt; using Gradescope. Please submit a PDF of the poster. <strong>No late days allowed for this submission!</strong></li>
</ul>
<!-- - We have divided you into three groups. We have tried to keep projects within a group closer to each other, specifically following group numbers correspond roughly to following areas -- Group 1: Lossless coders we saw in class and their optimizations, Group 2: Advanced lossless coders and Group 3: Compression and applications. You can check your groups in available [spreadsheet - see the presentations tab](https://docs.google.com/spreadsheets/d/1XhgCN3_LAaIHleKRIPZWmBJVAEO5Wgl-X0BCQ9tVcrQ/edit?usp=sharing). -->
<ul>
<li>You will have 5 minutes for your presentation<!-- , and 5 minutes for QnA . Here are some [guidelines](https://www.beautiful.ai/blog/lightning-talks-and-ignite-talks-a-beginners-guide) on how to give a good lightning talk-->.</li>
<li><!--You can assume you are presenting to peers who have taken the class and are aware of the basic terminology. -->Please use these 5 minutes to concisely convey what you have been working on with such an audience in mind, e.g. you don't have to explain entropy coding basics and/or image compression basics. You can use the allotted time to explain the algorithm you are working on, why is it exciting, what did you learn and show the results so-far. Main idea is to ensure you are able to convey the key ideas of your project to your peers.
</li>
<li>You can use any poster template you like. <!--We recommend using [Google Slides](https://www.google.com/slides/about/) or [PowerPoint](https://products.office.com/en-us/powerpoint). You can also use [LaTeX Beamer](https://www.overleaf.com/learn/latex/Beamer) if you are comfortable with it.--></li>
<li>We plan to have some drinks and snacks for everyone during the presentation. We also plan to have pizza during the presentation. :)</li>
<li>We will be making the projects public after the class ends. If you do not wish your project to be made publically available, please let us know.</li>
</ul>
<h3 id="iv-final-report-and-code"><a class="header" href="#iv-final-report-and-code">IV. Final report and code</a></h3>
<p><strong>Due: 2025/12/12 (NO LATE DAYS)</strong></p>
<ul>
<li>
<p>The final submission involves a 4-5 page report on your project idea. It should introduce the problem, describe in detail the technical details, and also briefly talk about results and other implementation details.</p>
</li>
<li>
<p>You also need to submit a link to your code repository. The expectation is to submit a well documented and well tested code which is reproducible, and something someone else (or even you) can build upon.</p>
</li>
</ul>
<p>Your mentor will work with you on setting expectations, and helping you review the report/code before the submission.</p>
<h2 id="project-suggestions"><a class="header" href="#project-suggestions">Project Suggestions</a></h2>
<p>The list of projects from previous iterations of the course can be found <a href="https://docs.google.com/spreadsheets/d/1Ooxk-3b4kDvAxSOX5jaITWVCkMp48eFG522fU8_UdW8/edit?usp=sharing">[here]</a>.</p>
<p>Next, we suggest a (non-exhaustive) list of projects which the teaching staff came up with. You can pick/modify one of these projects, or just use these as inspiration to choose one of your own!</p>
<p><strong>1. Domain-specific compression with <a href="https://github.com/facebook/openzl">OpenZL</a></strong></p>
<p><em>Potential Mentor</em>: Shubham</p>
<p>OpenZL is a recent data compression framework for domain-specific data, in particular tabular data, structured data, logging data and databases/Parquet files. The framework supports parsing of the data into columnar format, applying various transformations, and applying general-purpose compressors like zstd or Huffman. In the project, the student will first understand OpenZL and identify a particular dataset of interest. Then they will work on finding a good configuration for OpenZL to get the best compression for the dataset of interest. The findings will include the optimal configuration, the compression ratio achieved, and various tradeoffs in compression ratio vs. speed. This project is a great opportunity for learning about lossless domain-specific compression, and is closely aligned with various compression practices in industry.</p>
<p><strong>2. <a href="https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform">BWT</a>: Burrows Wheeler Transform for searching over compressed text</strong></p>
<p><em>Potential Mentor</em>: Pulkit</p>
<p>We will briefly touch the world of BWT based compressors in HW3. The project will go beyond HW3 and implement fast and efficient versions of BWT transforms. It will also explore how to use BWT for searching over compressed data.</p>
<p><strong>3. <a href="https://en.wikipedia.org/wiki/Context_tree_weighting">CTW</a>: Context Tree Weighting</strong></p>
<p><em>Potential Mentor</em>: Shubham</p>
<p>We learned that <code>k-th</code> order probability model can be used with arithmetic coding to achieve good compression. However, the larger the <code>k</code>, the slower the adaptability of the model. CTW tries to solve this problem by "mixing" the distributions of different order efficiently. This project will involve understanding the CTW algorithm, understanding why CTW has such good theoretical properties (Rissanen lower bound etc.) and implementing it in SCL.</p>
<p>A few useful links: <a href="https://docs.google.com/presentation/d/1LUbo-6mLpYTwcELOLlRR4ohku9j2kCiQj_2sYPh0uWA/edit#slide=id.g5eaf3d9f0e_0_75">ppt</a>, <a href="https://github.com/fumin/ctw">ctw-cpp-implementation</a>, <a href="https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/CTW.pdf">original paper</a>, <a href="https://web.stanford.edu/class/ee477/lectures2011/lecture4.pdf">lecture notes</a></p>
<p><strong>4. <a href="https://bjlkeng.github.io/posts/lossless-compression-with-latent-variable-models-using-bits-back-coding/">Bits-back coding</a> and compression of data with permutation invariance</strong></p>
<p><em>Potential Mentor</em>: Pulkit</p>
<p>We learnt about <code>rANS</code> in class and will learn about <code>bits-back coding</code> in HW2. Recent <a href="https://arxiv.org/abs/2107.09202">paper</a> and <a href="https://github.com/facebookresearch/multiset-compression">implementation</a> show that it is possible to have a very general method which allows saving bits by not saving the order in the data (utilizing permutation invariance). This uses bits-back coding underneath. This project will involve understanding and re-implementing this method in SCL.</p>
<p><strong>5. Tabular data compression using <a href="https://en.wikipedia.org/wiki/Chow%E2%80%93Liu_tree">Chow-Liu trees</a></strong></p>
<p><em>Potential Mentor</em>: Shubham</p>
<p>Tabular data is quite commonly found in data logs, log files, etc. and is typically compressed as-is using gzip. A recent <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5641593/">paper</a> proposes a way to exploit the dependencies between columns to improve compression. The goal of the project is to understand the paper, and implement an equivalent variant using SCL. This project will involve implementing and testing the performance of various existing compressors on variety of real life tabular datasets.</p>
<p><strong>6. Adaptive Huffman codes, and other variants</strong></p>
<p><em>Potential Mentor</em>: Pulkit</p>
<p>Huffman coding algorithm is a very useful and versatile algorithm. This project explores some variants of the standard algorithm to improve its performance. We will explore two of such variations in this project. In the first part of the project, we will implement Depth-limited huffman coding as a part of SCL (<a href="https://www.ics.uci.edu/~dan/pubs/LenLimHuff.pdf">original paper</a>, <a href="http://cbloomrants.blogspot.com/2010/07/07-03-10-length-limitted-huffman-codes.html">Charles Bloom's blog</a>).</p>
<p>The second part of the project would entail implementing <a href="https://en.wikipedia.org/wiki/Adaptive_Huffman_coding">Dynamic Huffman tree</a> as part of SCL. Dynamic Huffman coding deals with the question of how can we update huffman tree as the source distribution changes in practice.</p>
<p><strong>7. PPM, PPMD</strong></p>
<p><em>Potential Mentor</em>: Shubham</p>
<p><a href="https://en.wikipedia.org/wiki/Prediction_by_partial_matching">PPM: prediction by partial matching</a> is a very interesting algorithm for adaptive coding based on context matching. This project involves understanding and implementing this idea.</p>
<p><strong>8. LZ77 optimized implementation</strong></p>
<p><em>Potential Mentor</em>: Shubham</p>
<p>The most widely used general purpose compressors (gzip, zstd) are based on LZ77. The SCL implementation of LZ77 uses a simplistic greedy parsing strategy and uses static codes for encoding  the match lengths and offsets. In addition, the SCL LZ77 doesnâ€™t have a sliding window or a chained-hash implementation. In this project, we will work towards creating a better LZ77 implementation in SCL based on existing works such as gzip, zstd, LZMA as well as other resources such as this <a href="https://glinscott.github.io/lz/index.html">blog post</a> by Glinn Scott. After the implementation is finished, we will perform experiments to better analyze how the performance varies with the parameters.</p>
<p><strong>9. LZ78/LZW in SCL</strong></p>
<p><em>Potential Mentor</em>: Shubham</p>
<p>In the class we discussed the LZ77 compression algorithm and its practical implementation. There are other universal compressors which are quite interesting and have as good/better properties than LZ77. This project involves understanding and implementing LZ78 and/or Lempel-Ziv-Welsh (LZW) algorithm in SCL.</p>
<p><strong>10. Alias method for speeding up rANS</strong></p>
<p><em>Potential Mentor: Pulkit</em></p>
<p>rANS decoding is bottle-necked by the binary search of the <code>slot</code> (as we learnt in the class). The alias method can be used for speeding this part up! This project will involve understanding the alias method, and implementing it in SCL for speeding up rANS decoding. For reference, see blog by Fabian Giesen <a href="https://fgiesen.wordpress.com/2014/02/18/rans-with-static-probability-distributions/">here</a>.</p>
<p><strong>11. Recompression</strong></p>
<p><em>Potential Mentor: Shubham</em></p>
<p>In many scenarios we use suboptimal compression formats like gzip or jpeg due to legacy support and compatibility reasons. Significant storage savings can be obtained if the data were to be stored in a modern optimized format. However, there is often a requirement to be able to exactly recover the original compressed file, which might not be possible with a simple transcoding approach. In this project we will explore the idea of recompression, which is a technique to compress a file in a new format while preserving the original compressed file. The project will involve implementing a known or novel recompression technique and understanding the invertibility of the recompression. Some existing recompression works include <a href="https://github.com/google/brunsli">brunsli</a> and <a href="https://github.com/dropbox/lepton">lepton</a> for JPEG, and <a href="https://github.com/schnaader/precomp-cpp">precomp-cpp</a> for gzip.</p>
<p><strong>12. Hardware acceleration for compression</strong></p>
<p><em>Potential Mentor</em>: Shubham</p>
<p>Many of the widely used compression algorithms rely heavily on hardware acceleration, e.g., utilizing SIMD and AVX2 instructions on CPU. In addition, there is much interest in exploring the use of GPUs to accelerate compression. This project will first identify a particular existing or novel usage of hardware acceleration in a compression setting, followed by implementation of this acceleration. We will demonstrate the speedup obtained vs. an efficient implementation without acceleration and explain the core ideas behind the speedup. Some example optimized code bases include <a href="https://github.com/facebook/zstd">Zstd</a>, <a href="https://github.com/Cyan4973/FiniteStateEntropy">FSE</a>, <a href="https://github.com/IlyaGrebnov/libbsc">BSC</a> and <a href="https://github.com/facebookresearch/dietgpu">DietGPU</a>. The recent release of Zstd also enables hardware matchfinding, e.g., see <a href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-QuickAssist-Technology-Zstandard-Plugin-an-External/post/1509818">this Intel announcement</a>.</p>
<p><strong>13. LLM based compression</strong></p>
<p><em>Potential Mentor</em>: Shubham</p>
<p>With the new wave of LLMs, there have been multiple works looking at how they can be utilized for compression, including <a href="https://bellard.org/ts_server/ts_zip.html">ts_zip</a>, <a href="https://github.com/vcskaushik/LLMzip">LLMZip</a> and <a href="https://arxiv.org/abs/2309.10668">this paper</a>. The project can involve identifying a data domain, developing an end-to-end compressor, and evluating with different size LLM models.</p>
<p><strong>14. Understanding and implementing tANS as used in Zstd</strong></p>
<p><em>Potential Mentor</em>: Pulkit/Shubham</p>
<p>Zstd heavily relies on the <a href="https://github.com/Cyan4973/FiniteStateEntropy">FSE library</a> by the same author (Yann Collet) for achieving lightning fast and efficient entropy coding. This project will focus on understand and implementing aspects of FSE in SCL, based on the library itself and <a href="https://fastcompression.blogspot.com/">Yann's blog</a>.</p>
<p><strong>15. Ultra low bit rate image compression via text</strong></p>
<p><em>Potential Mentor</em>: Tsachy</p>
<p>We'll explore the use of text-to-image generative models for image compression at extremely low bit rates. The project will involve understanding and building on the framework in the paper <a href="https://arxiv.org/pdf/2307.01944.pdf">Text + Sketch: Image Compression at Ultra Low Rates</a>. Also another relevant reading is <a href="https://arxiv.org/abs/2305.01857">Toward Textual Transform Coding</a>.</p>
<p><strong>16. Ultra low bit rate video compression via text</strong></p>
<p><em>Potential Mentor</em>:  Pulkit/Tsachy</p>
<p>We'll explore the use of text-to-video generative models for video compression at extremely low bit rates. Relevant readings are:
(i) <a href="https://arxiv.org/abs/2305.01857">Toward Textual Transform Coding</a>
(ii) <a href="https://arxiv.org/abs/2106.14014">Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text</a></p>
<h3 id="open-ended-project-ideas"><a class="header" href="#open-ended-project-ideas">Open Ended project ideas</a></h3>
<p>The ideas below are not fleshed out yet, but project groups can talk to the instructors to come up with a concrete idea:</p>
<ol>
<li>
<p>Rate-Distortion Optimal Compression for Video Embeddings</p>
<p><em>Potential Mentor</em>: Pumiao Yan (Alumna of Tsachy's group)</p>
<p>Video understanding systems store high-dimensional embeddings (512-768 dims) for every frame, creating massive storage and retrieval costs. This project applies rate-distortion theory to find the optimal compression of video embeddings that minimizes storage (rate) while preserving semantic search quality (minimizing distortion). The key twist: unlike traditional rate-distortion where distortion measures pixel-level reconstruction error, we define distortion as the loss in retrieval accuracyâ€”how well can compressed embeddings still match relevant queries? Youâ€™ll implement a learned compression scheme using variational autoencoders, where the rate R = I(X;Z) (mutual information between original and compressed embeddings) and distortion D = task performance degradation. By varying the rate-distortion tradeoff parameter Î², youâ€™ll trace out the operational R-D curve and compare it to theoretical bounds derived from Shannonâ€™s rate-distortion theorem. This directly connects to EE274's coverage of lossy source coding, achieving 8x compression (512 â†’ 64 dims) while maintaining video search quality.</p>
</li>
<li>
<p>Multi-Resolution Semantic Pyramids for Progressive Video Retrieval</p>
<p><em>Potential Mentor</em>: Pumiao Yan (Alumna of Tsachy's group)</p>
<p>Searching large video databases is computationally prohibitiveâ€”matching a query against millions of high-dimensional frame embeddings requires examining every frame. This project applies rate-distortion theory to create a hierarchical representation where videos are encoded at multiple temporal resolutions: coarse chapter-level (64-dim, one per 5 minutes), medium scene-level (128-dim, one per 30 seconds), and fine frame-level (512-dim, one per frame). Users query the coarsest level first, then progressively refine by drilling down to finer resolutions only for relevant segmentsâ€”pruning the search space by 100x while maintaining retrieval accuracy. The core theoretical question: does this layered encoding approach the rate-distortion bound? Youâ€™ll analyze whether the total rate $R_{total}=R_{chapter}+R_{scene}+R_{frame}$ approaches the single-stage optimal rate $R(D)$ for achieving the finest-level distortion, demonstrating that hierarchical compression is rate-efficient. By implementing conditional encoding where each level compresses information given the previous coarser level, youâ€™ll show empirically that $R_{total} \leq R(D) + \epsilon$  where $\epsilon$ represents the rate penalty of multi-stage encoding, connecting directly to EE274's coverage of lossy source coding and rate-distortion theory with side information.</p>
</li>
<li>
<p>Compressing captions of video frames for boosting LVMs</p>
<p><em>Potential Mentor</em>: Tsachy</p>
<p>Consider the body of text resulting from concatenating the captions of frames, sampled at some frequency from a video, where the captions are obtained via a visual language model. The project will explore both lossless and lossy compression of such text. In the lossy case, part of the exploration will be dedicated to finding the right distortion criteria under both human and machine consumption. The compressors will be put to use for boosting the performance of Large Vision Models (LVMs).</p>
</li>
<li>
<p>Compressibility properties of LLM tokenizers</p>
<p><em>Potential Mentor</em>: Shubham</p>
<p>Modern LLMs tokenize the input into integer tokens before passing them into the neural network. While tokenization is not exactly focused on compression, it does offer some compressibility properties and LLM providers often tout the reduced token consumption for various languages. In this project, students will explore the interplay between tokenization and compression, with experimentation across different tokenizers, languages, types of text (classical vs. modern, natural language vs. logs, etc.). They will study the properties of tokenizers with regards to compression, and how it compares to general-purpose compressors like zstd. Some recent works in similar directions include https://arxiv.org/pdf/2402.01035v1 and https://arxiv.org/pdf/2404.03626v1.</p>
</li>
<li>
<p>Video compression for AI</p>
<p><em>Potential Mentor</em>: Tsachy Weissman</p>
<p>We will examine ways to lossily compress a video before it is fed to a large video language model such that the inference time is reduced substantially, with little degradation to the accuracy.</p>
</li>
<li>
<p>Understanding and implementing different image/video perceptual metrics in SCL (eg: SSIM, MS-SSIM, VMAF, etc.).</p>
<p><em>Potential Mentor</em>: Pulkit</p>
</li>
<li>
<p>Implement a lossless image compressor beating PNG in SCL. (NOTE: there are compressors such as JPEG-XL, AVIF, which already do this, but you can go beyond those!)</p>
<p><em>Potential Mentor</em>: Pulkit</p>
</li>
<li>
<p>Attempt the CLIC image compression challenge! (a bit more challenging): <a href="http://compression.cc/tasks/">http://compression.cc/tasks/</a></p>
<p><em>Potential Mentor</em>: Pulkit</p>
</li>
<li>
<p><em>Joint image compression:</em> Often very similar images are stored (e.g., same scene captured twice from slightly different angles) but are compressed independently of each other. In this project we explore the possibility of better compression by jointly compressing the images (one idea could be to stitch them into a video and use video compression algorithms). Find/generate a suitable dataset and implement an algorithm (lossless or lossy). Demonstrate the savings achieved by joint vs. individual compression.</p>
<p><em>Potential Mentor</em>: Tsachy/Pulkit</p>
</li>
<li>
<p>Compression can be used to estimate entropy of a random variable. In this project we will explore the idea of using compression to estimate entropy of a random variable. We will implement a simple compression algorithm and use it to estimate entropy of a random variable. We will then compare the performance of this algorithm with other entropy estimation techniques and/or use it for a practical application such as anomaly detection and/or prediction in high-dimensional datasets via compression in a dataset you care about.</p>
<p><em>Potential Mentor</em>: Tsachy/Pulkit</p>
</li>
<li>
<p>NN-based lossy compressors vs. more traditional lossy compressors vs. the fundamental limits. Comparison between recent NN-based lossy compressors and more traditional ones based on vector quantization in terms of rate-distortion-complexity tradeoffs, experimenting on simulated data for which we know and can compare to the fundamental limits.</p>
<p><em>Potential Mentor</em>: Tsachy/Pulkit</p>
</li>
<li>
<p>Lossy compression of text under a meaningful distortion measure. Explore achievable rate distortion tradeoffs and/or fundamental limits in this space.</p>
<p><em>Potential Mentor</em>: Tsachy/Shubham</p>
</li>
<li>
<p>Neural audio compression: explore the recent work by Meta using neural nets for audio compression.</p>
<p><em>Potential Mentor</em>: Tsachy</p>
</li>
<li>
<p><em>Genome in a tweet:</em> The question to answer is can we compress a child's genome in a single tweet (~280 bytes) assuming side-information of the genomes of their parents is available in a tweet?. The project will try to answer this question theoretically, and practically by implementing a compressor. The project can also explore practically implementing ideas from distributed compression.</p>
<p><em>Potential Mentor</em>: Shubham</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stanford-compression-library-scl-a-brief-tutorial"><a class="header" href="#stanford-compression-library-scl-a-brief-tutorial">Stanford Compression Library (SCL): A brief tutorial</a></h1>
<p>This document provides an overview of motivation and SCL structure. We end this tutorial with an exercise on SCL and show how to use SCL in google colab notebook. For students, it is highly recommended to attempt the exercise. Please reach out to the teaching staff if you have any questions.</p>
<h2 id="introduction-2"><a class="header" href="#introduction-2">Introduction</a></h2>
<p>The Stanford Compression Library is being implemented to aid research in the area of data compression. As students working in the field of data compression, we noticed that although these pieces of software are used everyday (gzip, jpeg etc); unfortunately there is no good pedagogical/research implementation which can be tweaked quickly.</p>
<p>The goals of SCL are:</p>
<ul>
<li>To provide pedagogical implementations of common data compression algorithms</li>
<li>To provide convenient framework to quickly modify existing compression algorithm and to aid research in the area</li>
<li>More selfishly, to understand these algorithms better :)</li>
</ul>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<p>Before we dive deep into the library, please make sure you have followed the installation instructions in the <a href="https://github.com/kedartatwawadi/stanford_compression_library">README</a> and that all the tests are passing. Please file an issue if you face problems with the setup.</p>
<h2 id="using-the-library"><a class="header" href="#using-the-library">Using the library</a></h2>
<p>Let's look at the library structure next to understand how to use the library. Please look at next section on <a href="scl_tutorial/./basics.html">SCL basics</a> to understand the basic building blocks of SCL.</p>
<h2 id="exercise"><a class="header" href="#exercise">Exercise</a></h2>
<p>After understanding the abstractions and various classes introduced, lets take a look at the <a href="scl_tutorial/./exercise.html">following exercise</a>. This was a problem from year 2022 homework, and so to make it most useful it might be best to treat it as such!</p>
<div style="break-before: page; page-break-before: always;"></div><p>Let us go over the main library structure.</p>
<h2 id="library-structure----useful-building-blocks"><a class="header" href="#library-structure----useful-building-blocks">Library Structure -- Useful Building Blocks</a></h2>
<p>The main library folder is <code>scl</code>. The <code>scl</code> directory structure is as follows:</p>
<pre><code>â”œâ”€â”€ compressors
â”‚Â Â  â”œâ”€â”€ arithmetic_coding.py
â”‚Â Â  â”œâ”€â”€ elias_delta_uint_coder.py
â”‚Â Â  â”œâ”€â”€ fano_coder.py
â”‚Â Â  â”œâ”€â”€ fixed_bitwidth_compressor.py
â”‚Â Â  â”œâ”€â”€ ... (more compressors)
â”œâ”€â”€ core
â”‚Â Â  â”œâ”€â”€ data_block.py
â”‚Â Â  â”œâ”€â”€ data_encoder_decoder.py
â”‚Â Â  â”œâ”€â”€ data_stream.py
â”‚Â Â  â”œâ”€â”€ encoded_stream.py
â”‚Â Â  â””â”€â”€ prob_dist.py
â”œâ”€â”€ external_compressors
â”‚Â Â  â”œâ”€â”€ pickle_external.py
â”‚Â Â  â”œâ”€â”€ zlib_external.py
â”‚Â Â  â””â”€â”€ zstd_external.py
â””â”€â”€ utils
    â”œâ”€â”€ bitarray_utils.py
    â”œâ”€â”€ misc_utils.py
    â”œâ”€â”€ test_utils.py
    â””â”€â”€ tree_utils.py
</code></pre>
<p>The directories are self-explanatory, but here are some more details:</p>
<ul>
<li><a href="https://github.com/kedartatwawadi/stanford_compression_library/tree/main/scl/core">/core</a>: This contains the core part of the library which is common to almost all compressors. For eg: classes to represent input data, encoded bitarrays, Encoders, Decoders</li>
<li><a href="https://github.com/kedartatwawadi/stanford_compression_library/tree/main/scl/compressors">/compressors</a>: This includes compressors implemented natively in SCL.</li>
<li><a href="https://github.com/kedartatwawadi/stanford_compression_library/tree/main/scl/external_compressors">/external_compressors</a>: SCL-like interface to external compressors (such as <code>zlib</code> etc.)</li>
<li><a href="https://github.com/kedartatwawadi/stanford_compression_library/tree/main/scl/utils">/utils</a>: general utility functions for debugging, bitarray manipulation, testing etc.</li>
</ul>
<h2 id="1-the-core-library"><a class="header" href="#1-the-core-library">1. The <code>core</code> library</a></h2>
<p>We noticed that most of the compressors share a lot of commonalities. For example, a lot of them encode data in blocks and write to bits. The core library implements the basic frameworks and classes common to all compressors. We elaborate some of them below.</p>
<h3 id="11-datablock"><a class="header" href="#11-datablock">1.1 <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/core/data_block.py">DataBlock</a></a></h3>
<p>The encoders and decoders in SCL operate on blocks of data. Each input block is of type <code>DataBlock</code>. The <code>DataBlock</code> can be thought of as a thin wrapper around a list of input symbols. Let's take a look at the <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/d221a3208d46e6faab21d05e9b21e87dc088d9ae/scl/core/data_block.py#L5C9-L5C9">class definition</a>:</p>
<pre><code class="language-python">class DataBlock:
    """
    wrapper around a list of symbols.

    The class is a wrapper around a list of symbols (self.data_list). The data_block is typically used
    to represent input to the data encoders (or output from data decoders)

    Some utility functions (useful generally for compression) implemented are:
    - size
    - alphabet
    - empirical_distribution
    - entropy
    """

    def __init__(self, data_list: List):
        self.data_list = data_list

    @property
    def size(self):
        return len(self.data_list)
        
    ...
</code></pre>
<p>As you can see, the main data is stored in the <code>self.data_list</code> attribute, the other functions are helper functions which are useful in multiple places in the code.</p>
<p>One useful think in the SCL is that unit tests are present in the same file at the bottom, and are very useful as usage examples. For example, lets take a look at the <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/d221a3208d46e6faab21d05e9b21e87dc088d9ae/scl/core/data_block.py#L109">tests for DataBlock</a>:</p>
<pre><code class="language-python">def test_data_block_basic_ops():
    """checks basic operations for a DataBlock"""
    data_list = [0, 1, 0, 0, 1, 1]

    # create data block object
    data_block = DataBlock(data_list)

    # check size
    assert data_block.size == 6

    # check counts
    counts_dict = data_block.get_counts(order=0)
    assert counts_dict[0] == 3

    # check empirical dist
    prob_dist = data_block.get_empirical_distribution(order=0)
    assert prob_dist.prob_dict[0] == 0.5

    # check entropy
    entropy = data_block.get_entropy(order=0)
    assert entropy == 1.0
</code></pre>
<p>The tests above are useful for also checking out the various pre-implemented methods for the classes e.g. you can see how once you define the <code>data_block</code>, you can use <code>data_block.get_entropy(order=0)</code> to get 0th order entropy of the data, or <code>data_block.get_empirical_distribution(order=0)</code> to get the empirical distribution of the data.</p>
<h3 id="12-dataencoder-and-datadecoder"><a class="header" href="#12-dataencoder-and-datadecoder">1.2 <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/core/data_encoder_decoder.py">DataEncoder and DataDecoder</a></a></h3>
<p>Another abstraction in <code>core</code> library is that of <code>DataEncoder</code> and <code>DataDecoder</code>. Any compressor consists of an <em>Encoder</em> and a <em>Decoder</em>. The encoder maps input symbols (<code>DataBlock</code>) to bits (<code>BitArray</code>) while the decoder does the reverse mapping (bits to output symbols). In case of SCL, all encoders are subclasses of <code>DataEncoder</code> and all decoders are subclasses of <code>DataDecoder</code>. Let's take a look at the class definitions to understand better:</p>
<pre><code class="language-python">class DataEncoder(abc.ABC):
    ...
    def encode_block(self, data_block: DataBlock):
        ...
        # return encoded_bitarray
        raise NotImplementedError
    ...


class DataDecoder(abc.ABC):
    ...
    def decode_block(self, bitarray: BitArray):
        ...
        # return decoded_block, num_bits_consumed
        raise NotImplementedError
   ...
</code></pre>
<p>For now let's focus on the <code>encode_block</code> and <code>decode_block</code> member functions, which are inverses of each other. The <code>encode_block</code> function of <code>DataEncoder</code> maps input <code>DataBlock</code> to a <code>BitArray</code>, while the <code>decode_block</code> function of <code>DataDecoder</code> does the reverse. Note that <code>decode_block</code> also returns the <code>num_bits_consumed</code>. This is useful as the input <code>BitArray</code> might contain bits written by other encoders, and so the <code>decode_block</code> might not consume all the bits. We will see how this is useful in combining multiple encoders.</p>
<p>The <code>encode_block</code> and <code>decode_block</code> functions are the core logic of any compressor, and is usually the only part subclassing encoders/decoders need to implement. Here is an example of the  <code>encode_block</code> of <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/compressors/arithmetic_coding.py#L80">arithmetic code</a>.</p>
<p>The <code>DataEncoder</code> and <code>DataDecoder</code> also contains other functions which are useful to convert our encoder/decoders until practical coders which can handle multiple blocks of data etc. Do take a look at the <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/core/data_encoder_decoder.py"><code>encode</code>, <code>decode</code>, <code>encode_file</code></a> functions if you are interested!</p>
<h3 id="13-probabilitydist"><a class="header" href="#13-probabilitydist">1.3 <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/core/prob_dist.py">ProbabilityDist</a></a></h3>
<p>The final abstraction in <code>core</code> library that we will discuss is that of <code>ProbabilityDist</code>. The <code>ProbabilityDist</code> class is used to represent probability distributions. The class is a thin wrapper around a dictionary which maps symbols to their probabilities (<code>prob_dict</code>). It provides some useful member properties to extract relevant information, such as the <code>alphabet</code> (list of symbols) and <code>prob_list</code> (list of probabilities). Take a look at the class definition and some of the function methods below:</p>
<pre><code class="language-python">class ProbabilityDist:
    ...
    def __init__(self, prob_dict=None):
        self._validate_prob_dist(prob_dict)

        # NOTE: We use the fact that since python 3.6, dictionaries in python are
        # also OrderedDicts. https://realpython.com/python-ordereddict/
        self.prob_dict = prob_dict
    ...
    def __repr__(self):
        return f"ProbabilityDist({self.prob_dict.__repr__()}"
    ...
    @property
    def alphabet(self):
        return list(self.prob_dict)

    @property
    def prob_list(self):
        return [self.prob_dict[s] for s in self.alphabet]

    @classmethod
    def get_sorted_prob_dist(cls, prob_dict, descending=False):
        """
        Returns ProbabilityDist class object with sorted probabilities.
        By default, returns Probabilities in increasing order (descending=False), i.e.,
        p1 &lt;= p2 &lt;= .... &lt;= pn (python-default)
        """
        return cls(dict(sorted(prob_dict.items(), key=lambda x: x[1], reverse=descending)))
    ...
    def cumulative_prob_dict(self):
    ...
    def entropy(self):
    ...
    def probability(self, symbol):
    ...
    def neg_log_probability(self, symbol):
    ... 
</code></pre>
<p>It also provides some useful functions to manipulate the probability distributions. We will see in the construction of various codes such as Huffman code that sorting the probabilities is a useful operation and so the <code>ProbabilityDist</code> class provides <code>get_sorted_prob_dist</code> function to get the <code>prob_dict</code> in sorted order. Other such operations include computing cumulative probabilities (<code>cumulative_prob_dict</code>), computing entropy (<code>entropy</code>), probability of a particular symbol (<code>probability(symbol)</code>), negative log probability of a particular symbol (<code>neg_log_probability(symbol)</code>), etc. Please have a look at the class definition for more details.</p>
<h2 id="2-the-compressors-library"><a class="header" href="#2-the-compressors-library">2. The <code>compressors</code> library</a></h2>
<p>We natively implemented some of the compressors in the <code>compressors</code> library. The <code>compressors</code> library is contains these compressors. We will give a detailed example below but please refer to the library for further exploration.</p>
<p>For instance, let's look at the <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/compressors/shannon_coder.py">Shannon Coder</a> we have seen in class. It subclasses from <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/compressors/prefix_free_compressors.py">Prefix-free Coder</a> which in-turn subclasses from <code>DataEncoder</code> and <code>DataDecoder</code> from the <code>core</code> library. <code>Prefix-free Coder</code> has implementations of <code>PrefixFreeEncoder</code>, <code>PrefixFreeDecoder</code> and <code>PrefixFreeTree</code> which are utility abstract classes
useful for implementing any prefix free code. <code>Shannon Coder</code> is one specific example of a prefix free code.</p>
<p>Let's first look at the encoding part of the Shannon Coder. You will notice that <code>encode_block</code> function is already implemented in the <code>PrefixFreeCoder</code> class: <code>encode_block</code> function just loops over each symbol in the input and concatenate the bitstreams. Therefore, we only need to implement the <code>encode_symbol</code> function in the inherited <code>ShannonEncoder</code> part. Let's take a look at the <code>encode_symbol</code> function in the <code>ShannonEncoder</code> class:</p>
<pre><code class="language-python">class ShannonEncoder(PrefixFreeEncoder):
    """
    PrefixFreeEncoder already has a encode_block function to encode the symbols once we define a encode_symbol function
    for the particular compressor.
    """

    def __init__(self, prob_dist: ProbabilityDist):
        self.prob_dist = prob_dist
        self.encoding_table = ShannonEncoder.generate_shannon_codebook(self.prob_dist)

    @classmethod
    def generate_shannon_codebook(cls, prob_dist):
        # sort the probability distribution in decreasing probability and get cumulative probability which will be
        # used for encoding
        sorted_prob_dist = ProbabilityDist.get_sorted_prob_dist(
            prob_dist.prob_dict, descending=True
        )
        cum_prob_dict = sorted_prob_dist.cumulative_prob_dict

        codebook = {}
        for s in sorted_prob_dist.prob_dict:
            # get the encode length for the symbol s
            encode_len = math.ceil(sorted_prob_dist.neg_log_probability(s))

            # get the code as a truncated floating point representation
            _, code = float_to_bitarrays(cum_prob_dict[s], encode_len)
            codebook[s] = code
        return codebook

    def encode_symbol(self, s):
        return self.encoding_table[s]
</code></pre>
<p>As evident, it encodes individual symbol by creating a codebook based on sorted probabilities. Look at the <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/d221a3208d46e6faab21d05e9b21e87dc088d9ae/scl/compressors/shannon_coder.py#L1">readme for this script</a> or refer to <a href="https://stanforddatacompressionclass.github.io/notes/lossless_iid/prefix_free_codes.html#designing-prefix-free-codes">class notes</a> for details.</p>
<p>Next, let's look at the decoding part of the Shannon Coder. You will again notice that <code>decode_block</code> function is already implemented in the <code>PrefixFreeCoder</code> class: <code>decode_block</code> function just loops over the bitstream and utilizes efficient decoding of prefix-free codes to output individual symbol and the bits consumed during decoding (as explained above in <code>DataDecoder</code> class). Therefore, we only need to implement the <code>decode_symbol</code> function in the inherited <code>ShannonDecoder</code> part. Let's take a look at the <code>decode_symbol</code> function in the <code>ShannonDecoder</code> class:</p>
<pre><code class="language-python">class ShannonDecoder(PrefixFreeDecoder):
    """
    PrefixFreeDecoder already has a decode_block function to decode the symbols once we define a decode_symbol function
    for the particular compressor.
    PrefixFreeTree provides decode_symbol given a PrefixFreeTree
    """

    def __init__(self, prob_dist: ProbabilityDist):
        encoding_table = ShannonEncoder.generate_shannon_codebook(prob_dist)
        self.tree = PrefixFreeTree.build_prefix_free_tree_from_code(encoding_table)

    def decode_symbol(self, encoded_bitarray: BitArray) -&gt; Tuple[Any, BitArray]:
        decoded_symbol, num_bits_consumed = self.tree.decode_symbol(encoded_bitarray)
        return decoded_symbol, num_bits_consumed
</code></pre>
<p>Here you see something interesting: we were able to use <code>PrefixFreeTree</code> class to implement the <code>decode_symbol</code> function. This is because the Shannon code is just a special case of a prefix-free code, and so we can use the <code>PrefixFreeTree</code> to first get the encoding tree used to encode the message given probability distribution and then use it to decode the message. Obviously in practice, we also need to communicate or be able to replicate the probability distribution used to encode the message at the decoder, but we will ignore that for now.</p>
<p>Again, please look at the corresponding files to get better understanding of the code.</p>
<h2 id="3-the-external_compressors-library"><a class="header" href="#3-the-external_compressors-library">3. The <code>external_compressors</code> library</a></h2>
<p>This library includes implementation of external compressors so that they can be natively used within SCL, such as <a href="https://github.com/facebook/zstd">zstd</a> and <a href="https://www.zlib.net">zlib</a>. These might be beneficial to use in some cases, for example, if you want to compare the performance of a compressor with a state-of-the-art compressor, or use them in conjunction with a lossy compressor technique you implement in SCL. Please look at the folder with test cases for examples on how to use these.</p>
<h2 id="4-the-utils-library"><a class="header" href="#4-the-utils-library">4. The <code>utils</code> library</a></h2>
<p>Finally, we cover <code>utils</code> library which has some useful functions for debugging, bitarray manipulation, testing etc. The naming should be evident, and you should visit them while working on problem sets or contributing to the SCL to get help from these helper functions. Feel free to contribute back if you think you have some useful functions to add to the library.</p>
<p>Some notable ones are:</p>
<h3 id="41-bitarray_utilsuint_to_bitarray"><a class="header" href="#41-bitarray_utilsuint_to_bitarray">4.1 <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/d221a3208d46e6faab21d05e9b21e87dc088d9ae/scl/utils/bitarray_utils.py#L27">bitarray_utils.uint_to_bitarray</a></a></h3>
<p>This function converts an unsigned integer to a bitarray. For example, <code>uint_to_bitarray(5, 4)</code> will return <code>BitArray('0b0101')</code>. It is very useful in encoding lengths to help decode a bitstream.</p>
<h3 id="42-bitarray_utilsbitarray_to_uint"><a class="header" href="#42-bitarray_utilsbitarray_to_uint">4.2 <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/d221a3208d46e6faab21d05e9b21e87dc088d9ae/scl/utils/bitarray_utils.py#L36">bitarray_utils.bitarray_to_uint</a></a></h3>
<p>This function implements reverse of previous method, i.e. it converts a bitarray to an unsigned integer. For example, <code>bitarray_to_uint(BitArray('0b0101'))</code> will return <code>5</code>. As expected, it will be very useful in decoding lengths from a bitstream.</p>
<h3 id="43-tree_utilsbinarynode"><a class="header" href="#43-tree_utilsbinarynode">4.3 <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/d221a3208d46e6faab21d05e9b21e87dc088d9ae/scl/utils/tree_utils.py#L6C7-L6C17">tree_utils.BinaryNode</a></a></h3>
<p>This class implements a binary tree node. It is useful in implementing prefix-free trees, or trees in general. It also has some useful debugging functions such as <code>print_tree</code> which can be used to print the tree in a nice format.</p>
<h3 id="44-test_utilsget_random_data_block"><a class="header" href="#44-test_utilsget_random_data_block">4.4 <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/d221a3208d46e6faab21d05e9b21e87dc088d9ae/scl/utils/test_utils.py#L17C5-L17C26">test_utils.get_random_data_block</a></a></h3>
<p>This function can be used to generate random data blocks with given probability distribution. It is useful for generating random test data during testing of compressors.</p>
<h3 id="45-test_utilstry_lossless_compression"><a class="header" href="#45-test_utilstry_lossless_compression">4.5 <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/d221a3208d46e6faab21d05e9b21e87dc088d9ae/scl/utils/test_utils.py#L73">test_utils.try_lossless_compression</a></a></h3>
<p>This function can be used to test if a compressor is lossless. It takes a compressor (encoder-decoder pair), and a data block as input, and checks if the compressor is able to losslessly compress the data block. It is useful for testing if a compressor is lossless.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="exercise-1"><a class="header" href="#exercise-1">Exercise</a></h1>
<p>To make sure we understand the abstractions and various classes introduced, lets take a look at the following exercise. This was a problem from year 2022 homework, and so to make it most useful it might be best to treat it as such!</p>
<p><em>The solution to the exercise and how to use SCL in a Google colab notebook is also available <a href="https://colab.research.google.com/drive/1zf5Kk6rhf0mhfYjIQx8inx1TpwVpYLTg?usp=sharing">here</a>.</em></p>
<p>We will use a simple pedagogical compressor <code>compressors/universal_uint_coder.py</code> as an example, and then use it to implement a more complex compressor <code>HWs/tutorial/universal_int_coder.py</code>. To access the starter file, ensure you are in the <code>EE274_Fall25/HWs</code> branch of <code>SCL</code>. Checkout to the branch for getting access to tutorial (and HW problem templates).</p>
<pre><code class="language-sh"> git checkout EE274_Fall25/HWs
</code></pre>
<p>Let's start with understanding the <code>compressors/universal_uint_coder.py</code> compressor (<a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/main/scl/compressors/universal_uint_coder.py">link</a>). The compressor is a simple implementation of a universal compressor for unsigned integers. First let us note the structure of the compressor. The compressor has two main components:</p>
<ul>
<li>
<p><em>UniversalUintEncoder</em>: This class is used to encode a list of unsigned integers to a bitarray. It subclasses from <code>DataEncoder</code> class as described in detail in the <a href="scl_tutorial/./basics.html">basics</a>. Therefore, we only need to implement the <code>encode_block</code> function. In this particular case, we can encode symbols individually and concatenate them to output the final bitstream. Hence, we can use the <code>encode_symbol</code> function to encode each symbol individually. The <code>encode_symbol</code> function takes in an unsigned integer and returns a bitarray.</p>
</li>
<li>
<p><em>UniversalUintDecoder</em>: This class is used to decode a bitarray to a list of unsigned integers. It subclasses from <code>DataDecoder</code> class as described in detail in the <a href="scl_tutorial/./basics.html">basics</a>. Therefore, we only need to implement the <code>decode_block</code> function. Similar to <code>encode_block</code>, we can decode symbols individually and append them to a list to output the final list of symbols. Hence, we can use the <code>decode_symbol</code> function to decode each symbol individually. The <code>decode_symbol</code> function takes in a bitarray and returns an unsigned integer and the number of bits consumed to decode the symbol.</p>
</li>
</ul>
<p>Next, we will understand the functioning of the compressor by going through the following exercise. Look at the compressor helper <a href="https://github.com/kedartatwawadi/stanford_compression_library/blob/d221a3208d46e6faab21d05e9b21e87dc088d9ae/scl/compressors/universal_uint_coder.py#L1C7-L1C7">docstring</a> for more details on the compressor.</p>
<ol>
<li>
<p>First, let's get familiarized with how to use an encoder/decoder. The file <code>test_universal_uint_encode_decode</code> shows a simple way to do so. In the example, we encode and decode a list of unsigned integers <code>[0, 1, 3, 4, 100]</code>. Modify the test to encode <code>[23, 30, 100012]</code> and report the length of the <code>encoded_bitarray</code>.</p>
<p><strong>Solution</strong></p>
<p>The length of the encoded bitarray with symbols <code>[23, 30, 100012]</code> is <code>54</code>.</p>
</li>
</ol>
<p>Next, let's now try to understand how the compressor losslessly compresses unsigned integers.</p>
<ol start="2">
<li>
<p>Given an unsigned integer <code>u</code>, what is the length of the code output by <code>encode_symbol</code>?</p>
<p><strong>Solution</strong></p>
<p>We need $\lfloor log_2(u) \rfloor + 1$ bits to represent an unsigned integer <code>u</code> in it's binary form (for $u = 0$, we need 1 bit) and again $\lfloor log_2(u) \rfloor + 1$ bits to encode the length in the unary as described in <code>compressors/universal_uint_coder.py</code> (again for $u = 0$, we need 1 bit for this). Therefore, we need $2\times(\lfloor log_2(u) \rfloor + 1)$ bits to encode an unsigned integer <code>u</code> (and 2 bits if $u = 0$).</p>
</li>
<li>
<p>Briefly explain how the <code>decode_symbol</code> function works, and how is it able to decode the input losslessly.</p>
<p><strong>Solution</strong></p>
<p>The <code>decode_symbol</code> function first finds the number of bits in the binary encoding of the symbol by utilizing the unary coding. In unary coding (as used by <code>compressors/universal_uint_coder.py</code>), if the length of the symbol is $n$, it is encoded as $(n-1)$ <code>1</code>s followed by one <code>0</code>. So in the first part of the code, decoder goes through the bits till it finds a <code>0</code> to find the length of the binary encoding.</p>
<pre><code class="language-python">     # initialize num_bits_consumed
     num_bits_consumed = 0

     # get the symbol length
     while True:
         bit = encoded_bitarray[num_bits_consumed]
         num_bits_consumed += 1
         if bit == 0:
             break
     num_ones = num_bits_consumed
</code></pre>
<p>Next, it uses the knowledge of number of bits (say <code>num_ones</code>) in the binary representation of the unsigned integer to read the next <code>num_ones</code> bits and convert it to an unsigned integer.</p>
<pre><code class="language-python">     # decode the symbol
     symbol = bitarray_to_uint(
         encoded_bitarray[num_bits_consumed: num_bits_consumed + num_ones]
     )
     num_bits_consumed += num_ones
</code></pre>
</li>
<li>
<p>The <code>compressors/universal_uint_coder.py</code> unfortunately only encodes unsigned integers. How will you extend the uint coder to create an encoder/decoder which handles compressing signed integers losslessly? Add your code in the file <code>HWs/tutorial/universal_int_coder.py</code>. NOTE: you mainly have to implement <code>encode_symbol</code> and <code>decode_symbol</code> functions. At the end, the test as present in the file (<code>test_universal_integer_encode_decode</code>) should pass. Report the length of the <code>encoded_bitarray</code> output by the test on Gradescope.</p>
<p><strong>Solution</strong></p>
<p>One way of implementing a <code>universal_integer_coder</code> by utilizing the <code>universal_uint_coder</code> coder is by doing a mapping of negative integers to positive integers. There are multiple possible way to do so and we accept all of them as a solution. One simple way to do the same is by mapping negative integers to positive integers in a zig-zag fashion as follows:</p>
<pre><code> 0 -&gt; 0
+1 -&gt; 1
-1 -&gt; 2
+2 -&gt; 3
-2 -&gt; 4
.
.
.
</code></pre>
<p>The <code>encode_symbol</code> for <code>universal_integer_coder</code> can be implemented as follows:</p>
<pre><code class="language-python">def encode_symbol(self, x: int):
     assert isinstance(x, int)

     #########################
     # ADD CODE HERE
     # Use the self.uint_encoder here
     # raise NotImplementedError
     if x &lt;= 0:
         return self.uint_encoder.encode_symbol(2*(-x))
     else:
         return self.uint_encoder.encode_symbol(2*x - 1)
     ########################
</code></pre>
<p>The <code>decode_symbol</code> for <code>universal_integer_coder</code> can be implemented as follows:</p>
<pre><code class="language-python">def decode_symbol(self, encoded_bitarray):
     #########################
     # ADD CODE HERE
     # Use the self.uint_decoder here
     # raise NotImplementedError
     x, num_bits = self.uint_decoder.decode_symbol(encoded_bitarray)
     return (-x//2, num_bits) if x % 2 == 0 else ((x+1)//2, num_bits)
     ########################
</code></pre>
<p>The length of the <code>encoded_bitarray</code> output using our implementation is <code>66</code>.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quiz-problems-2023"><a class="header" href="#quiz-problems-2023">Quiz Problems (2023)</a></h1>
<p>Here are the quiz problems from the <a href="https://stanforddatacompressionclass.github.io/Fall23/">2023 iteration of the course</a>. Solutions for most of these are discussed in the lectures following the quiz (videos linked in the course website). Solutions for quiz 11, 16 and 17 are provided along with the questions below.</p>
<ol>
<li><a href="quiz_problems_2023.html#quiz-1-lossless-data-compression-basics">Quiz 1 (Lossless Data Compression Basics)</a></li>
<li><a href="quiz_problems_2023.html#quiz-2-prefix-free-codes">Quiz 2 (Prefix Free Codes)</a></li>
<li><a href="quiz_problems_2023.html#quiz-3-kraft-inequality-entropy">Quiz 3 (Kraft Inequality, Entropy)</a></li>
<li><a href="quiz_problems_2023.html#quiz-4-huffman-codes">Quiz 4 (Huffman Codes)</a></li>
<li><a href="quiz_problems_2023.html#quiz-5-asymptotic-equipartition-property">Quiz 5 (Asymptotic Equipartition Property)</a></li>
<li><a href="quiz_problems_2023.html#quiz-6-arithmetic-coding">Quiz 6 (Arithmetic Coding)</a></li>
<li><a href="quiz_problems_2023.html#quiz-7-asymmetric-numeral-systems">Quiz 7 (Asymmetric Numeral Systems)</a></li>
<li><a href="quiz_problems_2023.html#quiz-8-beyond-iid-distributions-conditional-entropy">Quiz 8 (Beyond IID distributions: Conditional entropy)</a></li>
<li><a href="quiz_problems_2023.html#quiz-9-context-based-ac--llm-compression">Quiz 9 (Context-based AC &amp; LLM Compression)</a></li>
<li><a href="quiz_problems_2023.html#quiz-10-lz-and-universal-compression">Quiz 10 (LZ and Universal Compression)</a></li>
<li><a href="quiz_problems_2023.html#quiz-11-lossy-compression-basics-quantization">Quiz 11 (Lossy Compression Basics; Quantization)</a></li>
<li><a href="quiz_problems_2023.html#quiz-12-mutual-information-rate-distortion-function">Quiz 12 (Mutual Information; Rate-Distortion Function)</a></li>
<li><a href="quiz_problems_2023.html#quiz-13-gaussian-rd-water-filling-intuition-transform-coding">Quiz 13 (Gaussian RD, Water-Filling Intuition; Transform Coding)</a></li>
<li><a href="quiz_problems_2023.html#quiz-14-transform-coding-in-real-life-image-audio-etc">Quiz 14 (Transform Coding in real-life: image, audio, etc.)</a></li>
<li><a href="quiz_problems_2023.html#quiz-15-image-compression-jpeg-bpg">Quiz 15 (Image Compression: JPEG, BPG)</a></li>
<li><a href="quiz_problems_2023.html#quiz-16-learnt-image-compression">Quiz 16 (Learnt Image Compression)</a></li>
<li><a href="quiz_problems_2023.html#quiz-17-humans-and-compression">Quiz 17 (Humans and Compression)</a></li>
</ol>
<hr />
<h2 id="quiz-1-lossless-data-compression-basics"><a class="header" href="#quiz-1-lossless-data-compression-basics">Quiz 1 (Lossless Data Compression Basics)</a></h2>
<h3 id="q1-3-points"><a class="header" href="#q1-3-points">Q1 (3 points)</a></h3>
<p>We want to design fixed length codes for an alphabet of size 9.</p>
<h4 id="q11-1-point"><a class="header" href="#q11-1-point">Q1.1 (1 point)</a></h4>
<p>What is the length in bits/symbol for such a code?</p>
<h4 id="q12-1-point"><a class="header" href="#q12-1-point">Q1.2 (1 point)</a></h4>
<p>You observe that there is an overhead above due to 9 not being a power of 2. To partially circumvent this, you decide to encode pairs of symbols together, hence effectively working with an alphabet of size 81. What is the length in bits/symbol for a fixed length code applied on blocks of 2 symbols?</p>
<h4 id="q13-1-point"><a class="header" href="#q13-1-point">Q1.3 (1 point)</a></h4>
<p>Now the alphabet size is 81 because we will be encoding pairs of symbols together, so the length in bits/block is $\lceil{\log_2(81)\rceil} = 7$. However, we are encoding blocks of 2 symbols, so the length in bits/symbol is $\lceil{\log_2(81)\rceil} / 2 = 3.5$. Did working in blocks of 2 give a better compression ratio?</p>
<p>( ) Yes</p>
<p>( ) No</p>
<h3 id="q2-1-point"><a class="header" href="#q2-1-point">Q2 (1 point)</a></h3>
<p>Given a random sequence $X = {x_1, x_2, \ldots, x_n}$ sampled from probability distribution $P(A)=0.5, P(B) = 0.3, P(C) = 0.2$, and the code below, compute the expected code length $E[l(X)]$ in bits/symbol.</p>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>Codeword</th></tr></thead><tbody>
<tr><td>A</td><td><code>0</code></td></tr>
<tr><td>B</td><td><code>01</code></td></tr>
<tr><td>C</td><td><code>11</code></td></tr>
</tbody></table>
</div>
<h2 id="quiz-2-prefix-free-codes"><a class="header" href="#quiz-2-prefix-free-codes">Quiz 2 (Prefix Free Codes)</a></h2>
<h3 id="q1-3-points-1"><a class="header" href="#q1-3-points-1">Q1 (3 points)</a></h3>
<p>Consider the source with alphabet $\mathcal{X} = {A,B,C,D,E}$ and probability distribution though $P(A)=P(B)=P(C)=0.25, P(D)=0.13, P(E)=0.12$. We use a prefix code with codeword lengths $l(x) = \lceil \log_2 \frac{1}{P(x)}\rceil$, e.g., the construction we saw in class.</p>
<h4 id="q11-1-point-1"><a class="header" href="#q11-1-point-1">Q1.1 (1 point)</a></h4>
<p>Provide the codewords for such a code. Note that your answer need not be unique, just ensure the code is prefix-free and satisfies the lengths above.</p>
<h4 id="q12-1-point-1"><a class="header" href="#q12-1-point-1">Q1.2 (1 point)</a></h4>
<p>Compute the expected code length for this code.</p>
<h4 id="q13-1-point-1"><a class="header" href="#q13-1-point-1">Q1.3 (1 point)</a></h4>
<p>Come up with a prefix code that has a lower expected code length than the one above. Compute the expected length for your code.</p>
<h3 id="q2-3-points"><a class="header" href="#q2-3-points">Q2 (3 points)</a></h3>
<p>For each of the codes below, is it prefix free?</p>
<h4 id="q21-1-point"><a class="header" href="#q21-1-point">Q2.1 (1 point)</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>Codeword</th></tr></thead><tbody>
<tr><td>A</td><td><code>00</code></td></tr>
<tr><td>B</td><td><code>01</code></td></tr>
<tr><td>C</td><td><code>10</code></td></tr>
<tr><td>D</td><td><code>11</code></td></tr>
<tr><td>E</td><td><code>110</code></td></tr>
</tbody></table>
</div>
<p>( ) Yes</p>
<p>( ) No</p>
<h4 id="q22-1-point"><a class="header" href="#q22-1-point">Q2.2 (1 point)</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>Codeword</th></tr></thead><tbody>
<tr><td>A</td><td><code>001</code></td></tr>
<tr><td>B</td><td><code>011</code></td></tr>
<tr><td>C</td><td><code>100</code></td></tr>
<tr><td>D</td><td><code>111</code></td></tr>
<tr><td>E</td><td><code>110</code></td></tr>
</tbody></table>
</div>
<p>( ) Yes</p>
<p>( ) No</p>
<h4 id="q23-1-point"><a class="header" href="#q23-1-point">Q2.3 (1 point)</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Symbol</th><th>Codeword</th></tr></thead><tbody>
<tr><td>A</td><td><code>00</code></td></tr>
<tr><td>B</td><td><code>00</code></td></tr>
<tr><td>C</td><td><code>10</code></td></tr>
<tr><td>D</td><td><code>111</code></td></tr>
<tr><td>E</td><td><code>110</code></td></tr>
</tbody></table>
</div>
<p>( ) Yes</p>
<p>( ) No</p>
<h2 id="quiz-3-kraft-inequality-entropy"><a class="header" href="#quiz-3-kraft-inequality-entropy">Quiz 3 (Kraft Inequality, Entropy)</a></h2>
<h3 id="q1-5-points"><a class="header" href="#q1-5-points">Q1 (5 points)</a></h3>
<p>Recall that a random variable <code>X</code> defined on a binary alphabet ${0,1}$ is $Ber(p), p \in [0,1]$ if $P(1) = p, P(0) = 1 - p$.</p>
<h4 id="q11-1-point-2"><a class="header" href="#q11-1-point-2">Q1.1 (1 point)</a></h4>
<p>What is the entropy of random variable $X$, i.e. $H(X) = H(Ber(p)) = $</p>
<p>( ) $p\log_2(\frac{1}{1-p})+p\log_2\frac{1}{p}$</p>
<p>( ) $p\log_2{\frac{1}{p}}$</p>
<p>( ) $p\log_2{\frac{1}{p}} + (1-p)\log_2(\frac{1}{1-p})$</p>
<h4 id="q12-1-point-2"><a class="header" href="#q12-1-point-2">Q1.2 (1 point)</a></h4>
<p>What is the KL-divergence between distributions $Ber(p)$ and $Ber(q)$, $D_{KL}(Ber(p)||Ber(q)) = $</p>
<p>( ) $p \log_2 \frac{p}{q} + (1-p) \log_2 \frac{1-p}{1-q}$</p>
<p>( ) $q \log_2 \frac{p}{q} + (1-q) \log_2 \frac{1-p}{1-q}$</p>
<p>( ) $(1-p) \log_2 \frac{p}{q} + p \log_2 \frac{1-p}{1-q}$</p>
<h4 id="q13-1-point-2"><a class="header" href="#q13-1-point-2">Q1.3 (1 point)</a></h4>
<p>$D_{KL}(Ber(0.3)||Ber(0.7)) = $</p>
<h4 id="q14-1-point"><a class="header" href="#q14-1-point">Q1.4 (1 point)</a></h4>
<p>What is $max_{p\in [0,1], q \in [0,1]} D_{KL}(Ber(p)||Ber(q))$?</p>
<p><strong>Hint:</strong> try setting $p$ or $q$ to $0$.</p>
<p>( ) 1.0</p>
<p>( ) $\pi$</p>
<p>( ) $H(Ber(0.5))$</p>
<p>( ) $\infty$</p>
<h4 id="q15-1-point"><a class="header" href="#q15-1-point">Q1.5 (1 point)</a></h4>
<p>Is $D_{KL}(Ber(p)||Ber(q)) = D_{KL}(Ber(q)||Ber(p))$ for general $p$ and $q$?</p>
<p>( ) Yes</p>
<p>( ) No</p>
<h3 id="q2-3-points-1"><a class="header" href="#q2-3-points-1">Q2 (3 points)</a></h3>
<p>Consider a binary source <code>X</code> with $P(0)=0.999, P(1)=0.001$.</p>
<h4 id="q21-1-point-1"><a class="header" href="#q21-1-point-1">Q2.1 (1 point)</a></h4>
<p>If we design a Shannon tree code for this source (i.e. the construction we saw in the class), what is the expected code length $E[l(X)]$?</p>
<h4 id="q22-1-point-1"><a class="header" href="#q22-1-point-1">Q2.2 (1 point)</a></h4>
<p>What is the entropy $H(X)$ of this source?</p>
<h4 id="q23-1-point-1"><a class="header" href="#q23-1-point-1">Q2.3 (1 point)</a></h4>
<p>How suboptimal is Shannon code in this case? Report $E[l(X)]/H(X)$.</p>
<h2 id="quiz-4-huffman-codes"><a class="header" href="#quiz-4-huffman-codes">Quiz 4 (Huffman Codes)</a></h2>
<h3 id="q1-huffman-codes-on-bernoulli-random-variables-4-points"><a class="header" href="#q1-huffman-codes-on-bernoulli-random-variables-4-points">Q1: Huffman codes on Bernoulli random variables (4 points)</a></h3>
<p>Recall that a $Ber(p), p \in [0,1]$ random variable is defined on a binary alphabet ${0,1}$ such that $P(1) = p$.</p>
<h4 id="q11-1-point-3"><a class="header" href="#q11-1-point-3">Q1.1 (1 point)</a></h4>
<p>What is the Huffman Code for $Ber(0.5)$?</p>
<h4 id="q12-1-point-3"><a class="header" href="#q12-1-point-3">Q1.2 (1 point)</a></h4>
<p>What is the Huffman Code for $Ber(0.9)$?</p>
<h4 id="q13-1-point-3"><a class="header" href="#q13-1-point-3">Q1.3 (1 point)</a></h4>
<p>State True or False:
Using the above Huffman code is the best you can do to encode a $Ber(0.5)$ source?</p>
<p>( ) True</p>
<p>( ) False</p>
<h4 id="q14-1-point-1"><a class="header" href="#q14-1-point-1">Q1.4 (1 point)</a></h4>
<p>State True or False:
Using the above Huffman code is the best you can do to encode a $Ber(0.9)$ source?</p>
<p>( ) True</p>
<p>( ) False</p>
<h3 id="q2-huffman-codes-on-particular-realization-of-a-source-4-points"><a class="header" href="#q2-huffman-codes-on-particular-realization-of-a-source-4-points">Q2: Huffman codes on particular realization of a source (4 points)</a></h3>
<p>Assume the following alphabet $\mathcal{X} = {A, B, C, D}$ with corresponding probabilities of occurrence for these symbols being $P = {0.25, 0.25, 0.25, 0.25}$.</p>
<p>Before Huffman solved the optimal prefix-free coding problem, Mahatma Gandhi (no relation!) also came up with another code for these symbols (let's call them Gandhi codes):</p>
<ul>
<li>A -&gt; 0</li>
<li>B -&gt; 10</li>
<li>C -&gt; 110</li>
<li>D -&gt; 111</li>
</ul>
<p>Your friend argues Gandhi's code is better than Huffman's code. For this they generate a random instance of this source and get the symbols $AAABBBCD$.</p>
<h4 id="q21-1-point-2"><a class="header" href="#q21-1-point-2">Q2.1 (1 point)</a></h4>
<p>What is the total length of communicated bitstream for the above random instance using Gandhi codes?</p>
<h4 id="q22-1-point-2"><a class="header" href="#q22-1-point-2">Q2.2 (1 point)</a></h4>
<p>What is the total length of communicated bitstream for the above random instance using Huffman codes for the given source distribution?</p>
<h4 id="q23-1-point-2"><a class="header" href="#q23-1-point-2">Q2.3 (1 point)</a></h4>
<p>True or False:
Gandhi codes are optimal code for this source.</p>
<p>( ) True</p>
<p>( ) False</p>
<h4 id="q24-1-point"><a class="header" href="#q24-1-point">Q2.4 (1 point)</a></h4>
<p>True or False:
Gandhi codes are optimal if you had to communicate this particular instance of the source.</p>
<p>( ) True</p>
<p>( ) False</p>
<h2 id="quiz-5-asymptotic-equipartition-property"><a class="header" href="#quiz-5-asymptotic-equipartition-property">Quiz 5 (Asymptotic Equipartition Property)</a></h2>
<h3 id="q1-typical-set-size-2-points"><a class="header" href="#q1-typical-set-size-2-points">Q1: Typical Set Size (2 points)</a></h3>
<p>Consider a binary source with $P(0)=P(1)=0.5$. What is the size of the typical set $A^{(n)}$, in terms of $n$?</p>
<h3 id="q2-kl-divergence-3-points"><a class="header" href="#q2-kl-divergence-3-points">Q2: KL Divergence (3 points)</a></h3>
<p>Consider a source $P=Ber(0.5)$. Zoro knows the distribution of this source and designs a per-symbol Huffman code for $P=Ber(0.5)$ to encode a sequence of symbols obtained using this source. However, Luffy doesn't know the distribution of this source and encodes it using a per-symbol Huffman code assuming that the sequence of symbols came from $Q=Ber(0.25)$.</p>
<h4 id="q21-1-point-3"><a class="header" href="#q21-1-point-3">Q2.1 (1 point)</a></h4>
<p>How many extra number of bits in expectation (per-symbol) does Luffy need over Zoro to encode a sequence from the above source $P$?</p>
<h4 id="q22-1-point-3"><a class="header" href="#q22-1-point-3">Q2.2 (1 point)</a></h4>
<p>What is the KL divergence $D(P||Q)$ between distributions $P$ and $Q$ specified above?</p>
<h4 id="q23-1-point-3"><a class="header" href="#q23-1-point-3">Q2.3 (1 point)</a></h4>
<p>In the class we learnt that KL divergence is an indicator of the excess code-length for mismatched codes. How do you explain that the two answers above do not match?</p>
<h2 id="quiz-6-arithmetic-coding"><a class="header" href="#quiz-6-arithmetic-coding">Quiz 6 (Arithmetic Coding)</a></h2>
<h3 id="q1-code-length-vs-code-value-2-points"><a class="header" href="#q1-code-length-vs-code-value-2-points">Q1: Code-length vs Code Value (2 points)</a></h3>
<p>Consider a Bernoulli random variable (we have been seeing them a lot in the quizzes so far :)) - $Ber(p)$ where $P(A)=p$ and $P(B)=(1-p)$. Consider sequence of symbols <code>AB</code> and <code>BA</code>, to be encoded using Arithmetic Coding (assume idealized version where the codelength is equal to $\log_2(\frac{1}{P(X^n)})$).</p>
<h4 id="q11-1-point-4"><a class="header" href="#q11-1-point-4">Q1.1 (1 point)</a></h4>
<p><code>AB</code> and <code>BA</code> have the same code-length.</p>
<p>( ) True</p>
<p>( ) False</p>
<h4 id="q12-1-point-4"><a class="header" href="#q12-1-point-4">Q1.2 (1 point)</a></h4>
<p><code>AB</code> and <code>BA</code> have the same output codeword.</p>
<p>( ) True</p>
<p>( ) False</p>
<h3 id="q2-arithmetic-decoder-3-points"><a class="header" href="#q2-arithmetic-decoder-3-points">Q2: Arithmetic Decoder (3 points)</a></h3>
<p>Assume a probability distribution over symbols $\mathcal{X}={A, B, C}$ with respective probabilities $p(A) = 0.5$, $p(B) = 0.25$, $p(C) = 0.25$. An arithmetic decoder receives as input bitstream <code>100111</code> for an input of length 3. What is the decoded sequence?</p>
<h2 id="quiz-7-asymmetric-numeral-systems"><a class="header" href="#quiz-7-asymmetric-numeral-systems">Quiz 7 (Asymmetric Numeral Systems)</a></h2>
<h3 id="q1-rans-encoding-4-points"><a class="header" href="#q1-rans-encoding-4-points">Q1: rANS encoding (4 points)</a></h3>
<p>Recall the rANS encoding procedure as discussed in class (For reference you can look at Slides 44 and 45 in lecture slides on website to see summary of encoding and decoding steps: https://stanforddatacompressionclass.github.io/Fall23/static_files/L7.pdf)</p>
<p>In this question, we will ask you to encode and decode a sequence. We will be using same symbols as discussed in class for ease.</p>
<p>Say $\mathcal{X} = {0, 1, 2}$ be our symbols with probabilities ${3/8, 3/8, 2/8}$ respectively. You want to encode stream of symbols <code>2,0,1,0</code> using rANS.</p>
<h4 id="q11-1-point-5"><a class="header" href="#q11-1-point-5">Q1.1 (1 point)</a></h4>
<p>What's the state value (<code>x</code>) in rANS after symbol <code>2</code>?</p>
<h4 id="q12-1-point-5"><a class="header" href="#q12-1-point-5">Q1.2 (1 point)</a></h4>
<p>What's the state value (<code>x</code>) in rANS after encoding symbols <code>2,0</code>?</p>
<h4 id="q13-1-point-4"><a class="header" href="#q13-1-point-4">Q1.3 (1 point)</a></h4>
<p>What's the final state value (<code>x</code>) at the end of encoding stream <code>2,0,1,0</code>?</p>
<h3 id="q2-rans-decoding-4-points"><a class="header" href="#q2-rans-decoding-4-points">Q2: rANS decoding (4 points)</a></h3>
<p>Now we will decode using rANS. We have the same setup as before. Now your decoder knows that the number of symbols are <code>4</code> and the final state your decoder received is <code>117</code>.</p>
<h4 id="q21-1-point-4"><a class="header" href="#q21-1-point-4">Q2.1 (1 point)</a></h4>
<p>What is the value of <code>block_id</code> after running the <code>decode_block</code> for first time?</p>
<h4 id="q22-1-point-4"><a class="header" href="#q22-1-point-4">Q2.2 (1 point)</a></h4>
<p>What is the value of <code>slot</code> after running the <code>decode_block</code> for first time?</p>
<h4 id="q23-1-point-4"><a class="header" href="#q23-1-point-4">Q2.3 (1 point)</a></h4>
<p>What is the first decoded symbol? Note that this corresponds to the last encoded symbol since rANS decoding proceeds in reverse.</p>
<h4 id="q24-1-point-1"><a class="header" href="#q24-1-point-1">Q2.4 (1 point)</a></h4>
<p>What is the updated state value (<code>x</code>) after first step?</p>
<h3 id="q3-when-to-use-ans-2-points"><a class="header" href="#q3-when-to-use-ans-2-points">Q3: When to use ANS? (2 points)</a></h3>
<p>In which of the following scenarios would you consider using ANS as opposed to Huffman or Arithmetic coding? Select all that apply.</p>
<p>[ ] Your application requires the best compression ratio and you are willing to sacrifice encoding/decoding speeds.</p>
<p>[ ] Your application requires extremely fast encoding/decoding and you are willing to sacrifice compression ratio.</p>
<p>[ ] Your application requires adaptive decoding (i.e., the encoder and decoder need to build a model as they go through the data).</p>
<p>[ ] You care about achieving close-to-optimal compression but also want good speed.</p>
<p>[ ] You are working with a modern processor and want to exploit parallel processing to get higher speeds, while still achieving close-to-optimal compression.</p>
<h2 id="quiz-8-beyond-iid-distributions-conditional-entropy"><a class="header" href="#quiz-8-beyond-iid-distributions-conditional-entropy">Quiz 8 (Beyond IID distributions: Conditional entropy)</a></h2>
<h3 id="q1-entropy-for-markov-chain-5-points"><a class="header" href="#q1-entropy-for-markov-chain-5-points">Q1: Entropy for Markov Chain (5 points)</a></h3>
<p>Recall the Markov chain setting we covered in class.</p>
<p>Now, we change it to the following setting</p>
<p>$$ U_1 \sim Ber(0.5)$$
$$ P(U_{i+1} = 1| U_i = 0) = 1$$
$$ P(U_{i+1} = 0| U_i = 0) = 0$$
$$ P(U_{i+1} = 1| U_i = 1) = 0.5$$
$$ P(U_{i+1} = 0| U_i = 1) = 0.5$$</p>
<h4 id="q11-1-point-6"><a class="header" href="#q11-1-point-6">Q1.1 (1 point)</a></h4>
<p>What is $H(U_1)$?</p>
<h4 id="q12-1-point-6"><a class="header" href="#q12-1-point-6">Q1.2 (1 point)</a></h4>
<p>What is $H(U_2)$?</p>
<h4 id="q13-1-point-5"><a class="header" href="#q13-1-point-5">Q1.3 (1 point)</a></h4>
<p>What is $H(U_2 | U_1)$?</p>
<h4 id="q14-1-point-2"><a class="header" href="#q14-1-point-2">Q1.4 (1 point)</a></h4>
<p>Is this process stationary?</p>
<p>( ) Yes</p>
<p>( ) No</p>
<h4 id="q15-1-point-1"><a class="header" href="#q15-1-point-1">Q1.5 (1 point)</a></h4>
<p>Now consider the chain with the same transition probabilities but the initial distribution $P(U_1)$ modified to the stationary distribution of this Markov Chain which is $Ber(2/3)$, i.e. $P(U_1=0) = 1/3$ and $P(U_1=1) = 2/3$.</p>
<p>Calculate the entropy rate $H(\mathbf{U})$ of this stationary Markov source.</p>
<h2 id="quiz-9-context-based-ac--llm-compression"><a class="header" href="#quiz-9-context-based-ac--llm-compression">Quiz 9 (Context-based AC &amp; LLM Compression)</a></h2>
<h3 id="q1-select-the-best-compressor-2-points"><a class="header" href="#q1-select-the-best-compressor-2-points">Q1: Select the best compressor (2 points)</a></h3>
<p>Which lossless compressor is most suitable under following situations:</p>
<h4 id="q11-1-point-7"><a class="header" href="#q11-1-point-7">Q1.1 (1 point)</a></h4>
<p>You know that the data is roughly 2nd order Markov but do not know the transition probabilities.</p>
<p>( ) context-based arithmetic coding</p>
<p>( ) context-based adaptive arithmetic coding</p>
<h4 id="q12-1-point-7"><a class="header" href="#q12-1-point-7">Q1.2 (1 point)</a></h4>
<p>You know that the data is 3rd order Markov and know the exact distribution.</p>
<p>( ) context-based arithmetic coding</p>
<p>( ) context-based adaptive arithmetic coding</p>
<h3 id="q2-first-order-adaptive-arithmetic-coding-3-points"><a class="header" href="#q2-first-order-adaptive-arithmetic-coding-3-points">Q2: First order adaptive arithmetic coding (3 points)</a></h3>
<p>Consider first order adaptive arithmetic coding with $\mathcal{X} = {A,B,C}$. The initial counts are set to 1, i.e., $c(A,A)=c(A,B)=\dots=c(C,C)=1$.</p>
<p>You are encoding $X_1X_2X_3=BAB$, and assume that for encoding the first symbol $X_1=B$ you take $X_0=A$ (padding). Thus, after encoding $X_1$, you will have $c(A,B)=2$ and the rest of the counts still $1$. Let $\hat{P}$ be the probability model for this predictor.</p>
<h4 id="q21-1-point-5"><a class="header" href="#q21-1-point-5">Q2.1 (1 point)</a></h4>
<p>What is $\hat{P}(X_2=A|X_0X_1=AB)$?</p>
<h4 id="q22-1-point-5"><a class="header" href="#q22-1-point-5">Q2.2 (1 point)</a></h4>
<p>What is $\hat{P}(X_3=B|X_0X_1X_2=ABA)$?</p>
<h4 id="q23-1-point-5"><a class="header" href="#q23-1-point-5">Q2.3 (1 point)</a></h4>
<p>Assuming arithmetic coding uses exactly $\sum_{i} \log_2 \frac{1}{\hat{P}(X_i|X_0,\dots,X_{i-1})}$, what is the encoded size for the sequence $X_1X_2X_3=BAB$?</p>
<h2 id="quiz-10-lz-and-universal-compression"><a class="header" href="#quiz-10-lz-and-universal-compression">Quiz 10 (LZ and Universal Compression)</a></h2>
<h3 id="q1-lz77-decoder-3-points"><a class="header" href="#q1-lz77-decoder-3-points">Q1: LZ77 Decoder (3 points)</a></h3>
<p>Your decoder receives the following output from a LZ77 implementation. Note that your decoder doesn't know the parser which was used to generate this table (and in-fact you can show it's not the same as we have seen in the class) -- e.g., the decoder doesn't know minimum match-length used during generation of this table. In this question we will decode this table step-by-step.</p>
<p>Table:</p>
<div class="table-wrapper"><table><thead><tr><th>Unmatched Literals</th><th>Match Length</th><th>Match Offset</th></tr></thead><tbody>
<tr><td>AABBB</td><td>4</td><td>1</td></tr>
<tr><td>-</td><td>5</td><td>9</td></tr>
<tr><td>CDCD</td><td>2</td><td>2</td></tr>
</tbody></table>
</div>
<h4 id="q11-1-point-8"><a class="header" href="#q11-1-point-8">Q1.1 (1 point)</a></h4>
<p>What is the output string after parsing the first row? Your answer should be the symbols which the decoder outputs (e.g., ABABABABA).</p>
<h4 id="q12-1-point-8"><a class="header" href="#q12-1-point-8">Q1.2 (1 point)</a></h4>
<p>What is the output string after parsing the second row?</p>
<h4 id="q13-1-point-6"><a class="header" href="#q13-1-point-6">Q1.3 (1 point)</a></h4>
<p>What is the output string after parsing the third row?</p>
<h3 id="q2-lz77-encoding-3-points"><a class="header" href="#q2-lz77-encoding-3-points">Q2: LZ77 Encoding (3 points)</a></h3>
<p>You realize that Kedar's last name (<code>TATWAWADI</code>) is a very good example to try LZ77 parsing as covered in class. Pulkit tried to do the LZ77 parsing and obtained the following table. Fill the cells with missing elements.</p>
<p>Partial Table:</p>
<div class="table-wrapper"><table><thead><tr><th>Unmatched Literals</th><th>Match Length</th><th>Match Offset</th></tr></thead><tbody>
<tr><td>TA</td><td>1</td><td><code>X1</code></td></tr>
<tr><td>W</td><td><code>X2</code></td><td>3</td></tr>
<tr><td>-</td><td>2</td><td>2</td></tr>
<tr><td><code>X3</code></td><td>-</td><td>-</td></tr>
</tbody></table>
</div>
<h4 id="q21-1-point-6"><a class="header" href="#q21-1-point-6">Q2.1 (1 point)</a></h4>
<p>What is match offset <code>X1</code>?</p>
<h4 id="q22-1-point-6"><a class="header" href="#q22-1-point-6">Q2.2 (1 point)</a></h4>
<p>What is match length <code>X2</code>?</p>
<h4 id="q23-1-point-6"><a class="header" href="#q23-1-point-6">Q2.3 (1 point)</a></h4>
<p>What are the unmatched literals <code>X3</code>?</p>
<h3 id="q3-2-points"><a class="header" href="#q3-2-points">Q3 (2 points)</a></h3>
<p>Consider an English text. Also consider a reversibly transformed version where each byte is replaced with itself plus one. So A becomes B, B becomes C and so on.</p>
<h4 id="q31-1-point"><a class="header" href="#q31-1-point">Q3.1 (1 point)</a></h4>
<p>Zstd would perform similarly on both the original text and the transformed version.</p>
<p>( ) True</p>
<p>( ) False</p>
<h4 id="q32-1-point"><a class="header" href="#q32-1-point">Q3.2 (1 point)</a></h4>
<p>A LLM-based compressor trained on English would perform similarly on both the original text and the transformed version.</p>
<p>( ) True</p>
<p>( ) False</p>
<h3 id="q4-1-point"><a class="header" href="#q4-1-point">Q4 (1 point)</a></h3>
<p>Your company produces a lot of data of a particular kind and you are asked to find ways to efficiently compress it to save on bandwidth and storage costs. Which of these is a good approach to go about this?</p>
<p>( ) Use CMIX since your company deserves the best possible compression irrespective of the compute costs.</p>
<p>( ) Use gzip because LZ77 is an universal algorithm and so it has to be the best compressor for every occasion.</p>
<p>( ) Make a statistically accurate model of your data, design a predictor, and then use a context-based arithmetic coder.</p>
<p>( ) Understand the application requirements, try existing compressors like zstd and then evaluate whether there are benefits to create a domain specific compressor based on an approximate model for the data.</p>
<h2 id="quiz-11-lossy-compression-basics-quantization"><a class="header" href="#quiz-11-lossy-compression-basics-quantization">Quiz 11 (Lossy Compression Basics; Quantization)</a></h2>
<h3 id="q1-quantization-of-a-uniform-rv-5-points"><a class="header" href="#q1-quantization-of-a-uniform-rv-5-points">Q1: Quantization of a Uniform RV (5 points)</a></h3>
<p>You are given samples from a Uniform random variable $U \sim\text{Unif}[0, 1]$. We will quantize samples from the random variable $U$ under mean-square error (MSE) distortion criteria in the following questions.</p>
<h4 id="q11-optimal-scalar-one-bit-quantizer-1-point"><a class="header" href="#q11-optimal-scalar-one-bit-quantizer-1-point">Q1.1: Optimal Scalar One-bit Quantizer (1 point)</a></h4>
<p>What are the optimal quantization levels (intervals and reconstruction points) for a scalar one-bit quantizer under MSE distortion?</p>
<h4 id="q12-distortion-under-scalar-quantizer-1-point"><a class="header" href="#q12-distortion-under-scalar-quantizer-1-point">Q1.2: Distortion under scalar quantizer (1 point)</a></h4>
<p>What is the average distortion per-symbol under the optimal scalar quantization scheme described above?</p>
<h4 id="q13-vector-quantization-1-point"><a class="header" href="#q13-vector-quantization-1-point">Q1.3: Vector quantization (1 point)</a></h4>
<p>Now we will use a 2D vector quantizer under MSE distortion which still uses one-bit per symbol. This quantizer takes 2 symbols at a time, represents them as a 2D vector and then uses the optimal 2-bit quantizer.</p>
<p>Which of the following vector quantizer is best in terms of distortion under MSE? Choose all options which have the same lowest expected MSE distortion.</p>
<p>[ ] ${(0.125,0.5), (0.375,0.5), (0.625,0.5), (0.875,0.5)}$</p>
<p>[ ] ${(0.5,0.125), (0.5,0.375), (0.5,0.625), (0.5,0.875)}$</p>
<p>[ ] ${(0.25, 0.25), (0.25,0.75), (0.75,0.25), (0.75,0.75)}$</p>
<h4 id="q14-which-is-better-1-point"><a class="header" href="#q14-which-is-better-1-point">Q1.4: Which is better? (1 point)</a></h4>
<p>Which of the two -- scalar or the best 2D quantizer above, is better for lossy compression of uniformly distributed data?
Hint: Remember to compare with respect to bits per symbol.</p>
<p>( ) Scalar</p>
<p>( ) 2D-Vector</p>
<p>( ) Both are equivalent</p>
<h3 id="solutions"><a class="header" href="#solutions">Solutions</a></h3>
<p>Quiz 11 solutions were not covered in lecture, so here they are!</p>
<p>1.1</p>
<p>Solution: For a one-bit quantizer, we need to divide the interval $[0,1]$ into two parts and assign a reconstruction point to each. By symmetry, we can argue that the two intervals must be $[0,0.5]$ and $[0.5,1]$. Now for the reconstruction points, we need to find a point that minimizes the $\int_0^{0.5} (x-\hat{x})^2 dx$ for the first interval which is simply the mean value $0.25$ (can verify that computing the integral and then finding the minima - in general for a different distortion we are looking for the "median" corresponding to that distortion, for MSE the median is the mean!). Similarly for the second interval the reconstruction point will be $0.75$.</p>
<p>$X&lt;1/2$, $X&gt;1/2$ Reconstruction points = ${0.25, 0.75}$</p>
<p>1.2</p>
<p>Solution: Let us first compute the integral $\int_0^{\Delta} (x-\frac{\Delta}{2})^2 dx$ which evaluates to $\frac{\Delta^3}{12}$. Now for our case the distortion will be given by the squared error across the range of $x$ from $[0,1]$ (note that the pdf is simply equal to $1$ in this whole range). We can split into two intervals according to the reconstruction point and get $$\int_0^{0.5} (x-0.25)^2 dx + \int_{0.5}^{1} (x-0.75)^2 dx$$ Reusing the result we just computed with $\Delta = 0.5$, we get the answer as $$2\times \frac{0.5^3}{12} \approx 0.0208$$</p>
<p>1.3</p>
<p>[ ] ${(0.125,0.5), (0.375,0.5), (0.625,0.5), (0.875,0.5)}$</p>
<p>[ ] ${(0.5,0.125), (0.5,0.375), (0.5,0.625), (0.5,0.875)}$</p>
<p>[X] ${(0.25, 0.25), (0.25,0.75), (0.75,0.25), (0.75,0.75)}$</p>
<p>Solution: Here we are trading off between assiging two intervals to each dimension vs. providing all four intervals to a single dimension. We can reuse the result from above for this: $$\int_0^{\Delta} (x-\frac{\Delta}{2})^2 dx = \frac{\Delta^3}{12}$$</p>
<p>A single interval (zero bits) would have $\Delta = 1$ giving us $\frac{1}{12} \approx 0.083$. Two intervals (one bit) would give us $\Delta = 0.5$ giving us $2\times\frac{0.5^3}{12} \approx 0.0208$. Four intervals (two bits) would give us $\Delta = 0.25$ giving us $4\times\frac{0.25^3}{12} \approx 0.0052$.</p>
<p>Now we can see that using one bit for each dimension gives us the lowest sum because $0.0208+0.0208 &lt; 0.083 + 0.0052$. Thus the correct option is the third one.</p>
<p>1.4</p>
<p>( ) Scalar</p>
<p>( ) 2D-Vector</p>
<p>(X) Both are equivalent</p>
<p>Solution: Both are equivalent because the 2D quantizer we chose above effectively does optimal scalar quantization with one bit per dimension.</p>
<h2 id="quiz-12-mutual-information-rate-distortion-function"><a class="header" href="#quiz-12-mutual-information-rate-distortion-function">Quiz 12 (Mutual Information; Rate-Distortion Function)</a></h2>
<h3 id="q1-calculate-mutual-information-2-points"><a class="header" href="#q1-calculate-mutual-information-2-points">Q1: Calculate Mutual Information (2 points)</a></h3>
<p>You have been given following joint probability distribution table for $(X,Y)$ on binary alphabets:</p>
<div class="table-wrapper"><table><thead><tr><th>P(X=x,Y=y)</th><th>y = 0</th><th>y = 1</th></tr></thead><tbody>
<tr><td>x = 0</td><td>0.5</td><td>0</td></tr>
<tr><td>x = 1</td><td>0.25</td><td>0.25</td></tr>
</tbody></table>
</div>
<h4 id="q11-joint-entropy-1-point"><a class="header" href="#q11-joint-entropy-1-point">Q1.1: Joint Entropy (1 point)</a></h4>
<p>Calculate the joint entropy $H(X,Y)$.</p>
<h4 id="q12-mutual-information-1-point"><a class="header" href="#q12-mutual-information-1-point">Q1.2: Mutual Information (1 point)</a></h4>
<p>Calculate the mutual information $I(X;Y)$.</p>
<h3 id="q2-rate-distortion-2-points"><a class="header" href="#q2-rate-distortion-2-points">Q2: Rate-Distortion (2 points)</a></h3>
<p>Consider a uniformly distributed source on alphabet ${0, 1, 2}$.</p>
<p>You have been asked to lossily compress this source under MSE (mean square error) distortion and have been asked to calculate the rate distortion function $R(D)$ for a given distortion value $D$.</p>
<h4 id="q21-1-point-7"><a class="header" href="#q21-1-point-7">Q2.1 (1 point)</a></h4>
<p>What is $R(D=0)$?</p>
<h4 id="q22-1-point-7"><a class="header" href="#q22-1-point-7">Q2.2 (1 point)</a></h4>
<p>What is $R(D=1)$?</p>
<h3 id="q3-1-point"><a class="header" href="#q3-1-point">Q3 (1 point)</a></h3>
<p>For a $Ber(1/2)$ source with Hamming distortion, we saw in class that $R(D) = 1-H_b(D)$, where $H_b(p)$ is entropy of a binary random variable with probability $p$. Which of the following are correct?
(Choose all that apply)</p>
<p>[ ] There exists a scheme working on large block sizes achieving distortion D and rate &lt; $1-H_b(D)$.</p>
<p>[ ] There exists a scheme working on large block sizes achieving distortion D and rate &gt; $1-H_b(D)$.</p>
<p>[ ] There exists a scheme working on large block sizes achieving distortion D and rate arbitrarily close to $1-H_b(D)$.</p>
<p>[ ] There exists a scheme working on single symbols at a time (block size = 1) achieving distortion D and rate arbitrarily close to $1-H_b(D)$.</p>
<h2 id="quiz-13-gaussian-rd-water-filling-intuition-transform-coding"><a class="header" href="#quiz-13-gaussian-rd-water-filling-intuition-transform-coding">Quiz 13 (Gaussian RD, Water-Filling Intuition; Transform Coding)</a></h2>
<h3 id="q1-reverse-water-filling-3-points"><a class="header" href="#q1-reverse-water-filling-3-points">Q1: Reverse water filling (3 points)</a></h3>
<p><strong>Note</strong>: The problem appears long, but it's mostly just a recap from class!</p>
<p>Recall the problem of compressing two independent Gaussian sources $X_1,X_2$ with means $0$ and variances $\sigma_1^2$ and $\sigma_2^2$. For the squared error distortion we saw in class, the rate distortion function is given by</p>
<p>$$min_{\frac{1}{2} (D_1+D_2)\leq D}\  {1\over 2} [({1\over 2} \log \frac{\sigma_1^2}{D_1})<em>+ + ({1\over 2} \log \frac{\sigma_2^2}{D_2})</em>+]$$</p>
<p>where $(x)_+ = \max{0,x}$.</p>
<p>The solution suggests we split the overall distortion between the two components and then use the optimal strategy for each component independently.</p>
<p>We also saw in class that the optimal split into $D_1$ and $D_2$ is given by a reverse water filling idea, which is expressed in equation as follows:</p>
<p>For a given parameter $\theta$, a point on the optimal rate distortion curve is achieved by setting</p>
<p>$$D_i = min {\theta, \sigma_i^2}$$ for $i = 1,2$
$$D = {1\over 2} (D_1+D_2)$$</p>
<p>And the rate given by
$${1\over 2} [({1\over 2} \log \frac{\sigma_1^2}{D_1})<em>+ + ({1\over 2} \log \frac{\sigma_2^2}{D_2})</em>+]$$</p>
<p>This can be expressed in figures as follows (assuming without loss of generality that $\sigma_1^2 &lt; \sigma_2^2$):</p>
<p>When $D$ is smaller than both $\sigma_1^2$ and $\sigma_2^2$, we choose both $D_1$ and $D_2$ to be equal to $D$.
<img src="quiz_problems_2023_images/quiz13_water_filling_1.png" alt="" /></p>
<p>When $D$ exceeds $\sigma_1^2$ but is below $\frac{1}{2}(\sigma_1^2 + \sigma_2^2)$, we set $D_1$ to be $\sigma_1^2$, and choose $D_2$ such that the average distortion is $D$.
<img src="quiz_problems_2023_images/quiz13_water_filling_2.png" alt="" /></p>
<p>When $D$ is equal to $\frac{1}{2}(\sigma_1^2 + \sigma_2^2)$ we can just set $D_1 = \sigma_1^2$ and $D_2 = \sigma_2^2$.
<img src="quiz_problems_2023_images/quiz13_water_filling_3.png" alt="" /></p>
<p>Now consider a setting with $\sigma_1^2 = 1$ and $\sigma_2^2 = 3$.</p>
<h4 id="q11-1-point-9"><a class="header" href="#q11-1-point-9">Q1.1 (1 point)</a></h4>
<p>At $D = 1.5$, what are the optimal values of $D_1$ and $D_2$:</p>
<p>( ) $D_1 = D_2 = 1$</p>
<p>( ) $D_1 = D_2 = 0.5$</p>
<p>( ) $D_1 = 1$, $D_2 = 3$</p>
<p>( ) $D_1 = 1$, $D_2 = 2$</p>
<h4 id="q12-1-point-9"><a class="header" href="#q12-1-point-9">Q1.2 (1 point)</a></h4>
<p>At $D = 2$, what is the optimal rate</p>
<p>( ) $0$ bits/source component</p>
<p>( ) $1$ bits/source component</p>
<p>( ) $2$ bits/source component</p>
<p>( ) $3$ bits/source component</p>
<h4 id="q13-1-point-7"><a class="header" href="#q13-1-point-7">Q1.3 (1 point)</a></h4>
<p>Which of the following is correct?</p>
<p>[ ] For $D$ below the two variances, we divide the distortions equally among the two components.</p>
<p>[ ] For $D$ below the two variances, we use a higher bitrate for the component with higher variance.</p>
<p>[ ] For $D$ between the two variances, we use zero bitrate for one of the component.</p>
<p>[ ] For $D$ between the two variances, we use zero bitrate for both of the components.</p>
<h2 id="quiz-14-transform-coding-in-real-life-image-audio-etc"><a class="header" href="#quiz-14-transform-coding-in-real-life-image-audio-etc">Quiz 14 (Transform Coding in real-life: image, audio, etc.)</a></h2>
<h3 id="q1-2-points"><a class="header" href="#q1-2-points">Q1 (2 points)</a></h3>
<h4 id="q11-vector-quantization-1-point"><a class="header" href="#q11-vector-quantization-1-point">Q1.1: Vector Quantization (1 point)</a></h4>
<p>In which of the following cases do you expect vector quantization to improve the lossy compression performance?
(select all the correct options)</p>
<p>[ ] i.i.d. data compressed with scalar quantization</p>
<p>[ ] non-i.i.d. (correlated) data with scalar quantization</p>
<h4 id="q12-transform-coding-1-point"><a class="header" href="#q12-transform-coding-1-point">Q1.2: Transform Coding (1 point)</a></h4>
<p>In which of the following cases do you expect transform coding to improve the lossy compression performance?
(select all the correct options)</p>
<p>[ ] i.i.d. data</p>
<p>[ ] non-i.i.d. (correlated) data</p>
<h3 id="q2-3-points-2"><a class="header" href="#q2-3-points-2">Q2 (3 points)</a></h3>
<p>Match the signals to their DCT!</p>
<h4 id="signal-1"><a class="header" href="#signal-1">Signal 1</a></h4>
<p><img src="quiz_problems_2023_images/quiz14_signal_1.png" alt="" /></p>
<h4 id="signal-2"><a class="header" href="#signal-2">Signal 2</a></h4>
<p><img src="quiz_problems_2023_images/quiz14_signal_2.png" alt="" /></p>
<h4 id="signal-3"><a class="header" href="#signal-3">Signal 3</a></h4>
<p><img src="quiz_problems_2023_images/quiz14_signal_3.png" alt="" /></p>
<h4 id="dct-a"><a class="header" href="#dct-a">DCT A</a></h4>
<p><img src="quiz_problems_2023_images/quiz14_dct_1.png" alt="" /></p>
<h4 id="dct-b"><a class="header" href="#dct-b">DCT B</a></h4>
<p><img src="quiz_problems_2023_images/quiz14_dct_2.png" alt="" /></p>
<h4 id="dct-c"><a class="header" href="#dct-c">DCT C</a></h4>
<p><img src="quiz_problems_2023_images/quiz14_dct_3.png" alt="" /></p>
<h4 id="q21-1-point-8"><a class="header" href="#q21-1-point-8">Q2.1 (1 point)</a></h4>
<p>DCT for signal 1:</p>
<p>( ) DCT A</p>
<p>( ) DCT B</p>
<p>( ) DCT C</p>
<h4 id="q22-1-point-8"><a class="header" href="#q22-1-point-8">Q2.2 (1 point)</a></h4>
<p>DCT for signal 2:</p>
<p>( ) DCT A</p>
<p>( ) DCT B</p>
<p>( ) DCT C</p>
<h4 id="q23-1-point-7"><a class="header" href="#q23-1-point-7">Q2.3 (1 point)</a></h4>
<p>DCT for signal 3:</p>
<p>( ) DCT A</p>
<p>( ) DCT B</p>
<p>( ) DCT C</p>
<h3 id="q3-dct-truncation-1-point"><a class="header" href="#q3-dct-truncation-1-point">Q3: DCT truncation (1 point)</a></h3>
<p><img src="quiz_problems_2023_images/quiz14_dct_trunc_orig.png" alt="" /></p>
<p>For the signal shown above, we take the DCT and truncate (zero out) the 16 highest frequencies (out of 32 total components in the DCT). Identify the reconstructed signal obtained after performing the inverse DCT.</p>
<p>( ) A</p>
<p><img src="quiz_problems_2023_images/quiz14_dct_trunc_recon_1.png" alt="" /></p>
<p>( ) B</p>
<p><img src="quiz_problems_2023_images/quiz14_dct_trunc_recon_2.png" alt="" /></p>
<p>( ) C</p>
<p><img src="quiz_problems_2023_images/quiz14_dct_trunc_recon_3.png" alt="" /></p>
<h2 id="quiz-15-image-compression-jpeg-bpg"><a class="header" href="#quiz-15-image-compression-jpeg-bpg">Quiz 15 (Image Compression: JPEG, BPG)</a></h2>
<h3 id="q1-image-compression-4-points"><a class="header" href="#q1-image-compression-4-points">Q1: Image Compression (4 points)</a></h3>
<p><img src="quiz_problems_2023_images/quiz15_seal.png" alt="" /></p>
<p>Before the next big game, facing an inevitable loss, Berkeley students hacked into Stanford website and tried to mutilate the Stanford logo into a Berkeley blue color version (but did a bad job at it). The mutilated logo is shown as an image above.</p>
<p>This image is of dimensions $370\times370$, and contains $4$ channels (RGBA) instead of $3$ channels for colors we saw in class. The fourth channel is alpha channel which tells the transparency of the image. The bit-depth of this image is $8$, which basically implies that every pixel in each channel is 8 bits.</p>
<p>This file can be compressed losslessly using PNG to $\sim14.3~\text{KB}$ (kilo-bytes).</p>
<h4 id="q11-1-point-10"><a class="header" href="#q11-1-point-10">Q1.1 (1 point)</a></h4>
<p>What's the expected raw size of this image? Leave you answer in KB (note: Kilo Bytes not Kilo bits)</p>
<h4 id="q12-1-point-10"><a class="header" href="#q12-1-point-10">Q1.2 (1 point)</a></h4>
<p>In this image you can see that there are basically just two colors (white and a bad version of Berkeley blue color). What will be the expected image size if we use only 2 colors to compress this image in KB? Note, we assume that you still need 8 bits for the alpha channel.</p>
<h4 id="q13-1-point-8"><a class="header" href="#q13-1-point-8">Q1.3 (1 point)</a></h4>
<p>Now you also see that along with having just 2 colors, the image also has only two levels of transparency (perfectly transparent and perfectly opaque). Using these properties what will be the expected image size in KB?</p>
<h4 id="q14-1-point-3"><a class="header" href="#q14-1-point-3">Q1.4 (1 point)</a></h4>
<p>PNG seems to perform better than even using 1 bit for color and 1 bit for alpha!</p>
<p>Give one reason why this might be the case.
Note: there are many reasons! But we are only asking for one so feel free to choose.</p>
<h3 id="q2-jpee274g-compressor-4-points"><a class="header" href="#q2-jpee274g-compressor-4-points">Q2: JPEE274G Compressor (4 points)</a></h3>
<p>EE274 students decided to come together and form JPEE274G (Joint Photographers EE 274 Group) coming up with an image compressor with the same name. Help them make the design decisions.</p>
<h4 id="q21-1-point-9"><a class="header" href="#q21-1-point-9">Q2.1 (1 point)</a></h4>
<p>Riding on the compute revolution, JPEE274G decided to go for $64 \times 64$ block size instead of $8\times8$.</p>
<p>Suppose you have the same image at resolution $480\times480$, $720\times720$, $1080\times1080$</p>
<p>In which of the following case do we expect increasing the block-size help the the most.</p>
<p>( ) $480\times480$</p>
<p>( ) $720\times720$</p>
<p>( ) $1080\times1080$</p>
<h4 id="q22-1-point-9"><a class="header" href="#q22-1-point-9">Q2.2 (1 point)</a></h4>
<p>JPEE274G decided to use prediction of blocks based on previously encoded neighbors. In which of the following two images do we expect the prediction to help the most.</p>
<p><img src="quiz_problems_2023_images/quiz15_pokemon_1.jpeg" alt="" />
<img src="quiz_problems_2023_images/quiz15_pokemon_2.png" alt="" /></p>
<p>( ) Charizard (the one with the orange cranky being)</p>
<p>( ) Assorted PokÃ©mons (the one with Pokemon written in it)</p>
<h3 id="q3-predictive-coding-2-points"><a class="header" href="#q3-predictive-coding-2-points">Q3: Predictive Coding (2 points)</a></h3>
<p>You find a source where consecutive values are very close, so you decide to do predictive lossy compression. Encoder works in following fashion: it first transmits the first symbol and after that it quantizes the error based on prediction from last encoded symbol. The quantized prediction error is transmitted.</p>
<p>Formally, suppose $X_1,X_2,\dots$ is your original sequence and $\hat{X}_1,\hat{X}_2,\dots$ is the reconstruction sequence. Then we have:</p>
<ul>
<li>for the first symbol the reconstruction $\hat{X}_1 = X_1$, i.e., you are losslessly encoding the first symbol</li>
<li>prediction for $X_n$ is simply $\hat{X}_{n-1}$</li>
<li>prediction error is $e_n = X_n-\hat{X}_{n-1}$</li>
<li>quantized prediction error is $\hat{e}_n$</li>
<li>reconstruction for $X_n$ is $\hat{X}<em>n = \hat{X}</em>{n-1} + \hat{e}_n$</li>
<li>the transmitted sequence is $X_1, \hat{e}_2, \hat{e}_3, \dots$</li>
</ul>
<p>For this question, assume that the quantization for the prediction error is simply integer floor.</p>
<p>Example encoding for source sequence: $0.4, 1.1, 1.5, 0.9, 2.1, 2.9$</p>
<div class="table-wrapper"><table><thead><tr><th>$n$</th><th>$\hat{X}_{n-1}$</th><th>$X_n$</th><th>$e_n$</th><th>$\hat{e}_n$</th></tr></thead><tbody>
<tr><td>1</td><td>-</td><td>0.4</td><td>-</td><td>-</td></tr>
<tr><td>2</td><td>0.4</td><td>1.1</td><td>0.7</td><td>0</td></tr>
<tr><td>3</td><td>0.4</td><td>1.5</td><td>1.1</td><td>1</td></tr>
<tr><td>4</td><td>1.4</td><td>0.9</td><td>-0.5</td><td>-1</td></tr>
<tr><td>5</td><td>0.4</td><td>2.1</td><td>1.7</td><td>1</td></tr>
<tr><td>6</td><td>1.4</td><td>2.9</td><td>1.5</td><td>1</td></tr>
<tr><td>7</td><td>2.4</td><td>-</td><td>-</td><td>-</td></tr>
</tbody></table>
</div>
<h4 id="q31-errors-1-point"><a class="header" href="#q31-errors-1-point">Q3.1: Errors (1 point)</a></h4>
<p>What is the absolute value of reconstruction error $|X_n - \hat{X}_{n}|$ for the last symbol?</p>
<h4 id="q32-now-decode-1-point"><a class="header" href="#q32-now-decode-1-point">Q3.2: Now Decode (1 point)</a></h4>
<p>Given the transmitted sequence $X_1, \hat{e}_2, \hat{e}_3, \dots$ = $1.1, 0, 1, -1, 2, -1$, what is the final decoded value $\hat{X}_6$?</p>
<h2 id="quiz-16-learnt-image-compression"><a class="header" href="#quiz-16-learnt-image-compression">Quiz 16 (Learnt Image Compression)</a></h2>
<h3 id="q1-7-points"><a class="header" href="#q1-7-points">Q1 (7 points)</a></h3>
<p>Have a look at the notebook we showed in class to answer following questions. The notebook can be found at class website as well as on <a href="https://colab.research.google.com/drive/1O3eQAaxlyLYI1HO7K1b12eJQsQKxjWwx?usp=sharing">this link</a>.</p>
<h4 id="q11-2-points"><a class="header" href="#q11-2-points">Q1.1 (2 points)</a></h4>
<p>We have defined the transforms, but we need to define a training procedure to train these non-linear transforms. We are using the distribution <code>tfp.distributions.Normal(loc=0., scale=1.)</code> as a prior for codelayer <code>y</code>. i.e. we are making the <code>analysis_transform</code> decorrelate input data to unit gaussians.</p>
<p>In which case do you expect the output of <code>estimated_rate_loss(self.prior, y)</code> to be higher:</p>
<p>( ) <code>y=5</code></p>
<p>( ) <code>y=0</code></p>
<h4 id="q12-3-points"><a class="header" href="#q12-3-points">Q1.2 (3 points)</a></h4>
<p>Explain briefly why with unit normal prior for code layer <code>y</code> with <code>latent_dims=50</code>, the <code>bits_used</code> can never go below <code>50 bits</code>.</p>
<p>Hint: take a look at the prior distribution on the integers on slide 41 <a href="https://stanforddatacompressionclass.github.io/Fall23/static_files/slide_16_2023.pdf">here</a>.</p>
<p>Bonus: can you think of a tighter lower-bound?</p>
<h4 id="q13-2-points"><a class="header" href="#q13-2-points">Q1.3 (2 points)</a></h4>
<p>Which of the following would you expect to reduce the rate where the loss function is $R+\lambda D$.
Select all that apply.</p>
<p>[ ] increase $\lambda$</p>
<p>[ ] decrease $\lambda$</p>
<p>[ ] increase the latent space dimension</p>
<p>[ ] decrease the latent space dimension</p>
<h3 id="solutions-1"><a class="header" href="#solutions-1">Solutions</a></h3>
<ol>
<li></li>
</ol>
<p>(X) y=5</p>
<p>( ) y=0</p>
<p>Solution: Note that the rate loss is defined as the negative log probability of the quantized Gaussian probability distribution, and hence y farther away from 0 gives us lower probability and hence higher rate.</p>
<ol start="2">
<li>
<p>Solution: The best you can do in terms of bits used is to have all the quantized latent variables be 0 (since that has the highest probability). Looking at the figure, we see the probability is below 0.4 so the number of bits used by arithmetic coder is more than log2(1/0.4) ~ 1.32 bits/latent dimension. Thus we will not go below 50 bits and a tighter bound would be 50*1.32=66 bits.</p>
</li>
<li></li>
</ol>
<p>[ ] increase $\lambda$</p>
<p>[X] decrease $\lambda$</p>
<p>[ ] increase the latent space dimension</p>
<p>[X] decrease the latent space dimension</p>
<p>Solution: Lower $\lambda$ gives lower emphasis on distortion and hence the training will lead to a model with higher distortion but lower rate. Decreasing the latent space dimension will also reduce the rate (in particular it will reduce the lowest rate you can achieve as you have seen in the previous question).</p>
<h2 id="quiz-17-humans-and-compression"><a class="header" href="#quiz-17-humans-and-compression">Quiz 17 (Humans and Compression)</a></h2>
<h3 id="q1-mse-vs-l1-error-vs-visual-perception-1-point"><a class="header" href="#q1-mse-vs-l1-error-vs-visual-perception-1-point">Q1: MSE vs. L1 error vs. visual perception (1 point)</a></h3>
<p>Recall the distortion functions MSE (mean square error) and L1 distortion (mean absolute error). Which of the following is always true?</p>
<p>( ) Lower MSE implies better visual perception</p>
<p>( ) Lower L1 implies better visual perception</p>
<p>( ) None of the above</p>
<h3 id="q2-chroma-subsampling-2-points"><a class="header" href="#q2-chroma-subsampling-2-points">Q2: Chroma subsampling (2 points)</a></h3>
<p>In which of the following settings will chroma subsampling typically lead to most noticeable artifacts?
(select all that apply)</p>
<p>[ ] Landscape image (natural images have low spatial frequencies)</p>
<p>[ ] Text image (say screenshot of wikipedia page)</p>
<p>[ ] Cartoon/Animated Content (note cartoons are artificially generated and have sharp edges)</p>
<p>[ ] Videos (note videos have temporal variance in noise along with spatial variance)</p>
<h3 id="q3-ycbcr-matrix-4-points"><a class="header" href="#q3-ycbcr-matrix-4-points">Q3: YCbCr matrix (4 points)</a></h3>
<p>Look at the images we saw in class on color-space below. Recall that <code>Y</code> represents luminance or white/black opponent channel, <code>Cb</code> represents blue/yellow and <code>Cr</code> represents red/green color channel. It might be helpful to know that yellow can be represented as a combination of red and green. Answer the following questions:</p>
<p><img src="quiz_problems_2023_images/quiz17_chroma_1.png" alt="" />
<img src="quiz_problems_2023_images/quiz17_chroma_2.png" alt="" />
<img src="quiz_problems_2023_images/quiz17_chroma_3.png" alt="" /></p>
<p>A student comes up with a new colorspace conversion recommendation for RGB to YUV conversion. Which of the following matrices are possibly correct color conversions:
(select all that apply)</p>
<p>[ ]</p>
<p>$$
\begin{pmatrix}
Y' \
C_B \
C_R
\end{pmatrix} =
\begin{bmatrix}
0.33 &amp; 0.33 &amp; 0.33\
-0.25 &amp; -0.25 &amp; 0.5\
0.5 &amp; -0.5 &amp; 0
\end{bmatrix}
\begin{pmatrix}
R' \
G' \
B'
\end{pmatrix}
$$</p>
<p>[ ]</p>
<p>$$
\begin{pmatrix}
Y' \
C_B \
C_R
\end{pmatrix} =
\begin{bmatrix}
0.33 &amp; 0.33 &amp; 0.33\
-0.25 &amp; -0.25 &amp; 0.5\
0.5 &amp; 0 &amp; -0.5
\end{bmatrix}
\begin{pmatrix}
R' \
G' \
B'
\end{pmatrix}
$$</p>
<p>[ ]</p>
<p>$$
\begin{pmatrix}
Y' \
C_B \
C_R
\end{pmatrix} =
\begin{bmatrix}
0.33 &amp; 0.33 &amp; 0.33\
0 &amp; -0.5 &amp; 0.5\
0.5 &amp; -0.5 &amp; 0
\end{bmatrix}
\begin{pmatrix}
R' \
G' \
B'
\end{pmatrix}
$$</p>
<p>[ ]</p>
<p>$$
\begin{pmatrix}
Y' \
C_B \
C_R
\end{pmatrix} =
\begin{bmatrix}
0 &amp; 0.5 &amp; 0.5\
0 &amp; -0.5 &amp; 0.5\
0.5 &amp; -0.5 &amp; 0
\end{bmatrix}
\begin{pmatrix}
R' \
G' \
B'
\end{pmatrix}
$$</p>
<h3 id="solutions-2"><a class="header" href="#solutions-2">Solutions</a></h3>
<p>Q1</p>
<p>( ) Lower MSE implies better visual perception</p>
<p>( ) Lower L1 implies better visual perception</p>
<p>(X) None of the above</p>
<p>Solution: Neither MSE nor L1 are an accurate estimate of visual perception.</p>
<p>Q2</p>
<p>[ ] Landscape image (natural images have low spatial frequencies)</p>
<p>[X] Text image (say screenshot of wikipedia page)</p>
<p>[X] Cartoon/Animated Content (note cartoons are artificially generated and have sharp edges)</p>
<p>[ ] Videos (note videos have temporal variance in noise along with spatial variance)</p>
<p>Solution: As seen in class, chroma subsampling produces significant artifacts when you have a lot of sharp edges like in text/screenshot and cartoons.</p>
<p>Q3</p>
<p>Solution: Only the first matrix is correct. In each of the other options, one of the rows doesn't represent the proper meaning of the YCbCr components. For example in the second option, the third row corresponding to Cr has R-B instead of R-G.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="mermaid.min.js"></script>
        <script type="text/javascript" src="mermaid-init.js"></script>

        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </body>
</html>
